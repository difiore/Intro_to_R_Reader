# Introduction to Data Analysis

> Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise
> <div align="right">John W. Tukey, *The Future of Data Analysis* </div>

## Exploratory Data Analysis 
So far, we've learned about how to manipulate our data and how to graph our outputs. Both of these are critically important parts of what's known as exploratory data analysis - or EDA. When you're starting with a new dataset, you won't always immediately know what trends and patterns might be there to discover. The idea at this stage isn't to find out what's causing any trends in the data, to identify any significant results you might have, or to get publishable or presentable figures and tables - the point is to understand exactly what it is that you're dealing with. For more on this topic, check out [this post](https://simplystatistics.org/2019/04/17/tukey-design-thinking-and-better-questions/).

### Sidenote

EDA differs from what most people expect data analytics to be. Most people expect analytics to follow a clearcut path, going in a linear direction from deciding on a question to answering said question completely and accurately. But that really isn't the point of data analytics, and especially not EDA, as Tukey (the father of data analytics and modern statistics) says at the top of this chapter. Most people think of analytics as something you do to improve the strength of your evidence, but that's not what the discipline excels at. 

Rather, the discipline shines best when it's working on [improving the question you're asking](https://simplystatistics.org/2019/04/17/tukey-design-thinking-and-better-questions/), perhaps generating evidence for your case along the way. That's not to say you can't use analytics to try and answer a question - you can, and most people working in the field today use it in that manner. But you'll be a better analyst if you keep yourself open to new surprises, and chase them down, rather than attempting to make your analysis follow a linear path.

Speaking of surprises, I really enjoy [this quote](https://fivethirtyeight.com/features/election-update-why-our-model-thinks-beto-orourke-really-has-a-chance-in-texas/) from Nate Silver, founder and editor in chief of FiveThirtyEight:

> You ideally want to find yourself surprised by the data some of the time — just not too often. If you never come up with a result that surprises you, it generally means that you didn’t spend a lot of time actually looking at the data; instead, you just imparted your assumptions onto your analysis and engaged in a fancy form of confirmation bias. If you’re constantly surprised, on the other hand, more often than not that means your [code] is buggy or you don’t know the field well enough; a lot of the “surprises” are really just mistakes.  
> ---Nate Silver

Surprises are awesome, and are how discoveries are made. But at the same time, a lot of papers are retracted because their big surprise was actually just a glitch in the code. Whenever you find something you didn't expect, make sure you go back through your code and assumptions - it never hurts to double check!

For more on this topic, check out the awesome lecture notes for [Skepticism in Data Science](https://jhu-advdatasci.github.io/2018/lectures/12-being-skeptical.html) from John Hopkins University.

### The EDA Framework

I've found it useful, in composing my thoughts about exploratory data analysis, to fit the rough steps of EDA into a three-part framework. The first thing I do when presented with any new dataset is to **summarize** the data, to see what general shape it's in. This lets me catch outliers and typos, see if there's any values I have a question on, and start getting a sense of the scope of the dataset I'm working with. I'll then usually start to **visualize** the data, making exploratory graphs in order to get a more concrete sense of how each variable interacts with all the others, and to see if any patterns emerge from the dataset. I'll then usually start to **analyze** the patterns I'm finding, picking them apart to see if I can better understand how they work before I begin modeling them.

However, I don't believe EDA is so simple that I can say these are ordered steps - that summarizing comes before visualization, which happens before you being analyzing patterns. I believe that EDA typically involves multiple rounds of each step, where visualizations give you new ideas for summaries you'd be interested in, which perhaps give rise to patterns you want to analyze, which in turn inspire more visualizations. The point is to use your skills to play with your datasets, until you're satisfied you have enough information to proceed onto your analysis. 

## gapminder
We're going to use the `gapminder` dataset for this unit, in order to see more a more realistic application of exploratory data analysis that you might perform when working with a new dataset for the first time. In order to do this, we're going to install the `gapminder` package, then load it as usual:

```
install.package("gapminder")
```
```{r}
library(gapminder)
```

This package includes five data tables, all focusing on GDP and life expectancy values for a set of countries. The largest of these datasets is `gapminder_unfiltered`, which is what we'll be working with now. Let's take a look using `head()`:

```{r}
head(gapminder_unfiltered)
```

So as we can see, there are six columns in this dataset, which are more or less descriptively named. If you've got questions about the dataset, or what data is stored in any of the columns, you can check this dataset out with `?gapminder_unfiltered`.

## Summarizing Data
It would now be helpful for us to get a bit broader sense of what each column contains. One of the easiest ways to do that is with `summary()`:
```{r}
summary(gapminder_unfiltered)
```

You can see that `summary()` gives us some basic summary statistics of each column - and that these views are different for numeric and text columns. We can also start seeing how these values are distributed - it seems like the values cover six continents (though I'm a little iffy on what "FSU" means, and it looks like the Americas are merged), and 57 years (from 1950 to 2007). The numeric columns, meanwhile, have simple summary statistics listed. We can dig down a little deeper into these statistics using the `describe()` function, from the `psych` package:

```
install.packages("psych")
```
```{r}
library(psych)
```

```{r}
describe(gapminder_unfiltered)
```

Now in addition to the values from `summary()`, we get the number of observations, each column's standard deviation, trimmed mean, median absolute deviation, range, skew, kurtosis, and standard error. 

We can also, if we wanted, check to see how many unique countries are represented in the dataset, by checking the `length()` of a vector of all the `unique()` values of the country column:

```{r}
length(unique(gapminder_unfiltered$country))
```

If we wanted to make that a little simpler, by the way, we can use the `n_distinct()` function from `dplyr`. As a sidenote, we can use a function from a package without loading it (via `library`) by typing the package name followed by `::`, like so:

```{r}
dplyr::n_distinct(gapminder_unfiltered$country)
```

However, it makes sense for us to load the package explicitly, since we'll be using a lot of the functions from it for the rest of this chapter.

```{r}
library(dplyr)
```

So looking at those values, I'm noticing some extreme numbers in the life expectancy and GDP columns - specifically, the minimums seem much lower than I'd expect. We can pull those rows out of the dataset using `filter()`:

```{r}
gapminder_unfiltered %>%
  filter(lifeExp == min(lifeExp))

gapminder_unfiltered %>%
  filter(gdpPercap == min(gdpPercap))
```

As history buffs might know, these values are both (sadly) explained by politics and history. The other oddball I want to check is what the "FSU" continent is meant to represent:

```{r}
gapminder_unfiltered %>%
  filter(continent == "FSU")
```

Ahh! These countries are all **F**ormer **S**oviet **U**nion states, so are broken out from the other countries - and have a lot of incomplete data. 

The other notable values I'd like to check out are the super high *skew* and *kurtosis* values that `describe()` gave us for the GDP column. These values demonstrate that our data really doesn't follow the normal distribution (which has a value of 0 for both metrics) - but I'd like to visualize how:

```{r}
library(ggplot2)
ggplot(gapminder_unfiltered, aes(gdpPercap)) + 
  geom_histogram()
```

So our data is less variable than we'd expect (that's the high kurtosis, implying that the mode is more common than it would be in the normal distribution), and right-skewed (meaning that our mean is higher than the median, since there are a number of high outlier values). That's not a *bad* thing, but it's worth knowing how our data is structured and wondering why, and how it might affect the other variables we can measure.

This graph could probably be made more effective if we split it up by continent. Let's use `facet_wrap()` to create six panels of the graph, one for each continent:

```{r}
ggplot(gapminder_unfiltered, aes(gdpPercap)) + 
  geom_histogram() + 
  facet_wrap(~ continent)
```

If we wanted to calculate our values as a relative measurement rather than an absolute count - so that we can better see the patterns in other areas, and lower the high outlier in Africa - we can provide `..density..` as the y argument in `aes`:

```{r}
ggplot(gapminder_unfiltered, aes(gdpPercap, ..density..)) + 
  geom_histogram() + 
  facet_wrap(~ continent)
```

Now our Y axis represents the proportion of all values that particular GDP makes up, rather than the absolute count of values in that bin.

This is already helping me understand what we're working with - most of our lowest GDP measurements come from Africa, while Europe is pretty uniformly higher, and Asia has a more skewed spread than any other region. 

### Sidenote

Often, people dislike using histograms, and instead choose to use frequency polygons - implemented in ggplot using the `geom_freqpoly()` function:
```{r}
ggplot(gapminder_unfiltered, aes(gdpPercap, ..density..)) + 
  geom_freqpoly() + 
  facet_wrap(~ continent)
```

This sometimes then gives rise to a thought - since these lines take up so much less space, why not just combine them all into one panel, rather than six? And so they make graphs that look like this:

```{r}
ggplot(gapminder_unfiltered, aes(gdpPercap, ..density.., color = continent)) + 
  geom_freqpoly() 
```

This is a Bad Graph, as I hope should be obvious. It's almost impossible to pick out any individual line, and the whole thing just looks like a jumbled mess - which has led the community to name this sort of visualization as a "spaghetti graph". In cases like this, where a graph contains more than one or two lines (with obvious separations), it makes much more sense to facet your graphs.

## Visualizing Data
Anyway. These are the first visualizations we've made to start seeing what our data looks like. It's worthwhile to make a lot of these fast graphics in order to get a handle on your datasets, so you can see what angles might be useful for your analysis. One of the most useful is `pairs`, which will show you scatterplots between each of your variables. Let's try running it on the gapminder data now:

```{r}
pairs(gapminder_unfiltered)
```

To interpret this graph, understand that each text box ("country", "continent" and so on) represents that variable - in any given column, that text box is on the X axis, while for any given row the text box represents the Y axis.  

Even knowing that, the graph is still a little hard to understand - the boxes are small, and the "country" and "continent" graphs don't make a ton of sense. To get rid of those (and hopefully fix both problems at once), we could use `select()` from last chapter and make a new dataset:

```{r}
gp <- select(gapminder_unfiltered, 3:6)
pairs(gp)
```

But an even cleaner method is to skip the new dataset altogether, and just use `select_if()`. This function lets us only select columns that return `TRUE` when passed to a function, and works like this:

```{r}
head(select_if(gapminder_unfiltered, is.numeric))
```

Note how instead of column names we provided the name of a function, `is.numeric()`. We also didn't put parentheses after the `is.numeric` function - we just pass it the name. That lets us easily and cleanly select all the numeric columns, making sure we didn't skip any. We can then make the graph in a single line of code like this:

```{r}
pairs(select_if(gapminder_unfiltered, is.numeric))
```

This is a bit easier to read! It looks like life expectancy generally increases over time and with higher GDP, and that population and GDP both also increase over time. If we want a more numeric version of these graphs, we can calculate the actual correlation coefficients by swapping `cor()` in for `pairs()`:

```{r}
cor(select_if(gapminder_unfiltered, is.numeric))
```

These numbers range from -1 to 1, with higher absolute values representing higher correlations. So we can see that our visual judgements are pretty well supported by the numbers - especially that there's a strong relationship between GDP and life expectancy!

## Analyzing Patterns
Now we're moving into the next piece of EDA - exploring the patterns that we pick up on. This isn't something you need to save until you're done summarizing and visualizing your data - rather, it's a process that you should follow every time you find something that seems interesting in the data. A lot of these paths may be dead ends, but the ones that are worth following will often wind up shaping your analysis, providing insights into your dataset that you hadn't been expecting when you started.

I'd be interested in looking a little more into how GDP and life expectancy interact. Let's make a scatter plot for just those two variables:

```{r}
ggplot(gapminder_unfiltered, aes(gdpPercap, lifeExp)) + 
  geom_point()
```

This is already an interesting pattern, in and of itself - it seems like higher GDP results in higher life expectancy, but only to a point. This sort of shape - where there's an obvious and single curve to the data - always makes me think that log-transforming the data might make the relationship more linear:

```{r}
ggplot(gapminder_unfiltered, aes(gdpPercap, lifeExp)) + 
  geom_point() + 
  scale_x_log10()
```

And in this case it does, very dramatically. Now, I don't particularly like log-transformed graphs, as I discussed in chapter 2. However, it will be important for us to know that our data has this relationship when we go to perform our analyses. That, however, is a matter for the next chapter.

I've started to wonder now how other variables play into this pattern. For instance, I know that people lived less long back in 1950 than they do today, and I know that life expectancy in some areas of the world is significantly lower than others. Luckily enough, we have the ability now to add both these factors to our visualization - here, I've split the graph into facets by continent, while recoloring the points by the year.

```{r}
ggplot(gapminder_unfiltered, aes(gdpPercap, lifeExp, color = year)) + 
  geom_point() + 
  facet_wrap(~ continent)
```

Splitting the graph into panels by continent was very effective, I think - Africa has notably lower life expectancy and GDP than other countries do, while Europe seems to always be higher than average. I don't know that coloring the points by year has quite the same effect, probably due to there being too many points on this graph - my guess is that we'll see that impact more while we're modeling the data in our next chapter. 

I'm feeling very comfortable with how we've progressed in this chapter - we've moved from downloading a brand new dataset to more thoroughly exploring it, and have picked up on a pattern that we're going to subject to a more thorough analysis. We could totally keep using our EDA skills in order to investigate patterns further - for instance, it might be interesting to examine what factors impact population over time, or see how rates of population change dependent upon life expectancy and GDP! However, the skeleton here demonstrating the steps of EDA should be enough to give you ideas of how to apply this framework to your own analyses moving forward. Instead, we're going to continue on with the `gapminder` package in our next unit, as we begin making models to actually analyze this pattern we've found.

## Exercises

1. The output of `psych::describe(gapminder_unfiltered)` put stars after `country` and `continent`. Why?
2. Make a histogram of a gapminder variable other than GDP. Describe the graph with regards to its skewdness and kurtosis. 
3. Answer the question: how does population seem to be impacted by the other variables in the dataset? Use the techniques we've demonstrated here to find out.
4. Just like `select()` has a cousin in `select_if()`, all the other main `dplyr` functions (`mutate()`, `filter()`, `group_by()`, and `summarise()`, among others) also have an `_if()` variant. Use `mutate_if()` to multiply all the numeric columns by 2.
