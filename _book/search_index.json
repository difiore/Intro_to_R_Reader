[
["index.html", "Introduction to Data Exploration and Analysis with R Welcome to IDEAR 0.1 The State of the Book 0.2 Book Outline 0.3 Other Sources", " Introduction to Data Exploration and Analysis with R Michael Mahoney 2019-06-19 Welcome to IDEAR There are only two kinds of programming languages: those people always bitch about and those nobody uses. — Bjarne Stroustrup Welcome to Introduction to Data Exploration and Analysis in R (IDEAr)! This book is designed as a crash course in coding with R and data analysis, built for people trying to teach themselves the techniques needed for most analyst jobs today. The book assumes no prior experience with R or data analysis, and aims to be equally applicable to industry and scientific purposes. This reader is currently being continuously deployed to bookdown.org and GitHub, particularly as new sections are completed or old ones restructured. This is so that I can get feedback from the small group of people who are using this book to learn R themselves, so I can adjust and adapt the text as needed. If you’d like to help with this process, I’d love to hear from you, at mike.mahoney.218@gmail.com, or see a pull request made on GitHub. More information about me can be found at my website, which just so happens to have been built in R. 0.1 The State of the Book When I began writing this book, I was designing it as course notes - a reader, of sorts - for an undergraduate course, which would be supported with lectures, homeworks, and other supporting materials. I was distributing it mostly to friends, who would text me directly with questions, and so was willing to let things be a little confusing or a little out of order. What I’ve found is that the book is useful mostly for people self-studying to learn data analytics and R, even though that isn’t what it was designed for. Meanwhile, it doesn’t seem like I’ll be teaching the theoretical course any time soon - so it would make sense that, if I’m going to leave this book up, it ought to be built for what people are actually using it for. The form should follow the function, I suppose. And so that’s what I’m doing now - redesigning this book in order to make it more useful to people self-studying, teaching themselves R via the examples and exercises contained in here. That means a lot of things are going to get shifted and edited - particularly towards the end of the book, where certain topics (the now-excised unit on machine learning, for instance) which were originally written with the intent of instructor support could be sacrificed, in order to give a more cohesive learning experience. That means that the book is about to shift a lot, which might be a pain for anyone currently using it - which I am sorry about! If you want, I can send anyone HTML or PDF files of old versions of the chapters, if that would be helpful. Just contact me via any method above. I’ve included a changelog at the end of the book, to detail exactly what’s being changed. Each chapter below is labeled “stable”, if I believe it will not change significantly moving forward, or “unstable”, if I see dramatic changes coming. 0.2 Book Outline This book serves as an introduction to R for scientific and business applications, focusing specifically on exploratory data analysis, hypothesis generation and confirmation, data visualization, and communication of results. It requires no prior knowledge of computer programming, computer science, or statistics, though a solid base in any of those fields will likely make your learning process faster. The goal is to leave you with the basic essentials of working in R, as well as a strong foundation in thinking like a data analyst that will help you understand how to tackle more complicated problems. You won’t be an R maestro, and you won’t have developed domain-specific knowledge - but you’ll have developed knowledge that will allow you to learn those from other resources. To that end, we’ll focus primarily on the basic language skills required to implement those more complicated methods, and will skip over things such as natural language processing and machine learning. To begin, we’ll introduce you to programming and the quirks of R, and how you can use those skills to make data visualizations. These chapters include: Introduction to R - Stable Data Visualization - Stable R Basics and Workflow - Stable (for now) We’ll then get into the data analysis workflow, stepping through each component of this process in turn. These chapters are: Data Wrangling - Stable Exploratory Data Analysis - Stable Modeling - Unstable Achieving Graphical Excellence - Stable Towards the end of the course, we’ll shift our focus to skills which will let you work in professional settings and larger groups, using your skills more efficiently and communicating better via code. These chapters include: Functions and Scripting - Stable More Complicated Analyses - Unstable Markdown and Clear Code - Unstable The end of the course then covers topics which I have found to be more specialized, and - while important - not as universally applicable to every project. This section includes the units: Working with Text - Stable Dates and Times - Stable Other Uses (What Next?) - Stable The backmatter of the reader then concerns how to get help outside of this book, containing both links to useful resources and some frequently asked questions. The two units in this section are: Basic Statistics - Stable Other Resources - Stable FAQ - Unstable 0.3 Other Sources If this introduction isn’t quite your style, I’d highly recommend Garrett Grolemund and Hadley Wickham’s R for Data Science, as well as Wickham’s Advanced R. Many other useful resources can be found in Chapter 12, at the end of this book. "],
["introduction-to-r.html", "1 Introduction to R 1.1 Why Does This Book Exist? 1.2 What is R? 1.3 What is coding? 1.4 Conventions of the book 1.5 Things You’ll Need 1.6 Introduction to RStudio 1.7 Your First Program 1.8 The iris Dataset 1.9 Graphing with R 1.10 Exercises", " 1 Introduction to R Q: Why are open-source statistical languages the best? A: Because they R! 1.1 Why Does This Book Exist? There are a lot of resources online to learn R. Some of them are extremely well-written and well-structured, but approach things from a different perspective than I find useful. Some of them are expansive and touch on a massive number of topics, but either don’t go as thoroughly into these topics as might be helpful, or have an interesting writing style or are otherwise hard to follow. I’ve got a very specific idea of how R should be taught, at least to those interested in using it for data science and other analytical applications. This reader represents that approach - we start off with data visualization, then exploration, and then get into data analysis and transformation. Ideally, this will quickly get you up to speed with tasks that are engaging and show obvious returns on your investment, while teaching the basics of the language you’ll need for more advanced tasks. But all that comes later. First off, we have to answer one of the most basic questions surrounding this book: what even is R? 1.2 What is R? R is a programming language used most commonly for data analysis and science. It’s completely free and is adaptable to almost any project - for instance, this book, my website, and plenty of statistical softwares are all written in R. However, where R really shines is in how easy it makes manipulating and analyzing data, which is what we’ll be focusing on here. 1.3 What is coding? Coding is giving very specific instructions to a very stupid machine. Or rather, a very literal machine - if you do things out of order, or misspell something, or capitalize things you shouldn’t, the computer won’t do it - instead, it’ll give you an error message. But if you get the syntax exactly right, the computer can do very complex tasks very quickly. If that sounds frustrating, well… it sometimes is! But getting good at coding is mostly learning to be okay with being frustrated - and learning what to do when your code is being frustrating. That being said, if your code doesn’t work when you’re getting started, make sure to check that you’ve typed exactly what’s written in the book. Usually errors in code when you’re starting out come from a misplaced comma or parenthesis, so make sure to check that you’ve copied things down correctly! 1.4 Conventions of the book We’ll go over standard code styles a bit later in the course - there is a Right Waytm to write code, but we won’t worry about that for a few more chapters. But so you can understand a few basics, here’s a few styles we’ll use in the text: If text is preformatted, it means it’s something from R - a function, some data, or anything else that you’d use in your code. Blocks of code will be represented as follows: print(&quot;Hello, World!&quot;) This format both makes it obvious that this is code - not text - and lets you copy and paste it into your R session to see the results for yourself. Code outputs, meanwhile, will mostly be shown like this: ## [1] &quot;Hello, World!&quot; Where text is commented out (that is, has a # in front, so R won’t parse it), so it won’t do anything if you put it in your session. Generally speaking, you should try and type out each block of code in this reader into R. It’s critical that you start getting the muscle memory of typing in R down - that you understand what needs to be capitalized, what needs to be quoted, and what you’re most likely to typo. You can copy and paste straight from this book, but you’ll be setting your own learning back. There are some exceptions to these general rules, but this is enough to get us started. With the introductions out of the way, we can start getting you set up to coding R. 1.5 Things You’ll Need There are two big pieces of software integral to this course reader, namely: R - download it here. RStudio - download it here. Choose the free desktop version - you don’t need the server software, and you don’t need the commercial license. We’ll be installing other pieces of software (in the form of R packages) throughout this reader, but each of those will be explicitly labeled when we use them. 1.6 Introduction to RStudio You’ll almost never need to use R directly in this course - the form of R that you download from that first link is much harder to use than anything professionals interact with. Most practitioners use what’s known as an IDE - an Interactive Development Environment. There’s a lot of choices of IDEs for R, but RStudio is the best one. Other textbooks would give you a more measured, even-handed approach to this issue. RStudio is the best one, though. This book is assuming you’re using RStudio for all the examples, questions, and assignments. As such, we’re going to go over what you’ll see when you open RStudio for the first time. On the bottom right there you’ll see a list of all the files available in the folder that you’re working in. This window will also show you graphs when you make them, and help files when you look for them. Above it is a list of everything that’s available in the current “environment” - that is, all the datasets/functions/code that you’ve already programmed in the session, that can be used again moving forward. On the left - taking up the full height of the pane - is something called the “Console”. This is the first place you can write R code, where you can give very specific commands to that very dumb machine. This is what we’ll be using for this entire unit - whenever you’re typing code from this reader into R, you should be typing it into the console. Try it now - it’s time for us to start coding! 1.7 Your First Program Two things to point out, before we get started - if you type either ( or &quot; into RStudio, you’ll notice that the closing mark ) or &quot; are automatically entered for you. If you type the closing mark, it won’t be entered - RStudio is smart enough to prevent you from duplicating this punctuation. However, your programs won’t work if you have any missing punctuation - if you’re getting weird error messages, make sure you have the proper quotes and parentheses! Secondly, if you hit “enter” while working in the console, you’ll tell R to process your code and give you the output. If you want to type more than one line at a time, hold shift while pressing enter. Okay, now let’s get going. Type the following into the console - but be careful, everything R does is case-sensitive: print(&quot;Hello, world!&quot;) What happened? If you did it right, you should have gotten the following: ## [1] &quot;Hello, world!&quot; It’s cool, right? Congratulations, you’re officially a programmer! What we just did was use a function (print()), R commands that take inputs in the form of arguments (inside the parenthesis) and use them to return an output. We’ll get under the hood of functions in unit 5 - for now, we’re going to use the many functions other people have put together for us. In addition to saying hi, you can use the R console to do math: 2 + 4 / 2 ## [1] 4 6^7 ## [1] 279936 18%%7 ## [1] 4 Note that R generally follows PEMDAS, with a few exceptions - we’ll cover those exceptions in another chapter. Also, %% is an operator (in the same way that +, *, -, and / are all operators - mathematical symbols that represent a function) which returns the remainder - the integer (whole number) which remains after you’ve divided as much as you can. So while 7 fits into 18 twice (7*2 = 14), it leaves 4 “left over” afterwards. R can also do a lot of more complicated things. By putting a list of values inside c(), you create what’s known as a vector - a list of objects that R can interact with. c(1, 2, 3) ## [1] 1 2 3 c(18, &quot;ESF&quot;, 98) ## [1] &quot;18&quot; &quot;ESF&quot; &quot;98&quot; c(&quot;ESF&quot;, &quot;Acorns&quot;, &quot;Stumpies&quot;) ## [1] &quot;ESF&quot; &quot;Acorns&quot; &quot;Stumpies&quot; (The “c” stands for combine, by the way.) R can use vectors to perform all sorts of matrix algebra operations. For instance, if you try to divide a vector by a scalar (a single number), it will dissolve every object in the vector by the scalar: c(1, 2, 3) / 3 ## [1] 0.3333333 0.6666667 1.0000000 Meanwhile, dividing a vector by a vector will divide each element in turn - so the first element of the first vector will be divided by the first element of the second, and so on: c(1, 2, 3) / c(1, 2, 3) ## [1] 1 1 1 (For more information on matrix algebra, click here. More information on matrix algebra implementation in R may be found here) That’s about as much matrix algebra as we have to worry about for this book, luckily enough. Let’s go back to our earlier example, where we defined three different vectors: c(1, 2, 3) ## [1] 1 2 3 c(18, &quot;ESF&quot;, 98) ## [1] &quot;18&quot; &quot;ESF&quot; &quot;98&quot; c(&quot;ESF&quot;, &quot;Acorns&quot;, &quot;Stumpies&quot;) ## [1] &quot;ESF&quot; &quot;Acorns&quot; &quot;Stumpies&quot; Look at the difference between that first and that second output - see how the numbers are in quotes the second time around? While R is capable of holding a lot of different types of data, a single vector isn’t. A vector has to either be a numeric or a character vector - it’s either numbers or symbols. This matters, because you can’t do math with symbols. For instance, if we divide the first vector by 3: c(1, 2, 3) / 3 ## [1] 0.3333333 0.6666667 1.0000000 Meanwhile, if we tried to divide our second vector: c(18, &quot;ESF&quot;, 98) / 3 # Error in c(18, &quot;ESF&quot;, 98)/3 : non-numeric argument to binary operator We get our first error message of the course. I should mention that I lied a little bit - vectors can also be a third type, logical. If a vector is made up of TRUE and FALSE elements - and no other elements - then the vector is considered logical. Logical values are exactly what they sound like - they return TRUE if something is true, and FALSE if not. For instance: 6 &gt; 4 ## [1] TRUE 4 &gt; 6 ## [1] FALSE (The &gt; and &lt; symbols are also a type of operator, known as a logical operator. These compare two values and return a logical value of either TRUE or FALSE. You can find a full list of operators at this link.) Now, the reason I only lied a little is that R understands logical values as binary values - TRUE is 1 and FALSE is 0. For instance: TRUE == 1 ## [1] TRUE FALSE == 0 ## [1] TRUE TRUE + 2 ## [1] 3 18^FALSE ## [1] 1 By the way, see how I used == to prove two things were equal? In R, = does something completely different than == - it assigns a value, which we’ll get into in unit 2. For now, just know that you should always use == to check if two values are equivalent. Still, vectors can only hold one type of data - a vector with a value of TRUE and a value of ESF will become a character vector, while c(TRUE, 18, 2) is a numeric vector. If you need to hold more than one type of data, you need a table - or, as they’re called in R, a dataframe. It is possible to make dataframes by hand in R - note, you can press SHIFT+Enter to start a new line: data.frame(x = c(1, 2, 3), y = c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;), z = c(TRUE, TRUE, FALSE)) ## x y z ## 1 1 a TRUE ## 2 2 b TRUE ## 3 3 c FALSE However, this is rare - most of the time, your data will be far too big to make inputting it in R make any sense. Instead, you’ll have to import it from a file elsewhere on your computer - but we’ll get to that later. You’ll see me building basic dataframes using this format throughout this book. I’ll often refer to these as df - the accepted shorthand for dataframe. By the way, if you make a typo, you can press the UP arrow to load the last line of code you sent to R - don’t worry about retyping everything! 1.8 The iris Dataset What’s very cool for our purposes is that R comes preloaded with a number of different datasets. Now, if you just type in the name of the dataset, you might overwhelm R for a moment - it will print out every single row of that dataset, no matter how long it is. Luckily for us, the head() command lets us see just the first few rows of the data. If we use the dataset iris (included in base R), for instance, we’d get the following result: head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa Note that while the default is to print six rows, you can choose how many rows to print by specifying n = ## in the head() function. You can even call the last few rows of your dataset by using the similar function tail() - for instance: tail(iris, n = 3) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 148 6.5 3.0 5.2 2.0 virginica ## 149 6.2 3.4 5.4 2.3 virginica ## 150 5.9 3.0 5.1 1.8 virginica Pretty neat! 1.9 Graphing with R This dataset contains measurements on 150 different irises, and is the data that was used to develop the first ever linear regressions. Even if you aren’t a stats nerd, this dataset lets us do some cool things. For instance, we can see that there are columns containing information on sepal length and sepal width - I wonder if they might be correlated? What would happen if we asked R to plot() them for us? Here, we’re going to use the $ operator for the first time - we’ll go over what exactly it does in unit 2, but for now, just know it’s how we specify which columns we want the plot() function to use: plot(iris$Sepal.Length, iris$Sepal.Width) As you can see, R has some basic plotting functions built right in. However, these plots can be pretty complex to fuss with - and it seems like this plot needs fussing, as right now it claims sepal width and length are unrelated! Luckily, there are better tools for creating graphics in the modern era. The next chapter will cover how to use those to create effective graphs for data exploration and communication. 1.10 Exercises 1.10.1 Calculate the following: The product of 9 * 9 9 squared The remainder from dividing 27 by 2 The remainder of 27 divided by 2, divided by 2 FALSE divided by TRUE. Why did you get this answer? "],
["visualizing-your-data.html", "2 Visualizing Your Data 2.1 What is a Visualization? 2.2 The Tidyverse Package 2.3 ggplot2 2.4 Diamonds 2.5 Other Popular Geoms 2.6 Designing Good Graphics 2.7 Saving Your Graphics 2.8 More Resources 2.9 Exercises", " 2 Visualizing Your Data 2.1 What is a Visualization? A data visualization is a method of representing data in a graphical format, useful both in communicating results of analyses and in exploring datasets to determine what analyses might be appropriate. At the end of the last chapter, we used R’s built in plot() function to make this graphic: plot(iris$Sepal.Length, iris$Sepal.Width) This is our first data visualization in R! Unfortunately, it has some obvious challenges - the axis labels are unhelpful, there’s no context as to what the graph is, and to be honest it’s a little bit ugly. If we wanted to, we could spend some time cleaning up this graphic and making it better: plot(iris$Sepal.Length, iris$Sepal.Width, xlab = &quot;Sepal length (cm)&quot;, ylab = &quot;Sepal width (cm)&quot;, pch = 19, las = 1, col = &quot;deepskyblue4&quot;) But doing anything much more than this gets very complicated very quickly. For this reason, I almost never use the base graphing functions in R - they’re just so complicated to use! 2.2 The Tidyverse Package Thankfully enough, R has a ton of add-on softwares - called packages - which make graphing (and many other tasks) with R significantly easier. Let’s install some of the most common ones now: install.packages(&quot;tidyverse&quot;) library(tidyverse) ## Registered S3 methods overwritten by &#39;ggplot2&#39;: ## method from ## [.quosures rlang ## c.quosures rlang ## print.quosures rlang ## Registered S3 method overwritten by &#39;rvest&#39;: ## method from ## read_xml.response xml2 ## ── Attaching packages ─────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.1.1 ✔ purrr 0.3.2 ## ✔ tibble 2.1.1 ✔ dplyr 0.8.0.1 ## ✔ tidyr 0.8.3 ✔ stringr 1.4.0 ## ✔ readr 1.3.1 ✔ forcats 0.4.0 ## ── Conflicts ────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() Note the quotes around “tidyverse” when you go to install it, but not when it’s inside of library(). The reason for this is a little complicated - basically, you don’t use quotes for things that are inside of R’s memory, like data, functions, and packages. You use quotes for everything else. If you get an error saying “no package named tidyverse”, try reinstalling the package. It might take a few minutes to load. What we just did was install a package called the tidyverse (with install.packages), and load it using library. Most common problems in R have already been solved by someone else, and most of those people have made their work publicly available for others to use in the form of a package. Packages only have to be installed once to be used - but you’ll have to call them using library() each time you restart R. The tidyverse is a pretty unique example of a package - it actually contains six packages, most of which are essential to using R like a modern professional. The most important one for us right now is called ggplot2. Don’t worry about having to load it - library(tidyverse) automatically loads this package for you. 2.3 ggplot2 ggplot is an attempt to extend R’s basic graphics abilities to make publication-quality graphics faster and easier than ever before. In fact, we can make a version of our scatterplot above, just by typing: ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)) + geom_point() Remember, R is case sensitive! There are five important steps that went into making that graph: First, the ggplot() call tells R that we want to create a ggplot object Second, the data = iris tells ggplot that everything we do should use the iris dataset Third, the aes() specifies the aesthetics of the graph - what goes on the X and Y axes, but also any other data we want represented in our plot Fourth, the + lets us add additional steps to our plot. Note that the + must always be at the end of a line - putting it at the start of a line will mess up your session! If you see a + in the console instead of a &gt; after trying to plot something, this is most likely what happened - press your escape key to exit the command. Finally, the geom tells ggplot what sort of graph we want. A geom is just the type of plot (or, well, the geometric object which represents data) - so geom_boxplot() generates a boxplot, while geom_col() makes a column chart. geom_point generates a scatterplot, but there are plenty of other options to choose from!x 2.3.1 Functions in ggplot The ggplot() and geom_point calls are known as functions - a type of R object that, when given certain parameters, gives a certain output. Those parameters - in this plot, our data =, x =, and y = calls - are known as arguments. Each of these arguments can have different values, if we want to change our graph. For instance, if we wanted to color and add a trendline for each species of iris, we could do the following: ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) + geom_point() + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; geom_smooth() adds a trendline to your graphs, with a shadow representing the 95% confidence interval around it. While some people refer to this as a line graph, it’s a separate thing entirely - a line graph connects the points, like this: ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) + geom_point() + geom_line() For now, we’re going to stick with our pretty smoothed trendline. ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) + geom_point() + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Notice how our points here are a little hard to see, as they’re drawn under the line and shading? That’s because we call geom_smooth() after geom_point(), and ggplot adds things to the graph in the order we call them. If we wanted to make the points slightly easier to see, we could just flip the order we call the functions: ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Our graph makes a lot more sense now - sepal length and width seem to be correlated, but each species is different. 2.3.2 Changing Aesthetics If we really wanted to, we could make other aesthetics also change with Species: ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) + geom_smooth(aes(linetype = Species)) + geom_point(aes(size = Species, shape = Species)) ## Warning: Using size for a discrete variable is not advised. ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; But that’s pretty ugly. We’ll get into graph best practices a little bit further into the unit - but generally speaking, a graph should contain exactly as much as it takes to get your point across, and no more. One aesthetic per variable is usually enough. In an important exception to that rule, it’s generally well advised to use different shapes and colors at the same time. Colorblind viewers may not be able to discern the different colors you’re using - so varying the shape of your points or type of your lines helps make your graphics more accessible to the reader. If you want, you can specify shapes using scale_shape functions, such as scale_shape_manual(). There are 25 shapes available for use in ggplot, each of which is named after a number - the number to the left of the shape in the figure below: So if we wanted, we could specify shapes for each species in our dataset pretty easily! I’ve done so below. I’m also going to control the colors by hand - R has a ton of colors available, and you can go crazy picking the best colors for a graph. You can also specify colors by using hex codes (e.g., &quot;#FFFFFF&quot;), but be warned that you might not get an exact match of what you were looking for - R will match as closely as it can from the colors it has available. ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) + geom_point(aes(shape = Species), size = 3) + scale_shape_manual(values = c(16, 17, 18)) + scale_color_manual(values = c(&quot;purple&quot;, &quot;black&quot;, &quot;orange&quot;)) I also made the points a little bigger by specifying size = 3 - note that it isn’t in the aesthetics function, because it doesn’t care about any of the data. Things only go inside aes() if they’re responsive to changes in the data you’re graphing. We can also vary the type of line that gets drawn when we use geom_smooth. This one only has six options, each of which has both a number and a name: You can manually specify linetypes with scale_linetype functions, similar to what we did with shapes. You can use either the names or the numbers - just make sure that the names go inside of quotes, while the numbers don’t! I’m going to make our same graph again, manually controlling the linetypes. I’m also going to get rid of that shadow - it represents the 95% confidence interval around the line (which we’ll discuss more in our statistics section), as identified via standard error. We can turn it off by setting se = FALSE in the geom_smooth() function call. ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) + geom_smooth(aes(linetype = Species), size = 1, se = FALSE) + scale_color_manual(values = c(&quot;purple&quot;, &quot;black&quot;, &quot;orange&quot;)) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;dashed&quot;, &quot;twodash&quot;)) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; We can also combine both graphs into one, more useful graphic: ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) + geom_smooth(aes(linetype = Species), size = 1, se = FALSE) + geom_point(aes(shape = Species), size = 3) + scale_shape_manual(values = c(16, 17, 18)) + scale_color_manual(values = c(&quot;purple&quot;, &quot;black&quot;, &quot;orange&quot;)) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;dashed&quot;, &quot;twodash&quot;)) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Nifty! Hopefully you’re getting a sense of how ggplot allows you to build a graph by addition, as it draws whatever functions you’ve called in turn. Note, by the way, that I’ve put aes() calls in both the ggplot() and geom functions. Geoms inherit from the ggplot() call - they’ll use whatever data and aesthetics are specified inside the parenthesis. However, if you want an aesthetic to only apply to one geom, you can put it inside that geom() call. This is pretty commonly used when an aesthetic only applies to one geom - for instance, our geom_smooth() can’t take a shape =. You have to be careful with this power, though! Sometimes, defining geom-specific aesthetics will give you misleading or simply wrong visualizations. For instance, what would happen if we draw our lines based on the petal length of each species, rather than the sepal width? ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) + geom_point(aes(shape = Species), size = 3) + geom_smooth(aes(y = Petal.Length, linetype = Species), size = 1, se = FALSE) + scale_shape_manual(values = c(16, 17, 18)) + scale_color_manual(values = c(&quot;purple&quot;, &quot;black&quot;, &quot;orange&quot;)) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;dashed&quot;, &quot;twodash&quot;)) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Our plot makes no sense! Lots of beginners are tripped up by this when they’re starting - a common assumption is that ggplot will add a second y-axis to the right hand of the plot. In reality, there is no way to graph two y-axes (of different values) on the same ggplot graph - and that’s on purpose, as dual y-axis plots are almost always misleading. It’s almost always better to just have two graphs next to each other, if you need to compare the data - though the linked article contains some other interesting suggestions. Anyway, thinking back to our other graphic: This graph is nice, but I think it could be even nicer. Specifically, there’s a lot of overlap between the versicolor and virginica species - it would be nice to see them side by side, rather than on the same plot. 2.3.3 Facetting Luckily, ggplot makes this easy for us via what’s known as facets. By adding facet_wrap() to our plot, we’re able to split the three species onto their own graphs, while keeping the axes standardized. ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(size = 3) + geom_smooth(size = 1, se = FALSE) + facet_wrap(~ Species) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; That makes seeing the differences much easier! Note that I got rid of the different species aesthetics - now that the species are each on their own plot, each species having a different color and shape doesn’t add any information to the visualization. facet_wrap() is very useful, in that it will automatically wrap our plots into however many rows and columns are required. If we want to be a little more specific in how our data is arranged, however, we can use facet_grid(). By specifying either rows = or cols =, we can finely control how our data is split: ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(size = 3) + geom_smooth(size = 1, se = FALSE) + facet_grid(rows = vars(Species)) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Heck, if we have two groups we want to compare, we can use both rows = and cols = at the same time! Unfortunately, iris doesn’t have two grouping variables in it - so I’m going to make another one (color): iris2 &lt;- iris iris2$color &lt;- rep(c(&quot;purple&quot;,&quot;red&quot;,&quot;black&quot;), 50) head(iris2) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species color ## 1 5.1 3.5 1.4 0.2 setosa purple ## 2 4.9 3.0 1.4 0.2 setosa red ## 3 4.7 3.2 1.3 0.2 setosa black ## 4 4.6 3.1 1.5 0.2 setosa purple ## 5 5.0 3.6 1.4 0.2 setosa red ## 6 5.4 3.9 1.7 0.4 setosa black As you can see, I’ve told R to replicate (or repeat, as I’ve always thought of it) the vector of purple, red, black 50 times - so about a third of each species will be in each color. Using that as our column grouping gives us: ggplot(data = iris2, aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(size = 3) + geom_smooth(size = 1, se = FALSE) + facet_grid(rows = vars(Species), cols = vars(color)) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = ## parametric, : pseudoinverse used at 5.2 ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = ## parametric, : neighborhood radius 0.2 ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = ## parametric, : reciprocal condition number 0 2.4 Diamonds 2.4.1 Visualizing Large Datasets For this next exercise, we’re going to be using the diamonds dataset, which contains data about 54,000 different diamond sales. It looks like this: head(diamonds) ## # A tibble: 6 x 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 We can plot the price of each diamond against its weight (or carat) pretty easily, using geom_point() like before: ggplot(diamonds, aes(carat, price)) + geom_point() Note that I’ve stopped explicitly writing data =, x =, and y =. Without that specification, R assumes that you’re providing arguments to the function in the order the function normally expects them - which, for ggplot(), is in the form ggplot(data, aes(x, y)). Most code you’ll find in the wild is written in this more compact format. Anyway, back to the graph. It’s a bit of a mess! It’s hard to discern a pattern when all 54,000 points are plotted in the same area. We can make things a bit better by making the points transparent, by giving them a low alpha = value: ggplot(diamonds, aes(carat, price)) + geom_point(alpha = 0.05) This is somewhat better! We can see that there’s a correlation between price and carat - but it’s hard to tell exactly what the trend looks like. Plus, there’s a good amount of empty space on the graph, which we could probably make better use of. 2.4.2 Axis Transformations We can consider transforming our axes to solve all these problems. For instance, if we plotted both our axes on log10 scales, we’d get the following graph: ggplot(diamonds, aes(carat, price)) + geom_point(alpha = 0.05) + scale_x_log10() + scale_y_log10() So we can see that, by log-transforming our variables, we get a linear-looking relationship in our data. Now, I’m personally not a fan of log graphs - and you shouldn’t be, either. But you’ll sometimes have data that can’t be properly explained without logarithims - or bosses who won’t listen to reason. As such, it’s worth knowing how to make R plot things exactly as you want it to. Usually, however, it makes a lot more sense to plot your data without any transformations, and just use transformed values as needed in your analyses. We’ll discuss that more in chapters 5 and 6, however. You can perform plenty of other axes transformations by specifying the trans argument inside of your scale function. For instance, if we wanted to use a natural log instead, we could type: ggplot(diamonds, aes(carat, price)) + geom_point(alpha = 0.05) + scale_y_continuous(trans = &quot;log&quot;) + scale_x_continuous(trans = &quot;log&quot;) To learn more about transformations, you can read the documentation by typing ?scale_x_continuous() into the console. 2.5 Other Popular Geoms 2.5.1 Histograms One of the most popular geoms is the histogram, which allows you to quickly visualize the distribution of a numeric value: ggplot(diamonds, aes(price)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Where did count come from? We only specified an x variable! The short answer is that ggplot calculated it by itself! ggplot only needs an x aesthetic to make a histogram - it will calculate the count of each level of the variable and use that as its y. Base R also has a histogram function in hist(), which I occasionally use while making exploratory graphs. It works pretty similarly to the ggplot method, but collapsed into a single function: hist(diamonds$price) 2.5.2 Bar Charts If we wanted to use a categorical value instead of a numeric one on the x-axis, we’d use a bar chart. It’s easy enough to make this in ggplot, using geom_bar(): ggplot(diamonds, aes(x = cut)) + geom_bar() If we wanted to communicate more information with this chart, we could think about what number of each cut type is made up of each clarity level. One way to do that is to map the fill of the barplot to the clarity variable: ggplot(diamonds, aes(cut, fill = clarity)) + geom_bar() Note that we use fill in this case, as we’re defining the color for the inside of the polygon, not the lines themselves. If we used color instead, we’d get something like this: ggplot(diamonds, aes(cut, color = clarity)) + geom_bar() Where only the borders of each polygon are colored. Now, ggplot’s default behavior when given a color or fill aesthetic is to make a stacked bar chart, as shown above. Stacked bar charts are awful. It’s really hard to compare values between bars, because the lower limits aren’t standardized. The one exception is if you’re only comparing two values and all bars sum to the same amount, like so: ## Make a table of x and y values, which are split into two groups by z. ## Each x has a y value for each level of z. df &lt;- data.frame(x = c(1, 1, 2, 2, 3, 3), y = c(40, 60, 30, 70, 20, 80), z = c(&quot;A&quot;,&quot;B&quot;,&quot;A&quot;,&quot;B&quot;, &quot;A&quot;, &quot;B&quot;)) df ## x y z ## 1 1 40 A ## 2 1 60 B ## 3 2 30 A ## 4 2 70 B ## 5 3 20 A ## 6 3 80 B ggplot(df, aes(x, y, fill = z)) + geom_col() Note that I’m using geom_col(), which makes column charts. This lets us define y as values other than the simple count - useful if we’re trying to graph the average value for each group, for instance. This simple stacked bar chart works well enough - it lets you compare the values of both A and B, since the groups share a border at either the top or bottom edge of the plot. For most purposes, though, a somewhat better option is the dodged bar chart: ggplot(diamonds, aes(cut, fill = clarity)) + geom_bar(position = &quot;dodge&quot;) Dodged bar plots are better than stacked bars when comparing more than one value for each item on the x axis of a chart. However, with enough series, dodged bar charts can also be decently confusing - try comparing the I1 values between Premium and Fair on this chart, for instance. 2.5.3 Jittered Points If you have to have this much information in a single graphic, geom_jitter can help. It generates a scatterplot, much like geom_point(), but “jitters” the points by adding statistical noise - making it easy to compare counts between all combinations of the two variables. ggplot(diamonds, aes(cut, clarity)) + geom_jitter(alpha = 0.05) You can use geom_jitter to make regular scatterplots, as well - for instance, we can see more of the points in our original iris scatterplot by adding a little bit of noise to the plot: ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_jitter() + scale_color_manual(values = c(&quot;purple&quot;, &quot;black&quot;, &quot;orange&quot;)) Note that this does actually move where your points are placed on the graph - if it’s important that a reader can extract actual values from your graphic, don’t jitter your points! That being said, in most cases where a reader needs specific numbers from your dataset, a table is usually better fit to your needs. 2.5.4 Boxplot The last main plot type we’ll go over is the boxplot. This is mostly used to show the distribution of data - it draws a plot with a line at the data’s median, box borders at the 25% and 75% values, and lines reaching to the 5% and 95% values. ggplot(diamonds, aes(cut, price)) + geom_boxplot() There are a lot of other things you can do with ggplot that we won’t go over here - you can find cheatsheets on the package here, and read more documentation here. Note that you can’t make pie charts with ggplot. You usually shouldn’t be using a pie chart anyway, but we’ll go over this in unit 8. 2.6 Designing Good Graphics These earlier charts are good examples of graphs we might use for data exploration, early in an analysis, to see what patterns we might be able to identify and tease out through further work. They aren’t, however, polished graphs ready for use in presentations. Graphics for communication require a little more attention to the principles of design. Graphics for communication, at their essence, exist to make arguments and communicate your point. In order to do that, a graphic has to be both visually clean and easily understood, while at the same time containing exactly enough information to get a point across - and nothing more. Learning how to make graphics like this is a skill unto itself, and should be a skill you pay attention to even after finishing this unit. After all, it doesn’t matter how smart you are and how perfect your analysis is if you aren’t able to tell anyone about it afterwards! The hard part about teaching graphic design is that it’s as much an art as a science - there is no one right way to make compelling graphics. What I’m going to teach in this section is as much my opinion as it is the correct way to do things - other textbooks and other people have their own preferred methods, none of which are inherently better or worse. For instance, ggplot comes with a number of preinstalled themes which you can add to any given plot. For a complete list, click here. We’ll just demo a few of the most common ones, using our old iris scatterplot: ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point() + scale_color_manual(values = c(&quot;purple&quot;, &quot;black&quot;, &quot;orange&quot;)) + theme_bw() ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point() + scale_color_manual(values = c(&quot;purple&quot;, &quot;black&quot;, &quot;orange&quot;)) + theme_minimal() ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point() + scale_color_manual(values = c(&quot;purple&quot;, &quot;black&quot;, &quot;orange&quot;)) + theme_classic() Plenty of other packages introduce other ggplots for you to use. My personal favorite is cowplot. Written by Claus O. Wilke, it provides some really interesting new extensions to ggplot, and sets the default theme to something that generally looks better than ggplot’s defaults. If we install it now: install.packages(&quot;cowplot&quot;) And then load it: library(cowplot) ## ## Attaching package: &#39;cowplot&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## ggsave ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point() + scale_color_manual(values = c(&quot;purple&quot;, &quot;black&quot;, &quot;orange&quot;)) This default is pretty similar to theme_classic(), except with different font sizes. However, if we add background_grid() to our plot: ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point() + scale_color_manual(values = c(&quot;purple&quot;, &quot;black&quot;, &quot;orange&quot;)) + background_grid() We get what I consider to be the nicest looking default option R will give you. If we want to override the default axis names, we can control that with labs(): ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point() + scale_color_manual(values = c(&quot;purple&quot;, &quot;black&quot;, &quot;orange&quot;)) + background_grid() + labs(x = &quot;Sepal Length&quot;, y = &quot;Sepal Width&quot;) With labs, we can also give our graphs a title and caption. This is generally a bad idea - if you’re going to include a graph in a report or publication, you’ll want to typeset these outside of the image file - but it makes understanding these graphs a little easier. ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point() + scale_color_manual(values = c(&quot;purple&quot;, &quot;black&quot;, &quot;orange&quot;)) + background_grid() + labs(x = &quot;Sepal Length&quot;, y = &quot;Sepal Width&quot;, title = &quot;Sepal Width as a Function of Sepal Length&quot;, subtitle = &quot;Data from R. A. Fischer&#39;s iris dataset, 1936&quot;, caption = &quot;Made in R with ggplot2&quot;) If we want to change anything about the theme (for instance, the text size or legend position), we can specify that in theme(): ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point() + scale_color_manual(values = c(&quot;purple&quot;, &quot;black&quot;, &quot;orange&quot;)) + background_grid() + labs(x = &quot;Sepal Length&quot;, y = &quot;Sepal Width&quot;) + theme(text = element_text(size = 12), axis.text = element_text(size = 10), legend.position = &quot;top&quot;) And we can keep specifying what we want until we’re satisfied with our graph. ggplot will also let us focus on specific parts of the data: ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point() + scale_color_manual(values = c(&quot;purple&quot;, &quot;black&quot;, &quot;orange&quot;)) + background_grid() + labs(x = &quot;Sepal Length&quot;, y = &quot;Sepal Width&quot;) + scale_x_continuous(limits = c(5, 7)) + scale_y_continuous(limits = c(2.5, 3.0)) ## Warning: Removed 93 rows containing missing values (geom_point). Of course, if you’re graphing things such as percentages, you should be careful about where you set your axes. Say we had a dataset where every 1 increase in some variable x saw a 1% increase in y, so that y increased almost 10% over the course of all x values. If you let ggplot set your axis defaults, you’d wind up with a perfect correlation: df &lt;- data.frame(x = 1:10, y = 61:70) ggplot(df, aes(x, y)) + geom_line() + background_grid() However, it’s probably more truthful to graph percentages on a 0-100 scale - doing so shows us that x has a weaker impact on y than the default would have us believe: ggplot(df, aes(x, y)) + geom_line() + background_grid() + scale_y_continuous(limits = c(0,100)) If there’s any part of your graph you want to change, try googling “change XX ggplot”. The first link will almost certainly have what you’re looking for. The reference guide is also a good place to go hunting for answers. 2.7 Saving Your Graphics When you’re satisfied with your graph, simply call the ggsave() function to save it to whatever file you’re working in. The first argument to this function should be your graph’s desired file name, with the extension - ggplot can save graphs as pngs, jpegs, pdfs, and several other formats. You can either add it to your workflow with +, or call it after you’ve plotted your graph - ggsave() will save whatever image was drawn last. For more information on specific things ggsave can do, type ?ggsave() into R. 2.8 More Resources In addition to the ggplot documentation, I highly reccomend the ggplot book. Additionally, almost any problem can be solved by googling - just put “ggplot” at the end of whatever your question is, and odds are you’ll find the perfect solution. 2.9 Exercises 2.9.1 Graph the following: A boxplot of the iris data, with species on the x axis and sepal length on the y A scatterplot of the iris data, plotting sepal length against width, where the points get bigger as sepal width increases Can you change the color of the boxes in the graph you made for problem 1? Can you change the color of the lines? 2.9.2 Use a new dataset: Also included in ggplot is the mpg dataset, containing fuel economy data for 38 different models of car. Use head() to examine the data. You can also type ?mpg to get documentation on what each variable represents. Is engine displacement (displ) correlated with highway miles per gallon (hwy)? Make a scatterplot to find out. What variables could we use to group the data? Does coloring points by any of these help explain the scatterplot from problem 2? What does the scatterplot look like if you make a scatterplot for cty plotted against hwy? Why? What geom could we use to better represent the data? 2.9.3 Looking ahead: What happens if you type in summary(mpg)? What do you think this output represents? What happens if you type in mpg[1]? How does this compare to mpg[[1]]? "],
["r-functions-and-workflow.html", "3 R Functions and Workflow 3.1 Workflow 3.2 Memory, Objects, and Names 3.3 Dataframes 3.4 Oddballs 3.5 R Studio Tips and Tricks 3.6 R Functions and Workflow Exercises", " 3 R Functions and Workflow Don’t worry if it doesn’t work right. If everything did, you’d be out of a job. — (Mosher’s Law of Software Engineering) 3.1 Workflow Now that we’re data visualization pros, it’s time to start building our analytic skills. While the tools used in the actual analyses are discussed in the next several chapters, there are a few tools involved in basic R usage that we need to cover before we can start in on any of those topics. This chapter should show you how to save, load, and execute your code effectively, and get you ready for the more extended analyses we’ll be doing next chapter 3.1.1 Scripts So far, we’ve been using the command line interface in the console to type our programs. While this works, you might have noticed how annoying it can be to type longer programs in. Additionally, you’re probably going to want to save your work at some point - and right now, you’d have to use Notepad or a similar program to save anything you’ve done. Luckily, there’s a better way. In the top left corner of R Studio, there’s a menu button called “File”. Click this, then click “New Project”. If you click “New Directory”, and then “New Project”, you’ll be able to create a folder where you can automatically store all of your R code and files. This will also create an R Project file, which you can load to return to where you left off the last time you closed R Studio. Let’s load the tidyverse again, now that we’re in a new directory: library(tidyverse) ## Registered S3 methods overwritten by &#39;ggplot2&#39;: ## method from ## [.quosures rlang ## c.quosures rlang ## print.quosures rlang ## Registered S3 method overwritten by &#39;rvest&#39;: ## method from ## read_xml.response xml2 ## ── Attaching packages ─────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.1.1 ✔ purrr 0.3.2 ## ✔ tibble 2.1.1 ✔ dplyr 0.8.0.1 ## ✔ tidyr 0.8.3 ✔ stringr 1.4.0 ## ✔ readr 1.3.1 ✔ forcats 0.4.0 ## ── Conflicts ────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() Now that you’re working in a new directory, go back into “File” and hover over “New File”. There’s a lot of options, but right now we care about two of them: R Scripts and R Notebooks. Open one of each. In your new script file, type the following: ggplot(iris, aes(Petal.Length, Petal.Width)) + geom_point() Highlight everything and then press Cmd/Ctrl and Enter at the same time. A graph should appear in your Viewer window. Whoops, looks like we forgot to color the points by species - add the color aesthetic to your plot. It should already be clear what the advantage of working with R Scripts is - you can change pieces of your code quickly, without having to worry about retyping things into the console. You can also save and open your scripts (Cmd/Ctrl+S, Cmd/Ctrl+O), which makes working on big projects much easier. Now change your code so it looks like this: a &lt;- ggplot(iris, aes(Petal.Length, Petal.Width)) + geom_point(aes(color=Species)) What we’re doing here is assigning the plot to a. Now, anytime you call a, the plot will appear - run your script, then call “a” from the console! a Now add the following line under the first two: a + theme(legend.position = &quot;top&quot;) This will move the legend to the top of our graph, much as if we had included the theme() function in our original plot. Run your script to see the difference! Your program now has - for the first time in this book - two steps to it: the assignment step (where we make a a plot), and the print step (where we add our theme() and print the plot). While in an R script, there are three ways you can run the whole program: Click the very top of the document and press Cmd/Ctrl+Enter once for each step Highlight the entire code and press Cmd/Ctrl+Enter to run it all at once While your cursor is anywhere in the script, press Cmd/Ctrl+Shift+Enter to run the whole program at once That last method is usually the fastest and easiest. 3.1.2 Notebooks While scripts are great, they do have some drawbacks. For instance, if you have to do more major and slower tasks - like loading datasets and libraries, or complicated math - you’ll have to redo that step every time you want to run the whole program, which is a pain. Also, running a script pulls up the console window, which is a little bit of a headache. For that reason, I tend to work in R Notebooks. Open your new notebook file, and you’ll see a new welcome page! The welcome page has a lot of good information in it - you can delete everything after the second set of three dashes once you’ve read it. Inside a notebook, you can make chunks by pressing Cmd/Ctrl+Alt+I. These chunks run as individual scripts, which you can run the exact same way by using combinations of Cmd/Ctrl, Shift, and Enter. Note, though, that your code must be inside these grey chunks to run - anything in the white space outside chunks will be interpreted as plain text by R! This is a feature, not a bug - it will let you explain why you wrote each piece of code, and record your thoughts at the time you were working - but is a common area for beginners to mess up. Using notebooks can be a little more efficient than scripts, because it offers you the ability to split your code steps into multiple pieces, which can let you iterate on an idea faster than using scripts alone. No matter which you prefer, you should aim to have one script or notebook per task you perform - don’t just have one long, continuous notebook for everything you’re doing. Also, make sure you give everything a descriptive name - there’s nothing worse than needing a file a month or so later and having to open every notebook you’ve ever made to find it! It’s also a good idea to make a new R Project, in a new folder, for each major project you start in on. These sorts of things might not matter too much to you while you’re learning - but once you’re doing more complicated things with R, having good habits like these are essential. The rest of this book will assume you’re working using notebooks. While all of these tasks may be done in scripts (or, if you really hate yourself, the console), I think that using notebooks makes much more sense if you’re interested in using R like a professional. 3.2 Memory, Objects, and Names Let’s go back to when we assigned a plot to a: a &lt;- ggplot(iris, aes(Petal.Length, Petal.Width)) + geom_point(aes(color = Species)) The &lt;- symbol is the assignment operator. We can use it to define the object a as all sorts of different objects: # Assign a the value 10 a &lt;- 10 # Print out the object a a ## [1] 10 a &lt;- c(1,50,200) a ## [1] 1 50 200 a &lt;- &quot;Hello, world!&quot; a ## [1] &quot;Hello, world!&quot; a &lt;- geom_point(data = iris, aes(Petal.Length, Petal.Width, color = Species)) ggplot() + a As you can see, every time we use &lt;-, we completely overwrite what was there before. This is part of the power of R, as it lets us update data and functions as needed - but at the same time it can be dangerous, as it can let us overwrite things we actually need! For instance, R won’t stop you from redefining functions (like ggplot()), operators (like +) or even numbers (like 1) as you wish - which might really screw things up later! You’ll notice that a is now listed in the upper-left hand corner of R Studio, under the “Environment” tab. That’s because a is now defined in memory - we can use it in any of our code, anywhere we want. In fact, you can even define a in one file and call it in another, so long as you’ve already run the code defining it in your current R Studio session. This is really cool for a lot of reasons - it lets us do more complicated things with R - but can also cause some problems. If you keep defining objects with names like a, it’s easy to forget which variable stands for what - and so you can wind up making mistakes when using those variables later on. For instance, we just overwrote a 3 times in that last example - imagine if we had important data stored in there! In order to avoid that sort of confusion, you should use descriptive names when creating objects. You should also decide on a standard way you’re going to format those object names - some people prefer snake_case_names, others use.periods, and I personally prefer what’s known as CamelCase. Different organizations and groups have different preferred styles (here’s Google’s), but what’s important right now is that you pick a style that makes sense to you. Be consistent using this style whenever you code - R won’t understand you if you mess up your capitalization! By the way - you might remember that I mentioned last unit that = could also be used as an assignment operator. That’s true, but you should try to never do it - it makes your code much harder to understand - for instance, compare these two formats: a &lt;- 10 a &lt;- a + 1 b = 10 b = b+1 a == b ## [1] TRUE R’s telling us that these two formats do exactly the same thing - define a variable as 10, and then overwrite the variable as the original value, plus one. But the top block of code makes a bit more sense than the lower one, which at a glance looks like we’re trying to test if b is equal to b + 1, which is nonsense. This is just one of the reasons it’s usually better to use &lt;- for assignments. 3.3 Dataframes Earlier in this course, we went over the different classes of vectors - character, numeric, and logical. If you’re ever trying to find out what class a vector belongs to, you can call the class() function: SampleVector &lt;- c(1,2,3) class(SampleVector) ## [1] &quot;numeric&quot; Note that we don’t put object names (such as the name of our vector) in quotes. The general distinction is that if something exists in the global environment, we don’t put it in quotes. If it isn’t, we do. You can see what’s in the current environment by looking at the “Environment” tab that I mentioned earlier - that tab is a list of all the objects you’ve defined so far in this session. Remember that even though your installed packages aren’t in that list, you still don’t put them in quotes when you call library(). A matrix made of vectors is known, in R, as a dataframe. We’ve already seen some simple dataframes in the past unit built using data.frame: data.frame(x = c(1,2,3), y = c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;), z = c(TRUE, TRUE, FALSE)) ## x y z ## 1 1 a TRUE ## 2 2 b TRUE ## 3 3 c FALSE This is an example of something known as rectangular data - the sort you’re likely to find in spreadsheets and many, if not most, business and scientific applications. We’ll be dealing with rectangular data almost exclusively in this course - while non-rectangular data is useful in many applications, it’s much harder to get started with. 3.4 Oddballs Predict what will happen when you run the following code - then run it! sqrt(2)^2 == 2 1/49 * 49 == 1 You’d expect both of these things to be true, but R seems to think otherwise. That’s because R has to estimate the true value of things like 1/49 - it only calculates to so many digits, because it can’t store an infinite number of decimal places. As such, 1/49 * 49 isn’t exactly equal to 1 - it’s just near it. To catch these sorts of things, use near() instead of ==: 1/49 * 49 == 1 ## [1] FALSE near(1/49 * 49, 1) ## [1] TRUE 3.5 R Studio Tips and Tricks There are a number of other functions in R Studio that might make your life as an analyst easier. Here’s a quick rundown of the most common ones: Pressing Ctrl/Cmd + F lets you search through a script or notebook, just like in any word processor. Ctrl/Cmd + Enter will run whatever line of code your cursor is on in a chunk or script. Ctrl/Cmd + Shift + Enter will run the entire chunk or script. Ctrl/Cmd + Alt + P will run all the chunks in a notebook above your cursor. Ctrl/Cmd + Alt + R runs all the chunks in that notebook, period. Ctrl/Cmd + S saves a file, just like you’d expect. Ctrl/Cmd + Shift + S saves every file you have open! F7 runs a spell check for the plain text in your notebook - note that if you don’t run this, your notebook won’t get spell checked at all. And if you’re looking for more shortcuts, Alt + Shift + K will open a window with all the shortcuts available to you. 3.6 R Functions and Workflow Exercises 3.6.1 Do the following: What class is the vector c(1, TRUE, 3)? Why is it not a character vector? Make and print this tibble. What do the abbreviations under each column name mean? tibble(x = c(1, 2, 3), y = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), z = c(TRUE, FALSE, TRUE)) "],
["data-wrangling.html", "4 Data Wrangling 4.1 Thinking with Data 4.2 The Data Analytics Model 4.3 Wrangle 4.4 The Pipe 4.5 Data Transformations 4.6 Missing Values 4.7 Count Data", " 4 Data Wrangling In God we trust. All others must bring data. — W. Edwards Deming 4.1 Thinking with Data At the start of this book, I established two essential aims for what this book would accomplish: The goal is to leave you with the basic essentials of working in R, as well as a strong foundation in thinking like a data analyst that will help you understand how to tackle more complicated problems. So far, we’ve mostly focused on that first goal, which has obvious measurable outcomes - hopefully, you’ve found that working in R is somewhat easier now than when you first started. However, we haven’t yet touched the second, for one specific reason: analytic thinking is much, much harder to do than learning a language. There are fewer strict rules, leaving much more room for judgement calls, and the world won’t throw an error code if you make a mistake. That being said, it’s a critical skill set to develop if you’re looking to use R effectively. It’s important to note that there’s a certain amount of art in data science (heck, that’s a book), and this chapter only reflects a certain model of how that art is best applied. You might come across other frameworks that work better for you. However, if you’re relatively new to working with data, this should be a good starting point for you to work from. 4.2 The Data Analytics Model My personal preferred model is very simple, with four steps: Wrangle your data Explore the data Build models from the data Communicate your results This is a slightly modified version of Peng and Matsui’s model, and it bears resemblance to Grolemund and Wickham’s version. This makes sense - these models are all different ways of describing the same processes, after all. We’re going to dedicate a chapter to each of these steps, starting from the top with data wrangling. I’d also like to note that these steps assume you’ve already decided what problem you’re looking to solve and how you might go about it. Despite the best promises of pundits and pushers worldwide, Big Datatm cannot solve every challenge in the world, and just poking around any dataset you’ve been given might stir up some incorrect or misguided conclusions. 4.3 Wrangle However, if you know what problem you’re looking to solve, your first step is gathering the right data to start identifying possible solutions. I’m not going to pretend that I can speak to what that gathering might look like for you - the steps you’ll go through to get your data are much different if you’re working in sciences versus the corporate world, and differ greatly even between various disciplines. What I can say, though, is that most data found in the wild contains a number of features that make it harder to analyze with computer software. Your data might be stored in a weird format, have missing or incorrect values, or be so unstructured that computer analyses don’t understand how to parse it - for instance, this is how a lot of text data is originally gathered. We’ll generally be working with structured data throughout this course - the sort of data you could put into a spreadsheet - but there are plenty of resources on how to deal with unstructured data if that’s an area you find yourself interested in. However, even our structured data will often not be in the ideal format for our desired analysis. As such, we have to spend our time wrangling the data (sometimes referred to as munging, though that term is falling out of favor) to get it into the shape we want it. This is often the most time consuming part of your work - estimates vary, but wrangling usually makes up 70-80% of the time spent on an analysis! Luckily enough, there are a number of versatile tools designed for wrangling our data which will take a lot of the trouble out of the process. 4.3.1 Tidy Data I mentioned earlier that we’d be primarily working with structured data, like you could put into a spreadsheet. In fact, we’ll be working with one specific type of structured data, known as rectangular data. This is the term used for that spreadsheet-esque data format, where data is neatly kept in columns and rows. There are other, more complex data structures that we could use, but we won’t need them in this course. Instead, in this course, we’ll almost always be working with a very specific type of rectangular data known as tidy data. Tidy dataframes always take the same shape: ## Registered S3 methods overwritten by &#39;ggplot2&#39;: ## method from ## [.quosures rlang ## c.quosures rlang ## print.quosures rlang ## Registered S3 method overwritten by &#39;rvest&#39;: ## method from ## read_xml.response xml2 ## ── Attaching packages ─────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.1.1 ✔ purrr 0.3.2 ## ✔ tibble 2.1.1 ✔ dplyr 0.8.0.1 ## ✔ tidyr 0.8.3 ✔ stringr 1.4.0 ## ✔ readr 1.3.1 ✔ forcats 0.4.0 ## ── Conflicts ────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## Warning in kableExtra::kable_styling(.): Please specify format in kable. ## kableExtra can customize either HTML or LaTeX outputs. See https:// ## haozhu233.github.io/kableExtra/ for details. . Variable_1 Variable_2 Variable… Variable_n Observation 1 Value Value Value Value Observation 2 Value Value Value Value … … … … … Observation n Value Value Value Value Tidy data is organized as follows: Each column is a single variable Each row is a single observation Each cell is a single value You can’t expect most data you encounter in the wild to already be tidy - but that’s why data workers make the big bucks. Luckily enough, there are tools designed to get you from untidy to tidy data easily, so you can then follow up with the fun parts of analyses. We’ll be working 4.3.2 Tidying Data As you might guess from the name, the tidyverse is specifically designed to work with tidy datasets. Let’s load it now: library(tidyverse) By storing all data in a tidy format, we’re able to quickly apply the same sets of tools to multiple different types of data. For instance, imagine a dataframe of seasonal temperatures, built as such: SeasonalTemps &lt;- data.frame(Year = c(2015, 2016, 2017, 2018), Winter = c(40, 38, 42, 44), Spring = c(46, 40, 50, 48), Summer = c(70, 62, 81, 76), Fall = c(52, 46, 54, 56)) SeasonalTemps ## Year Winter Spring Summer Fall ## 1 2015 40 46 70 52 ## 2 2016 38 40 62 46 ## 3 2017 42 50 81 54 ## 4 2018 44 48 76 56 (By the way, if you’re working in notebooks, I’d suggest typing each of these blocks of code into their own chunk - press Ctrl/Cmd + Alt + I in order to quickly add a new code chunk to your document.) This dataframe makes some sense - it’s pretty easy to understand as a human reader, and would probably be a good layout for a printed table. But the problems with this format become obvious when we, for instance, try to graph the data: ggplot(SeasonalTemps, aes(x = Year)) + geom_line(aes(y = Winter), color = &quot;purple&quot;) + geom_line(aes(y = Spring), color = &quot;green&quot;) + geom_line(aes(y = Summer), color = &quot;blue&quot;) + geom_line(aes(y = Fall), color = &quot;red&quot;) What a mess! That took far too long to type - a good general rule of thumb is that if you have to repeat yourself more than twice to do something, there’s a better way to do it. And, even after all our effort, our graph doesn’t have a legend, and the Y axis is labeled wrong. This is a good time for us to use those tools I mentioned earlier, to turn our data tidy! Luckily enough, the tidyverse contains a package designed for making our data tidier - called, helpfully enough, tidyr. We already loaded this package when we called the tidyverse earlier. tidyr provides two essential functions for “reshaping” the data - changing back and forth between the wide format we used above and a long format, easier used by our functions. To change our SeasonalTemps data to a long format, we can use the gather() function. This function gathers values stores in multiple columns into a single variable, and makes another variable - the key variable - representing what column the data was originally in. gather() takes three important arguments: data, the dataframe to gather key, what to name the key column value, what to name the column data was merged into Additionally, we can specify columns that we want to preserve in the new, long dataframe by putting -ColumnName at the end of the function. What this looks like for our seasonal data is something like this: LongTemps &lt;- gather(data = SeasonalTemps, key = Season, value = AvgTemp, -Year) LongTemps ## Year Season AvgTemp ## 1 2015 Winter 40 ## 2 2016 Winter 38 ## 3 2017 Winter 42 ## 4 2018 Winter 44 ## 5 2015 Spring 46 ## 6 2016 Spring 40 ## 7 2017 Spring 50 ## 8 2018 Spring 48 ## 9 2015 Summer 70 ## 10 2016 Summer 62 ## 11 2017 Summer 81 ## 12 2018 Summer 76 ## 13 2015 Fall 52 ## 14 2016 Fall 46 ## 15 2017 Fall 54 ## 16 2018 Fall 56 Note that you don’t have to type data =, key =, and value = - if you don’t, R assumes that you’ve listed the arguments in this order. This format makes graphing significantly easier: ggplot(LongTemps, aes(x = Year, y = AvgTemp, color = Season)) + geom_line() If, after all our hard work, we want to get back to our original wide format, we can undo our gather() using spread(). Again, I’m giving spread a data, key, and value argument - but this time, the function is making a new column for each value of our key: WideTemps &lt;- spread(LongTemps, Season, AvgTemp) WideTemps ## Year Fall Spring Summer Winter ## 1 2015 52 46 70 40 ## 2 2016 46 40 62 38 ## 3 2017 54 50 81 42 ## 4 2018 56 48 76 44 This new dataframe isn’t quite the same as our original - the columns are now in alphabetical order! If we wanted to rearrage them, I find the easiest way is using the select() function from dplyr(), another package in the tidyverse. By giving select() an argument for data and a vector of column names, we can rearrange the order the columns appear: OrderWideTemps &lt;- select(WideTemps, c(Year, Winter, Spring, Summer, Fall)) OrderWideTemps ## Year Winter Spring Summer Fall ## 1 2015 40 46 70 52 ## 2 2016 38 40 62 46 ## 3 2017 42 50 81 54 ## 4 2018 44 48 76 56 When doing this, though, we have to be careful we don’t accidentally forget a column: select(WideTemps, c(Year, Winter, Spring, Fall)) ## Year Winter Spring Fall ## 1 2015 40 46 52 ## 2 2016 38 40 46 ## 3 2017 42 50 54 ## 4 2018 44 48 56 Although, if we wanted to drop a column, we can do so by using a - sign: select(WideTemps, -Summer) ## Year Fall Spring Winter ## 1 2015 52 46 40 ## 2 2016 46 40 38 ## 3 2017 54 50 42 ## 4 2018 56 48 44 4.3.3 Separating Values Another way many datasets break the tidy format is by storing more than one value in a cell. For instance, say we had a dataframe of horse race results from three races, where the results were all written in the same column: df &lt;- tibble(Horses = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), Results = c(&quot;1-2-3&quot;, &quot;3-1-2&quot;, &quot;2-3-1&quot;), TotalMinutes = c(3, 3, 3), TotalSeconds = c(12, 44, 15)) df ## # A tibble: 3 x 4 ## Horses Results TotalMinutes TotalSeconds ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 1-2-3 3 12 ## 2 B 3-1-2 3 44 ## 3 C 2-3-1 3 15 This makes some amount of sense as a human reading it, but makes it really hard to do any sort of analysis. So we can use the separate() command from tidyr to split that Results column into three columns, one per race. separate() needs at minimum four arguments to work properly: data, the dataframe we’re operating on, col, the name of the column we’re splitting up, into, a character vector (that you’d create using c()) of the names you want to use for the new columns, and sep, the character that separates each of your values (in this case, &quot;-&quot;). Altogether, that gives us a function that works something like this: df2 &lt;- separate(df, Results, c(&quot;FirstRace&quot;, &quot;SecondRace&quot;, &quot;ThirdRace&quot;), sep = &quot;-&quot;) df2 ## # A tibble: 3 x 6 ## Horses FirstRace SecondRace ThirdRace TotalMinutes TotalSeconds ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 1 2 3 3 12 ## 2 B 3 1 2 3 44 ## 3 C 2 3 1 3 15 We can also see that there are some times, split into two columns (minutes and seconds) off to the right side of the table. If we’d rather those be in a single column, we can use the unite() function. This function works pretty similarly to separate(), with one important difference: while in separate(), the second argument was the column to be split, in unite() the second argument is the name of the column you want to combine values into: unite(df2, TotalTime, TotalMinutes, TotalSeconds, sep = &quot;:&quot;) ## # A tibble: 3 x 5 ## Horses FirstRace SecondRace ThirdRace TotalTime ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 A 1 2 3 3:12 ## 2 B 3 1 2 3:44 ## 3 C 2 3 1 3:15 4.4 The Pipe Looking back to our weather data, we can pick out a common problem facing young analysts. At this point, we’ve created four dataframes - SeasonalTemps, LongTemps, WideTemps, and OrderedWideTemps - which all contain the same data. When repeatedly making similar but different dataframes, it can be hard to keep track of which object has which data - and it can be hard to keep coming up with simple, descriptive names, too. One solution could be to keep overwriting the same object with the new data: a &lt;- 10 a &lt;- a*2 a &lt;- sqrt(a) But this breaks our rule - that if you have to repeat yourself more than twice, there’s a better way to do it. Plus, if you make a mistake while writing over a value that had your original data in it, you have to start all over again - assuming that your data was saved anywhere else! You could also try and do it all in a single statement: a &lt;- sqrt(10*2) But with bigger demands, this gets more complicated and harder to read: unite( (separate (df, Results, c(&quot;FirstRace&quot;, &quot;SecondRace&quot;, &quot;ThirdRace&quot;), sep = &quot;-&quot;)), TotalTime, TotalMinutes, TotalSeconds, sep = &quot;:&quot;) ## # A tibble: 3 x 5 ## Horses FirstRace SecondRace ThirdRace TotalTime ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 A 1 2 3 3:12 ## 2 B 3 1 2 3:44 ## 3 C 2 3 1 3:15 This is an example of what we call nested functions, where one function (in this case, separate()) is nested inside of another (unite()). These work like you might expect from your past algebra classes - f(g(x)) is the same as (f * g)(x). But even knowing that, this sort of code is what nightmares are born from! Luckily, the tidyverse also introduces a new operator %&gt;%, called the pipe, in the package magrittr. What the pipe does is pretty intuitive - it takes the output of whatever’s on the left side of the pipe, and uses it as the first input to whatever’s on the right side. For instance: Numbers &lt;- c(5,10,15,20,25) Numbers %&gt;% mean() ## [1] 15 Since all of the tidyverse functions take data as their first argument, this lets us chain together multiple functions and skip those assignment steps without having to nest functions inside of each other: LongTemps %&gt;% spread(Season, AvgTemp) %&gt;% select(-Summer) ## Year Fall Spring Winter ## 1 2015 52 46 40 ## 2 2016 46 40 38 ## 3 2017 54 50 42 ## 4 2018 56 48 44 This makes our code much more easy to understand than constantly using the &lt;- operator, plus it’s an improved way to perform multiple steps in a way that’s easy to read and harder to make serious mistakes doing. Even when a function doesn’t have data as its first input, you can still use a pipe by typing data = . into the function: LongTemps %&gt;% spread(data = ., Season, AvgTemp) %&gt;% select(-Summer) ## Year Fall Spring Winter ## 1 2015 52 46 40 ## 2 2016 46 40 38 ## 3 2017 54 50 42 ## 4 2018 56 48 44 And pipes work well with ggplot2, too: LongTemps %&gt;% ggplot(aes(x = Year, y = AvgTemp, color = Season)) + geom_line() (Note the change from %&gt;% to + once you’re inside the graph-building process - ggplot was designed before its creator learned to love the pipe, and as such uses + to combine steps.) 4.5 Data Transformations 4.5.1 Mutate The pipe becomes useful when we want to transform our data itself for a graph, rather than transform the axes. For example, remember how we made our log-log graph last chapter? LongTemps %&gt;% ggplot(aes(x = Year, y = AvgTemp, color = Season)) + geom_line() + scale_y_log10() This is useful, but ggplot only has a certain number of transformations built in (type ?scale_y_continuous() for more info). Additionally, sometimes we’ll want to transform our data for analyses - not just graphing. For this purpose, we can use dplyr’s mutate() function. Mutate takes three arguments: the dataframe (which it can get from %&gt;%), the name of your new column, and what value the new column should have. Say, for example, we wanted to multiply our average temperatures by two: LongTemps %&gt;% mutate(TwiceTemp = AvgTemp * 2) ## Year Season AvgTemp TwiceTemp ## 1 2015 Winter 40 80 ## 2 2016 Winter 38 76 ## 3 2017 Winter 42 84 ## 4 2018 Winter 44 88 ## 5 2015 Spring 46 92 ## 6 2016 Spring 40 80 ## 7 2017 Spring 50 100 ## 8 2018 Spring 48 96 ## 9 2015 Summer 70 140 ## 10 2016 Summer 62 124 ## 11 2017 Summer 81 162 ## 12 2018 Summer 76 152 ## 13 2015 Fall 52 104 ## 14 2016 Fall 46 92 ## 15 2017 Fall 54 108 ## 16 2018 Fall 56 112 You can make multiple columns in the same mutate() call, even referring to columns made earlier in the same block of code: LongTemps %&gt;% mutate(TwiceTemp = AvgTemp * 2, TwiceSquaredTemp = TwiceTemp^2, YearSeason = paste(Year, Season)) ## Year Season AvgTemp TwiceTemp TwiceSquaredTemp YearSeason ## 1 2015 Winter 40 80 6400 2015 Winter ## 2 2016 Winter 38 76 5776 2016 Winter ## 3 2017 Winter 42 84 7056 2017 Winter ## 4 2018 Winter 44 88 7744 2018 Winter ## 5 2015 Spring 46 92 8464 2015 Spring ## 6 2016 Spring 40 80 6400 2016 Spring ## 7 2017 Spring 50 100 10000 2017 Spring ## 8 2018 Spring 48 96 9216 2018 Spring ## 9 2015 Summer 70 140 19600 2015 Summer ## 10 2016 Summer 62 124 15376 2016 Summer ## 11 2017 Summer 81 162 26244 2017 Summer ## 12 2018 Summer 76 152 23104 2018 Summer ## 13 2015 Fall 52 104 10816 2015 Fall ## 14 2016 Fall 46 92 8464 2016 Fall ## 15 2017 Fall 54 108 11664 2017 Fall ## 16 2018 Fall 56 112 12544 2018 Fall Notice I used a new function, paste(), for that last column. Similarly to unite(), this function pastes together values into a single cell. However, unlike unite(), it can use other values in a dataframe, vectors, or strings. For instance: LongTemps %&gt;% mutate(YearSeason = paste(&quot;The&quot;, Season, &quot;of&quot;, Year)) ## Year Season AvgTemp YearSeason ## 1 2015 Winter 40 The Winter of 2015 ## 2 2016 Winter 38 The Winter of 2016 ## 3 2017 Winter 42 The Winter of 2017 ## 4 2018 Winter 44 The Winter of 2018 ## 5 2015 Spring 46 The Spring of 2015 ## 6 2016 Spring 40 The Spring of 2016 ## 7 2017 Spring 50 The Spring of 2017 ## 8 2018 Spring 48 The Spring of 2018 ## 9 2015 Summer 70 The Summer of 2015 ## 10 2016 Summer 62 The Summer of 2016 ## 11 2017 Summer 81 The Summer of 2017 ## 12 2018 Summer 76 The Summer of 2018 ## 13 2015 Fall 52 The Fall of 2015 ## 14 2016 Fall 46 The Fall of 2016 ## 15 2017 Fall 54 The Fall of 2017 ## 16 2018 Fall 56 The Fall of 2018 Anyway. If you’re transforming your data and only want to preserve your new columns, use transmute(): LongTemps %&gt;% mutate(TwiceTemp = AvgTemp * 2) %&gt;% transmute(AvgTemp = AvgTemp, TwiceSquaredTemp = TwiceTemp^2, YearSeason = paste(Year, Season)) ## AvgTemp TwiceSquaredTemp YearSeason ## 1 40 6400 2015 Winter ## 2 38 5776 2016 Winter ## 3 42 7056 2017 Winter ## 4 44 7744 2018 Winter ## 5 46 8464 2015 Spring ## 6 40 6400 2016 Spring ## 7 50 10000 2017 Spring ## 8 48 9216 2018 Spring ## 9 70 19600 2015 Summer ## 10 62 15376 2016 Summer ## 11 81 26244 2017 Summer ## 12 76 23104 2018 Summer ## 13 52 10816 2015 Fall ## 14 46 8464 2016 Fall ## 15 54 11664 2017 Fall ## 16 56 12544 2018 Fall Note how AvgTemp is present in the new table while TwiceTemp is not, due to the former being included in the transmute() call. 4.5.2 Tibbles As I mentioned earlier, data in R is stored in dataframes. However, you may have noticed that the dataframe outputs from tidyverse functions look pretty different in your R session (I’d even say nicer) than our raw datasets! That’s because of another useful tidyverse package, tibble. Of course, the outputs in this book are pretty much the same - the technology I’m using to publish this isn’t quite that advanced, yet. We don’t need to get too far into the mechanics of this package - if you load the tidyverse, any new dataframes you make will be converted into tibbles by default. If you want to force a dataframe into this format, use as.tibble(); if you need the basic dataframe, use as.data.frame(). 4.5.3 Subsetting Data Let’s go back to our iris dataset. I’m going to turn it into a tibble and then view it: iris &lt;- as.tibble(iris) ## Warning: `as.tibble()` is deprecated, use `as_tibble()` (but mind the new semantics). ## This warning is displayed once per session. iris ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # … with 140 more rows If we only wanted to work with part of this dataset, R gives us a lot of options to subset the data. For instance, if we only wanted the first column containing sepal length, we could type this: iris[, 1] ## # A tibble: 150 x 1 ## Sepal.Length ## &lt;dbl&gt; ## 1 5.1 ## 2 4.9 ## 3 4.7 ## 4 4.6 ## 5 5 ## 6 5.4 ## 7 4.6 ## 8 5 ## 9 4.4 ## 10 4.9 ## # … with 140 more rows If we wanted the first row, meanwhile, we’d type this: iris[1, ] ## # A tibble: 1 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa If we wanted several rows, we can specify them with c() or, if they’re consecutive, :. For instance: iris[c(1,2,3,4), ] ## # A tibble: 4 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa iris[1:4, ] ## # A tibble: 4 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa And if we wanted the value in the first row of the first column, we’d type this: iris[1,1] ## # A tibble: 1 x 1 ## Sepal.Length ## &lt;dbl&gt; ## 1 5.1 The pattern should be clear now - inside of the braces, you type the row number, a comma, and then the column number. Notice that [] always gives us a tibble (or dataframe) back. If we wanted a vector, we could use [[]]: iris[[1, 1]] ## [1] 5.1 If we want to use column names instead of numbers, we could use $ in the place of [[]] - note that this always returns a vector, not a dataframe: iris$Sepal.Length ## [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9 5.4 4.8 4.8 4.3 5.8 5.7 5.4 ## [18] 5.1 5.7 5.1 5.4 5.1 4.6 5.1 4.8 5.0 5.0 5.2 5.2 4.7 4.8 5.4 5.2 5.5 ## [35] 4.9 5.0 5.5 4.9 4.4 5.1 5.0 4.5 4.4 5.0 5.1 4.8 5.1 4.6 5.3 5.0 7.0 ## [52] 6.4 6.9 5.5 6.5 5.7 6.3 4.9 6.6 5.2 5.0 5.9 6.0 6.1 5.6 6.7 5.6 5.8 ## [69] 6.2 5.6 5.9 6.1 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7 5.5 5.5 5.8 6.0 5.4 ## [86] 6.0 6.7 6.3 5.6 5.5 5.5 6.1 5.8 5.0 5.6 5.7 5.7 6.2 5.1 5.7 6.3 5.8 ## [103] 7.1 6.3 6.5 7.6 4.9 7.3 6.7 7.2 6.5 6.4 6.8 5.7 5.8 6.4 6.5 7.7 7.7 ## [120] 6.0 6.9 5.6 7.7 6.3 6.7 7.2 6.2 6.1 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 ## [137] 6.3 6.4 6.0 6.9 6.7 6.9 5.8 6.8 6.7 6.7 6.3 6.5 6.2 5.9 The $ is really helpful in using other base R functions: mean(iris$Sepal.Length) ## [1] 5.843333 sd(iris$Sepal.Length) ## [1] 0.8280661 cor.test(iris$Sepal.Length, iris$Sepal.Width) ## ## Pearson&#39;s product-moment correlation ## ## data: iris$Sepal.Length and iris$Sepal.Width ## t = -1.4403, df = 148, p-value = 0.1519 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.27269325 0.04351158 ## sample estimates: ## cor ## -0.1175698 (Note that “cor.test()” runs Pearson’s correlation test for whatever vectors you feed it - more on that test later, or here). And $ also lets us filter our data with conditionals - getting values that are equal to something, larger or smaller than it, and so on. For instance, if we want a dataframe (so []) where the rows ([, ]) all have a Species value of (==) “setosa”: iris[iris$Species == &quot;setosa&quot;, ] ## # A tibble: 50 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # … with 40 more rows Note that the species name is in quotes, because it’s a character string. We don’t have to do that for numeric values: iris[iris$Sepal.Length &gt; 7.5, ] ## # A tibble: 6 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 7.6 3 6.6 2.1 virginica ## 2 7.7 3.8 6.7 2.2 virginica ## 3 7.7 2.6 6.9 2.3 virginica ## 4 7.7 2.8 6.7 2 virginica ## 5 7.9 3.8 6.4 2 virginica ## 6 7.7 3 6.1 2.3 virginica You can use ==, &gt;, &gt;=, &lt;, &lt;=, and != (not equal) to subset your data. 4.5.4 Filtering with the Tidyverse This code is hard to read as a human, and doesn’t work well with other functions. Instead, for more involved subsets, dplyr has a useful filter() function. It takes two arguments - your dataframe and the condition it should filter based on: iris %&gt;% filter(Species == &quot;setosa&quot;) ## # A tibble: 50 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # … with 40 more rows filter() can use all the same operators as the [] methods of subsetting. Additionally, you can use &amp; (“and”) and | (“or”) to chain filters together: iris %&gt;% filter(Species == &quot;setosa&quot; &amp; Sepal.Length == 5.1 &amp; Sepal.Width == 3.3) ## # A tibble: 1 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.3 1.7 0.5 setosa It’s important to remember that &amp; means things which satisfy EACH condition. A common mistake is to type: iris %&gt;% filter(Species == &quot;setosa&quot; &amp; Species == &quot;versicolor&quot;) ## # A tibble: 0 x 5 ## # … with 5 variables: Sepal.Length &lt;dbl&gt;, Sepal.Width &lt;dbl&gt;, ## # Petal.Length &lt;dbl&gt;, Petal.Width &lt;dbl&gt;, Species &lt;fct&gt; Which, because no flower is both species, returns nothing. In this case, you can either use an | (“or”) operator, or - particularly if you have several cases you want to accept - %in%: iris %&gt;% filter(Species %in% c(&quot;setosa&quot;, &quot;versicolor&quot;)) ## # A tibble: 100 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # … with 90 more rows So long as your species is %in% the vector c() you provide, it will show up in the output. (By the way, if you’ve coded with other languages, you might be used to seeing &amp;&amp; and || as the relevant operators. In R, these only check against the first value they evaluate against, and as such are almost never what you want to use. This is an incredibly common mistake made by people new to R!) 4.5.5 Working with Groups Say we wanted to find the mean sepal length in our dataset. That’s pretty easy: mean(iris$Sepal.Length) ## [1] 5.843333 But we already know from our graphs that sepal length differs dramatically between species. If we wanted to find the mean for each species, we could calculate it individually for each group: setosa &lt;- iris %&gt;% filter(Species == &quot;setosa&quot;) virginica &lt;- iris %&gt;% filter(Species == &quot;virginica&quot;) versicolor &lt;- iris %&gt;% filter(Species == &quot;versicolor&quot;) mean(setosa$Sepal.Length) ## [1] 5.006 mean(virginica$Sepal.Length) ## [1] 6.588 mean(versicolor$Sepal.Length) ## [1] 5.936 But that code is messy, the output is without any context, and it goes against our rule - that if you have to repeat yourself more than twice, there’s a better way to do it. The better way in the tidyverse is to use grouping and summary functions. In the following example, we’ll use group_by() to group our dataframes by the species types, and summarise() to calculate the mean for each of them (in a column called “MeanSepalLength”): iris %&gt;% group_by(Species) %&gt;% summarise(MeanSepalLength = mean(Sepal.Length)) ## # A tibble: 3 x 2 ## Species MeanSepalLength ## &lt;fct&gt; &lt;dbl&gt; ## 1 setosa 5.01 ## 2 versicolor 5.94 ## 3 virginica 6.59 This is a faster and easier to understand way to perform functions on groups of data. Note that summarise() uses the British spelling - almost all functions in R have British and American spellings built in (you can use color or colour aesthetics in ggplot, for instance), but this is an important exception. While there is a function called summarize(), it’s highly glitchy and its use is highly discouraged. You can use group_by() to calculate all sorts of things - for instance, we can calculate the distance of each plant’s sepal length from the group mean, as follows: iris %&gt;% group_by(Species) %&gt;% mutate(SLDistanceFromMean = Sepal.Length - mean(Sepal.Length)) ## # A tibble: 150 x 6 ## # Groups: Species [3] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # … with 140 more rows, and 1 more variable: SLDistanceFromMean &lt;dbl&gt; If you want to calculate variables for the whole dataset again, you’ll have to ungroup your data - dataframes will stay grouped until you actively ungroup them with ungroup(). For instance, to calculate the distance of each plant’s sepal length from the overall mean: iris %&gt;% select(c(Sepal.Length, Species)) %&gt;% group_by(Species) %&gt;% mutate(SLDistanceFromGroupMean = Sepal.Length - mean(Sepal.Length)) %&gt;% ungroup() %&gt;% mutate(SLDistanceFromTotalMean = Sepal.Length - mean(Sepal.Length)) ## # A tibble: 150 x 4 ## Sepal.Length Species SLDistanceFromGroupMean SLDistanceFromTotalMean ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 setosa 0.0940 -0.743 ## 2 4.9 setosa -0.106 -0.943 ## 3 4.7 setosa -0.306 -1.14 ## 4 4.6 setosa -0.406 -1.24 ## 5 5 setosa -0.006 -0.843 ## 6 5.4 setosa 0.394 -0.443 ## 7 4.6 setosa -0.406 -1.24 ## 8 5 setosa -0.006 -0.843 ## 9 4.4 setosa -0.606 -1.44 ## 10 4.9 setosa -0.106 -0.943 ## # … with 140 more rows (Note that I got rid of some columns with select() to make all the columns in the tibble fit on one page.) 4.6 Missing Values 4.6.1 Explicit Missing Values Working with data, there are often two types of missing values we have to worry about. The obvious one are explicit missing values, represented in R as NA (or, sometimes, NaN). Let’s make a dataframe: MissingExample &lt;- tibble(w = c(1, 2, 3), x = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), y = c(&quot;do&quot;, &quot;re&quot;, NA), z = c(807, NA, 780)) MissingExample ## # A tibble: 3 x 4 ## w x y z ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 A do 807 ## 2 2 B re NA ## 3 3 C &lt;NA&gt; 780 (I’m using tibble() in place of dataframe() here, but the outcome is almost identical.) NA values are a little tricky to work with - look what happens when we try to find the mean of z: mean(MissingExample$z) ## [1] NA The reason this happens is because we don’t know what the mean is - that NA value could be anything, so it’s impossible to know what the mean is. To get around this, we can set the na.rm argument to TRUE: mean(MissingExample$z, na.rm = TRUE) ## [1] 793.5 We can also solve the problem with filtering out the NA values. We can use is.na() to find out where certain values are, and then ask filter() to remove those rows from our dataset as follows: MissingExample %&gt;% filter(!is.na(z)) %&gt;% summarise(Mean = mean(z)) ## # A tibble: 1 x 1 ## Mean ## &lt;dbl&gt; ## 1 794. ! means “negation” in R, or “opposite” - so we’re asking filter() to return the opposite of any row where z is NA, or, alternatively, all the rows where it has a value. If we wanted to drop every row that has a NA, we could use the following tidyr function: MissingExample %&gt;% drop_na() ## # A tibble: 1 x 4 ## w x y z ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 A do 807 Or, if we knew the values we wanted those NA to represent, we could use replace_na(), also from tidyr. We just have to specify a list of what we want those values to be: MissingExample %&gt;% replace_na(list(y = &quot;mi&quot;, z = &quot;078&quot;)) ## # A tibble: 3 x 4 ## w x y z ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 A do 807 ## 2 2 B re 078 ## 3 3 C mi 780 Notice a difference in the z column with this example? Because I put “078” in quotes, it changed the entire column to a character vector - because quotes mean characters, and a vector can only hold one class of data. We’ll talk more about that list() function later on - that’s a little too complicated for this unit. 4.6.2 Implicit Missing Values The other, harder to identify type of missing value is the implicit missing value. Say we have a dataframe TreeData, which lists the species that are present at two different sites: TreeData &lt;- tibble(Site = c(&quot;A&quot;,&quot;A&quot;,&quot;A&quot;,&quot;B&quot;,&quot;B&quot;), Species = c(&quot;Red Maple&quot;, &quot;Sugar Maple&quot;, &quot;Black Cherry&quot;, &quot;Red Maple&quot;, &quot;Sugar Maple&quot;), Count = c(10,5,15,8,19)) TreeData ## # A tibble: 5 x 3 ## Site Species Count ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 A Red Maple 10 ## 2 A Sugar Maple 5 ## 3 A Black Cherry 15 ## 4 B Red Maple 8 ## 5 B Sugar Maple 19 This system makes a lot of sense - each row represents something you found at each site. The problem with this comes when we try to calculate summary statistics for each species: TreeData %&gt;% group_by(Species) %&gt;% summarise(Mean = mean(Count), StandardDev = sd(Count)) ## # A tibble: 3 x 3 ## Species Mean StandardDev ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Black Cherry 15 NA ## 2 Red Maple 9 1.41 ## 3 Sugar Maple 12 9.90 Black cherry has a missing (NaN) standard deviation, because as far as R knows, it only has one observation to make estimates with. In reality, the fact that black cherry was missing from site B is a data point in and of itself - it’s an implicit value of 0. To fix that, we can use the complete() command from tidyr. This function takes column names as arguments, and returns a dataframe with every combination of the values in those columns. We can also specify what to replace NA values with, much like we did in replace_na(), with fill: TreeData %&gt;% complete(Site, Species, fill = list(Count = 0)) ## # A tibble: 6 x 3 ## Site Species Count ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 A Black Cherry 15 ## 2 A Red Maple 10 ## 3 A Sugar Maple 5 ## 4 B Black Cherry 0 ## 5 B Red Maple 8 ## 6 B Sugar Maple 19 This way, when we go to calculate our summary statistics, we get better answers: TreeData %&gt;% complete(Site, Species, fill = list(Count = 0)) %&gt;% group_by(Species) %&gt;% summarise(Mean = mean(Count), StandardDev = sd(Count)) ## # A tibble: 3 x 3 ## Species Mean StandardDev ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Black Cherry 7.5 10.6 ## 2 Red Maple 9 1.41 ## 3 Sugar Maple 12 9.90 4.7 Count Data One other common issue with field data is that it’s in a summary form - for instance, our tree data summarizes the number of trees at each site into one column. This is often easier to record in the field and easier to read as a human - but it makes some analyses much harder! The function uncount() makes this pretty easy for us: LongTreeData &lt;- TreeData %&gt;% uncount(Count) LongTreeData ## # A tibble: 57 x 2 ## Site Species ## &lt;chr&gt; &lt;chr&gt; ## 1 A Red Maple ## 2 A Red Maple ## 3 A Red Maple ## 4 A Red Maple ## 5 A Red Maple ## 6 A Red Maple ## 7 A Red Maple ## 8 A Red Maple ## 9 A Red Maple ## 10 A Red Maple ## # … with 47 more rows And if we wanted to get back to the summary table, we can use count(): LongTreeData %&gt;% count(Site, Species) ## # A tibble: 5 x 3 ## Site Species n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 A Black Cherry 15 ## 2 A Red Maple 10 ## 3 A Sugar Maple 5 ## 4 B Red Maple 8 ## 5 B Sugar Maple 19 If we want to change that column n’s name to something more descriptive, we can use rename(): LongTreeData %&gt;% count(Site, Species) %&gt;% rename(Count = n) ## # A tibble: 5 x 3 ## Site Species Count ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 A Black Cherry 15 ## 2 A Red Maple 10 ## 3 A Sugar Maple 5 ## 4 B Red Maple 8 ## 5 B Sugar Maple 19 These skills will be the building blocks for everything else you do in R! It’s time for us to put them to good use - in the next chapter, we’ll start in on exploratory data analysis with a brand new dataset. 4.7.1 Work with other datasets: spread() the iris dataset so that each species’ petal width is in its own column. Then gather() the table back together. What’s different about this dataframe? Select all the rows of iris where the species is setosa. Now select all the rows where the species isn’t setosa. What’s the mean price for each cut of diamond in the diamonds dataset? Inspect the smiths dataset (loaded with the tidyverse - you can access it like iris). How can you fix those missing values? "],
["introduction-to-data-analysis.html", "5 Introduction to Data Analysis 5.1 Exploratory Data Analysis 5.2 gapminder 5.3 Summarizing Data 5.4 Visualizing Data 5.5 Analyzing Patterns 5.6 Exercises", " 5 Introduction to Data Analysis Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise John W. Tukey, The Future of Data Analysis 5.1 Exploratory Data Analysis So far, we’ve learned about how to manipulate our data and how to graph our outputs. Both of these are critically important parts of what’s known as exploratory data analysis - or EDA. When you’re starting with a new dataset, you won’t always immediately know what trends and patterns might be there to discover. The idea at this stage isn’t to find out what’s causing any trends in the data, to identify any significant results you might have, or to get publishable or presentable figures and tables - the point is to understand exactly what it is that you’re dealing with. For more on this topic, check out this post. 5.1.1 Sidenote EDA differs from what most people expect data analytics to be. Most people expect analytics to follow a clearcut path, going in a linear direction from deciding on a question to answering said question completely and accurately. But that really isn’t the point of data analytics, and especially not EDA, as Tukey (the father of data analytics and modern statistics) says at the top of this chapter. Most people think of analytics as something you do to improve the strength of your evidence, but that’s not what the discipline excels at. Rather, the discipline shines best when it’s working on improving the question you’re asking, perhaps generating evidence for your case along the way. That’s not to say you can’t use analytics to try and answer a question - you can, and most people working in the field today use it in that manner. But you’ll be a better analyst if you keep yourself open to new surprises, and chase them down, rather than attempting to make your analysis follow a linear path. Speaking of surprises, I really enjoy this quote from Nate Silver, founder and editor in chief of FiveThirtyEight: You ideally want to find yourself surprised by the data some of the time — just not too often. If you never come up with a result that surprises you, it generally means that you didn’t spend a lot of time actually looking at the data; instead, you just imparted your assumptions onto your analysis and engaged in a fancy form of confirmation bias. If you’re constantly surprised, on the other hand, more often than not that means your [code] is buggy or you don’t know the field well enough; a lot of the “surprises” are really just mistakes. —Nate Silver Surprises are awesome, and are how discoveries are made. But at the same time, a lot of papers are retracted because their big surprise was actually just a glitch in the code. Whenever you find something you didn’t expect, make sure you go back through your code and assumptions - it never hurts to double check! For more on this topic, check out the awesome lecture notes for Skepticism in Data Science from John Hopkins University. 5.1.2 The EDA Framework I’ve found it useful, in composing my thoughts about exploratory data analysis, to fit the rough steps of EDA into a three-part framework. The first thing I do when presented with any new dataset is to summarize the data, to see what general shape it’s in. This lets me catch outliers and typos, see if there’s any values I have a question on, and start getting a sense of the scope of the dataset I’m working with. I’ll then usually start to visualize the data, making exploratory graphs in order to get a more concrete sense of how each variable interacts with all the others, and to see if any patterns emerge from the dataset. I’ll then usually start to analyze the patterns I’m finding, picking them apart to see if I can better understand how they work before I begin modeling them. However, I don’t believe EDA is so simple that I can say these are ordered steps - that summarizing comes before visualization, which happens before you being analyzing patterns. I believe that EDA typically involves multiple rounds of each step, where visualizations give you new ideas for summaries you’d be interested in, which perhaps give rise to patterns you want to analyze, which in turn inspire more visualizations. The point is to use your skills to play with your datasets, until you’re satisfied you have enough information to proceed onto your analysis. 5.2 gapminder We’re going to use the gapminder dataset for this unit, in order to see more a more realistic application of exploratory data analysis that you might perform when working with a new dataset for the first time. In order to do this, we’re going to install the gapminder package, then load it as usual: install.package(&quot;gapminder&quot;) library(gapminder) This package includes five data tables, all focusing on GDP and life expectancy values for a set of countries. The largest of these datasets is gapminder_unfiltered, which is what we’ll be working with now. Let’s take a look using head(): head(gapminder_unfiltered) ## # A tibble: 6 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. So as we can see, there are six columns in this dataset, which are more or less descriptively named. If you’ve got questions about the dataset, or what data is stored in any of the columns, you can check this dataset out with ?gapminder_unfiltered. 5.3 Summarizing Data It would now be helpful for us to get a bit broader sense of what each column contains. One of the easiest ways to do that is with summary(): summary(gapminder_unfiltered) ## country continent year lifeExp ## Czech Republic: 58 Africa : 637 Min. :1950 Min. :23.60 ## Denmark : 58 Americas: 470 1st Qu.:1967 1st Qu.:58.33 ## Finland : 58 Asia : 578 Median :1982 Median :69.61 ## Iceland : 58 Europe :1302 Mean :1980 Mean :65.24 ## Japan : 58 FSU : 139 3rd Qu.:1996 3rd Qu.:73.66 ## Netherlands : 58 Oceania : 187 Max. :2007 Max. :82.67 ## (Other) :2965 ## pop gdpPercap ## Min. :5.941e+04 Min. : 241.2 ## 1st Qu.:2.680e+06 1st Qu.: 2505.3 ## Median :7.560e+06 Median : 7825.8 ## Mean :3.177e+07 Mean : 11313.8 ## 3rd Qu.:1.961e+07 3rd Qu.: 17355.8 ## Max. :1.319e+09 Max. :113523.1 ## You can see that summary() gives us some basic summary statistics of each column - and that these views are different for numeric and text columns. We can also start seeing how these values are distributed - it seems like the values cover six continents (though I’m a little iffy on what “FSU” means, and it looks like the Americas are merged), and 57 years (from 1950 to 2007). The numeric columns, meanwhile, have simple summary statistics listed. We can dig down a little deeper into these statistics using the describe() function, from the psych package: install.packages(&quot;psych&quot;) library(psych) describe(gapminder_unfiltered) ## vars n mean sd median trimmed ## country* 1 3313 93.30 53.42 92.00 93.47 ## continent* 2 3313 3.12 1.40 3.00 3.08 ## year 3 3313 1980.29 16.93 1982.00 1980.57 ## lifeExp 4 3313 65.24 11.77 69.61 66.63 ## pop 5 3313 31773251.41 104501904.44 7559776.00 12014676.53 ## gdpPercap 6 3313 11313.82 11369.01 7825.82 9629.08 ## mad min max range skew kurtosis ## country* 68.20 1.00 1.870000e+02 1.860000e+02 -0.01 -1.24 ## continent* 1.48 1.00 6.000000e+00 5.000000e+00 -0.05 -0.74 ## year 22.24 1950.00 2.007000e+03 5.700000e+01 -0.11 -1.19 ## lifeExp 8.45 23.60 8.267000e+01 5.907000e+01 -0.98 -0.05 ## pop 8509318.95 59412.00 1.318683e+09 1.318624e+09 7.44 63.47 ## gdpPercap 9200.98 241.17 1.135231e+05 1.132820e+05 2.08 8.82 ## se ## country* 0.93 ## continent* 0.02 ## year 0.29 ## lifeExp 0.20 ## pop 1815572.04 ## gdpPercap 197.52 Now in addition to the values from summary(), we get the number of observations, each column’s standard deviation, trimmed mean, median absolute deviation, range, skew, kurtosis, and standard error. We can also, if we wanted, check to see how many unique countries are represented in the dataset, by checking the length() of a vector of all the unique() values of the country column: length(unique(gapminder_unfiltered$country)) ## [1] 187 If we wanted to make that a little simpler, by the way, we can use the n_distinct() function from dplyr. As a sidenote, we can use a function from a package without loading it (via library) by typing the package name followed by ::, like so: dplyr::n_distinct(gapminder_unfiltered$country) ## [1] 187 However, it makes sense for us to load the package explicitly, since we’ll be using a lot of the functions from it for the rest of this chapter. library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union So looking at those values, I’m noticing some extreme numbers in the life expectancy and GDP columns - specifically, the minimums seem much lower than I’d expect. We can pull those rows out of the dataset using filter(): gapminder_unfiltered %&gt;% filter(lifeExp == min(lifeExp)) ## # A tibble: 1 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Rwanda Africa 1992 23.6 7290203 737. gapminder_unfiltered %&gt;% filter(gdpPercap == min(gdpPercap)) ## # A tibble: 1 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Congo, Dem. Rep. Africa 2002 45.0 55379852 241. As history buffs might know, these values are both (sadly) explained by politics and history. The other oddball I want to check is what the “FSU” continent is meant to represent: gapminder_unfiltered %&gt;% filter(continent == &quot;FSU&quot;) ## # A tibble: 139 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Armenia FSU 1992 68.7 3378331 1443. ## 2 Armenia FSU 1997 70.4 3059000 1791. ## 3 Armenia FSU 2002 71.4 3013818 2692. ## 4 Armenia FSU 2007 72.0 2971650 4943. ## 5 Belarus FSU 1973 72.7 9236465 4959. ## 6 Belarus FSU 1990 71.2 10215208 6808. ## 7 Belarus FSU 1991 70.6 10244639 6693. ## 8 Belarus FSU 1992 70.2 10306362 6014. ## 9 Belarus FSU 1993 69.0 10360516 5528. ## 10 Belarus FSU 1994 68.8 10387841 4869. ## # … with 129 more rows Ahh! These countries are all Former Soviet Union states, so are broken out from the other countries - and have a lot of incomplete data. The other notable values I’d like to check out are the super high skew and kurtosis values that describe() gave us for the GDP column. These values demonstrate that our data really doesn’t follow the normal distribution (which has a value of 0 for both metrics) - but I’d like to visualize how: library(ggplot2) ## Registered S3 methods overwritten by &#39;ggplot2&#39;: ## method from ## [.quosures rlang ## c.quosures rlang ## print.quosures rlang ## ## Attaching package: &#39;ggplot2&#39; ## The following objects are masked from &#39;package:psych&#39;: ## ## %+%, alpha ggplot(gapminder_unfiltered, aes(gdpPercap)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. So our data is less variable than we’d expect (that’s the high kurtosis, implying that the mode is more common than it would be in the normal distribution), and right-skewed (meaning that our mean is higher than the median, since there are a number of high outlier values). That’s not a bad thing, but it’s worth knowing how our data is structured and wondering why, and how it might affect the other variables we can measure. This graph could probably be made more effective if we split it up by continent. Let’s use facet_wrap() to create six panels of the graph, one for each continent: ggplot(gapminder_unfiltered, aes(gdpPercap)) + geom_histogram() + facet_wrap(~ continent) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. If we wanted to calculate our values as a relative measurement rather than an absolute count - so that we can better see the patterns in other areas, and lower the high outlier in Africa - we can provide ..density.. as the y argument in aes: ggplot(gapminder_unfiltered, aes(gdpPercap, ..density..)) + geom_histogram() + facet_wrap(~ continent) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Now our Y axis represents the proportion of all values that particular GDP makes up, rather than the absolute count of values in that bin. This is already helping me understand what we’re working with - most of our lowest GDP measurements come from Africa, while Europe is pretty uniformly higher, and Asia has a more skewed spread than any other region. 5.3.1 Sidenote Often, people dislike using histograms, and instead choose to use frequency polygons - implemented in ggplot using the geom_freqpoly() function: ggplot(gapminder_unfiltered, aes(gdpPercap, ..density..)) + geom_freqpoly() + facet_wrap(~ continent) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. This sometimes then gives rise to a thought - since these lines take up so much less space, why not just combine them all into one panel, rather than six? And so they make graphs that look like this: ggplot(gapminder_unfiltered, aes(gdpPercap, ..density.., color = continent)) + geom_freqpoly() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. This is a Bad Graph, as I hope should be obvious. It’s almost impossible to pick out any individual line, and the whole thing just looks like a jumbled mess - which has led the community to name this sort of visualization as a “spaghetti graph”. In cases like this, where a graph contains more than one or two lines (with obvious separations), it makes much more sense to facet your graphs. 5.4 Visualizing Data Anyway. These are the first visualizations we’ve made to start seeing what our data looks like. It’s worthwhile to make a lot of these fast graphics in order to get a handle on your datasets, so you can see what angles might be useful for your analysis. One of the most useful is pairs, which will show you scatterplots between each of your variables. Let’s try running it on the gapminder data now: pairs(gapminder_unfiltered) To interpret this graph, understand that each text box (“country”, “continent” and so on) represents that variable - in any given column, that text box is on the X axis, while for any given row the text box represents the Y axis. Even knowing that, the graph is still a little hard to understand - the boxes are small, and the “country” and “continent” graphs don’t make a ton of sense. To get rid of those (and hopefully fix both problems at once), we could use select() from last chapter and make a new dataset: gp &lt;- select(gapminder_unfiltered, 3:6) pairs(gp) But an even cleaner method is to skip the new dataset altogether, and just use select_if(). This function lets us only select columns that return TRUE when passed to a function, and works like this: head(select_if(gapminder_unfiltered, is.numeric)) ## # A tibble: 6 x 4 ## year lifeExp pop gdpPercap ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1952 28.8 8425333 779. ## 2 1957 30.3 9240934 821. ## 3 1962 32.0 10267083 853. ## 4 1967 34.0 11537966 836. ## 5 1972 36.1 13079460 740. ## 6 1977 38.4 14880372 786. Note how instead of column names we provided the name of a function, is.numeric(). We also didn’t put parentheses after the is.numeric function - we just pass it the name. That lets us easily and cleanly select all the numeric columns, making sure we didn’t skip any. We can then make the graph in a single line of code like this: pairs(select_if(gapminder_unfiltered, is.numeric)) This is a bit easier to read! It looks like life expectancy generally increases over time and with higher GDP, and that population and GDP both also increase over time. If we want a more numeric version of these graphs, we can calculate the actual correlation coefficients by swapping cor() in for pairs(): cor(select_if(gapminder_unfiltered, is.numeric)) ## year lifeExp pop gdpPercap ## year 1.00000000 0.383616006 0.013368315 0.31440915 ## lifeExp 0.38361601 1.000000000 -0.006116394 0.63376069 ## pop 0.01336832 -0.006116394 1.000000000 -0.04595259 ## gdpPercap 0.31440915 0.633760687 -0.045952593 1.00000000 These numbers range from -1 to 1, with higher absolute values representing higher correlations. So we can see that our visual judgements are pretty well supported by the numbers - especially that there’s a strong relationship between GDP and life expectancy! 5.5 Analyzing Patterns Now we’re moving into the next piece of EDA - exploring the patterns that we pick up on. This isn’t something you need to save until you’re done summarizing and visualizing your data - rather, it’s a process that you should follow every time you find something that seems interesting in the data. A lot of these paths may be dead ends, but the ones that are worth following will often wind up shaping your analysis, providing insights into your dataset that you hadn’t been expecting when you started. I’d be interested in looking a little more into how GDP and life expectancy interact. Let’s make a scatter plot for just those two variables: ggplot(gapminder_unfiltered, aes(gdpPercap, lifeExp)) + geom_point() This is already an interesting pattern, in and of itself - it seems like higher GDP results in higher life expectancy, but only to a point. This sort of shape - where there’s an obvious and single curve to the data - always makes me think that log-transforming the data might make the relationship more linear: ggplot(gapminder_unfiltered, aes(gdpPercap, lifeExp)) + geom_point() + scale_x_log10() And in this case it does, very dramatically. Now, I don’t particularly like log-transformed graphs, as I discussed in chapter 2. However, it will be important for us to know that our data has this relationship when we go to perform our analyses. That, however, is a matter for the next chapter. I’ve started to wonder now how other variables play into this pattern. For instance, I know that people lived less long back in 1950 than they do today, and I know that life expectancy in some areas of the world is significantly lower than others. Luckily enough, we have the ability now to add both these factors to our visualization - here, I’ve split the graph into facets by continent, while recoloring the points by the year. ggplot(gapminder_unfiltered, aes(gdpPercap, lifeExp, color = year)) + geom_point() + facet_wrap(~ continent) Splitting the graph into panels by continent was very effective, I think - Africa has notably lower life expectancy and GDP than other countries do, while Europe seems to always be higher than average. I don’t know that coloring the points by year has quite the same effect, probably due to there being too many points on this graph - my guess is that we’ll see that impact more while we’re modeling the data in our next chapter. I’m feeling very comfortable with how we’ve progressed in this chapter - we’ve moved from downloading a brand new dataset to more thoroughly exploring it, and have picked up on a pattern that we’re going to subject to a more thorough analysis. We could totally keep using our EDA skills in order to investigate patterns further - for instance, it might be interesting to examine what factors impact population over time, or see how rates of population change dependent upon life expectancy and GDP! However, the skeleton here demonstrating the steps of EDA should be enough to give you ideas of how to apply this framework to your own analyses moving forward. Instead, we’re going to continue on with the gapminder package in our next unit, as we begin making models to actually analyze this pattern we’ve found. 5.6 Exercises The output of psych::describe(gapminder_unfiltered) put stars after country and continent. Why? Make a histogram of a gapminder variable other than GDP. Describe the graph with regards to its skewdness and kurtosis. Answer the question: how does population seem to be impacted by the other variables in the dataset? Use the techniques we’ve demonstrated here to find out. Just like select() has a cousin in select_if(), all the other main dplyr functions (mutate(), filter(), group_by(), and summarise(), among others) also have an _if() variant. Use mutate_if() to multiply all the numeric columns by 2. "],
["modeling-data.html", "6 Modeling Data 6.1 Why Model? 6.2 Linear Models 6.3 Model Predictions 6.4 Classification 6.5 Logistic Models 6.6 Evaluating and Comparing Models 6.7 Conclusion 6.8 Exercises", " 6 Modeling Data “For such a model there is no need to ask the question “Is the model true?”. If “truth” is to be the “whole truth” the answer must be “No”. The only question of interest is “Is the model illuminating and useful?” 6.1 Why Model? We’ve now learned how to wrangle our data and begin exploring it in R, which are both crucial steps of the process. However, when people think about data analytics, they’re usually picturing a tool that will help them answer questions they have, providing information they can use to make decisions. The skills we’ve built so far will help you find questions in your data and begin exploring patterns, but this chapter will help you find answers. And so comes the penultimate step in our data analysis framework, making a model. Models are summaries of relationships within our data, letting us see more clearly how changes in certain variables impact the others. We can then use that information to predict future outcomes, or to help us understand our dataset. Now, we’ll be using our gapminder dataset throughout this chapter, which means there is one thing we can’t do with our models - we can’t use them to confirm a hypothesis. Hypothesis testing is much more prevalent in scientific fields than in business applications, and requires you to have started your analysis with a stated hypothesis and a plan of how you’ll test it. We didn’t do that - instead, we looked for how our variables interacted, and took the main pattern that we’ll be analyzing - how life expectancy interacts with GDP - straight from our exploratory analysis. As such, we can’t claim that our data proves anything, just that it shows our values are correlated. Instead, our analysis is entirely exploratory - which isn’t a bad thing! Often times, exploratory analysis is necessary to justify further confirmatory studies. I’ve always liked a quote from Brian McGill on the matter: If exploratory statistics weren’t treated like the crazy uncle nobody wants to talk about and everybody is embarrassed to admit being related to, science would be much better off. — Brian McGill But we’re getting off topic now. Suffice it to say that we’ll be building models that demonstrate how our variables are related, and using those models to generate predictions. The goal isn’t to teach you much about how these models work “under the hood” - for that, I’d suggest looking at other resources, perhaps starting with this paper. Instead, we’ll go over how to build and use these models in R, with more of an emphasis on “how” than “why”. 6.2 Linear Models Our first and simplest model is the linear model. Thinking back to last chapter, we made a graph that showed the relationship between GDP and life expectancy: library(gapminder) library(tidyverse) ## Registered S3 methods overwritten by &#39;ggplot2&#39;: ## method from ## [.quosures rlang ## c.quosures rlang ## print.quosures rlang ## Registered S3 method overwritten by &#39;rvest&#39;: ## method from ## read_xml.response xml2 ## ── Attaching packages ─────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.1.1 ✔ purrr 0.3.2 ## ✔ tibble 2.1.1 ✔ dplyr 0.8.0.1 ## ✔ tidyr 0.8.3 ✔ stringr 1.4.0 ## ✔ readr 1.3.1 ✔ forcats 0.4.0 ## ── Conflicts ────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ggplot(gapminder_unfiltered, aes(gdpPercap, lifeExp)) + geom_point() A linear model tries to summarize this relationship in a single straight line (sometimes known as a line of best fit), which in this case would look something like the following: ggplot(gapminder_unfiltered, aes(gdpPercap, lifeExp)) + geom_point() + geom_smooth(method = &quot;lm&quot;) (You might remember geom_smooth() from chapter 2 - it adds a trendline to our graph. By specifying method = &quot;lm&quot;, we told it to graph the line we’d get from a linear model.) This model claims that for every possible GDP level present in our dataset, there’s a set life expectancy. Those values are the ones traced by the line - so countries with a GDP of 0 should expect life expectancies around 60, for instance. This is a good moment to revisit the quote that opened this chapter, from legendary statistician George Box. His full quote has gotten a bit mangled over the years, to the point that most people know it more simply as: All models are wrong; some models are useful. For instance, it’s obvious that our linear model here is wrong for the great majority of countries - the richest countries in our dataset ought to be living well into their 130s, according to the line. However, it is useful to an extent - it shows us that our variables are positively correlated, and gives us an idea of how to move forward. It is not so useful, however, that we should spend too much time picking values out from along the line. Of course, it would be much easier for us to just calculate the number from a formula, rather than finding the point on the graph! To see the actual model formula, we can use the lm() function. This function creates a linear model and requires two main arguments: A formula, of the format Y ~ X (Where Y is your response variable, X your predictor variables, and you read the “~” as “as a function of”) A dataset The usage looks like this: lm(lifeExp ~ gdpPercap, data = gapminder_unfiltered) ## ## Call: ## lm(formula = lifeExp ~ gdpPercap, data = gapminder_unfiltered) ## ## Coefficients: ## (Intercept) gdpPercap ## 5.782e+01 6.562e-04 So what we can tell here is that for every 1 increase in GDP, we can expect to see a 0.0006562 year increase in life expectancy. That’s not particularly large - but then, a single dollar increase of GDP isn’t very much, either! To understand our model better, we can call the summary() function on the model: summary(lm(lifeExp ~ gdpPercap, data = gapminder_unfiltered)) ## ## Call: ## lm(formula = lifeExp ~ gdpPercap, data = gapminder_unfiltered) ## ## Residuals: ## Min 1Q Median 3Q Max ## -74.282 -3.902 3.282 6.161 16.003 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.782e+01 2.232e-01 258.97 &lt;2e-16 *** ## gdpPercap 6.563e-04 1.392e-05 47.14 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.108 on 3311 degrees of freedom ## Multiple R-squared: 0.4017, Adjusted R-squared: 0.4015 ## F-statistic: 2223 on 1 and 3311 DF, p-value: &lt; 2.2e-16 Here we get a little more information on how well our model fits the data. We can see p-values for our overall model and for each variable (check out Chapter 14 for more information on these), as well as our model R2. The R2 value represents how much of the variance in your dataset can be explained by your model - basically, how good a fit your model is to the data. Generally, we use the adjusted R2, which compensates for how many variables you’re using in your model - otherwise, adding another variable always increases your multiple R2. But you might recall that we saw a much more normal linear relationship between our variables when we log-transformed GDP: ggplot(gapminder_unfiltered, aes(log(gdpPercap), lifeExp)) + geom_point() + geom_smooth(method = &quot;lm&quot;) So that would make me guess that our model would also be improved by log-transforming the GDP term in our model - just look at how much better that line fits on the second graph. Let’s try it now: summary(lm(lifeExp ~ log(gdpPercap), data = gapminder_unfiltered)) ## ## Call: ## lm(formula = lifeExp ~ log(gdpPercap), data = gapminder_unfiltered) ## ## Residuals: ## Min 1Q Median 3Q Max ## -32.622 -2.154 0.826 3.380 17.884 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.87455 0.77505 -6.289 3.61e-10 *** ## log(gdpPercap) 8.02711 0.08785 91.374 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.274 on 3311 degrees of freedom ## Multiple R-squared: 0.716, Adjusted R-squared: 0.716 ## F-statistic: 8349 on 1 and 3311 DF, p-value: &lt; 2.2e-16 Note that I didn’t print out the model statement this time - summary() provides us all the information lm() does, so we can just call it instead. We can see that our R2 value shot up - we’re now at 0.716, instead of 0.402! So log-transforming our data seems to help our model fit the data better. We also, last chapter, realized that the continent a country was on and the year of an observation were probably relevant to the analysis. We can add those to our model using +: summary(lm(lifeExp ~ log(gdpPercap) + continent + year, data = gapminder_unfiltered)) ## ## Call: ## lm(formula = lifeExp ~ log(gdpPercap) + continent + year, data = gapminder_unfiltered) ## ## Residuals: ## Min 1Q Median 3Q Max ## -24.3094 -2.2425 0.2411 2.7085 15.4838 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.144e+02 1.085e+01 -28.97 &lt;2e-16 *** ## log(gdpPercap) 5.015e+00 1.040e-01 48.21 &lt;2e-16 *** ## continentAmericas 9.459e+00 3.521e-01 26.86 &lt;2e-16 *** ## continentAsia 8.084e+00 3.078e-01 26.26 &lt;2e-16 *** ## continentEurope 1.227e+01 3.393e-01 36.15 &lt;2e-16 *** ## continentFSU 9.903e+00 4.943e-01 20.04 &lt;2e-16 *** ## continentOceania 1.040e+01 4.645e-01 22.39 &lt;2e-16 *** ## year 1.653e-01 5.611e-03 29.46 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.029 on 3305 degrees of freedom ## Multiple R-squared: 0.8179, Adjusted R-squared: 0.8175 ## F-statistic: 2120 on 7 and 3305 DF, p-value: &lt; 2.2e-16 This also got our R2 up, all the way to 0.818! However, you might notice that our summary output is getting much longer - we now have a row for each of the continents in our dataset (minus Africa, for reasons that are outside of this text - the short explanation is that Africa is what all other continents are being compared to; so being in Europe gives you, on average, 12.27 more years of life than being in Africa, for instance). If we wanted a bit more legible output, we could instead use an ANOVA test - which deals with categorical variables (like continent) as a single unit, rather than as each of the possible levels: anova(aov(lifeExp ~ log(gdpPercap) + continent + year, data = gapminder_unfiltered)) ## Analysis of Variance Table ## ## Response: lifeExp ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## log(gdpPercap) 1 328670 328670 12994.28 &lt; 2.2e-16 *** ## continent 5 24795 4959 196.06 &lt; 2.2e-16 *** ## year 1 21950 21950 867.82 &lt; 2.2e-16 *** ## Residuals 3305 83595 25 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 However, ANOVA isn’t particularly useful for most purposes, as it just reports if a variable has an impact on your response, rather than how it does so (more on that here or here), so we won’t go into depth with it. Suffice it to say that ANOVA is a special type of linear regression, sometimes used in science, that isn’t particularly useful for most other applications. Anyway. The last thing I want to mention is that the model assumes each predictor variable is independent from each other, and they don’t vary with one another. However, we can be pretty sure that isn’t true for GDP and continent - we can generally assume that most countries in Oceania have a higher GDP per capita than most countries in Africa, for instance. As such, we should include an interaction term between those two variables - which we can do by replacing the + between those terms in our model statement with *. summary(lm(lifeExp ~ log(gdpPercap) * continent + year, data = gapminder_unfiltered)) ## ## Call: ## lm(formula = lifeExp ~ log(gdpPercap) * continent + year, data = gapminder_unfiltered) ## ## Residuals: ## Min 1Q Median 3Q Max ## -24.4999 -2.1933 0.4236 2.3497 15.3682 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.441e+02 1.124e+01 -30.604 &lt; 2e-16 ## log(gdpPercap) 5.013e+00 2.285e-01 21.942 &lt; 2e-16 ## continentAmericas -4.510e+00 2.922e+00 -1.543 0.1228 ## continentAsia 5.749e+00 2.065e+00 2.784 0.0054 ## continentEurope 3.166e+01 2.663e+00 11.887 &lt; 2e-16 ## continentFSU 5.070e+01 7.517e+00 6.745 1.80e-11 ## continentOceania -1.465e+00 3.941e+00 -0.372 0.7100 ## year 1.803e-01 5.694e-03 31.661 &lt; 2e-16 ## log(gdpPercap):continentAmericas 1.563e+00 3.514e-01 4.447 8.99e-06 ## log(gdpPercap):continentAsia 2.812e-01 2.705e-01 1.040 0.2985 ## log(gdpPercap):continentEurope -2.037e+00 3.142e-01 -6.481 1.05e-10 ## log(gdpPercap):continentFSU -4.663e+00 8.633e-01 -5.402 7.07e-08 ## log(gdpPercap):continentOceania 1.283e+00 4.477e-01 2.865 0.0042 ## ## (Intercept) *** ## log(gdpPercap) *** ## continentAmericas ## continentAsia ** ## continentEurope *** ## continentFSU *** ## continentOceania ## year *** ## log(gdpPercap):continentAmericas *** ## log(gdpPercap):continentAsia ## log(gdpPercap):continentEurope *** ## log(gdpPercap):continentFSU *** ## log(gdpPercap):continentOceania ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.908 on 3300 degrees of freedom ## Multiple R-squared: 0.8268, Adjusted R-squared: 0.8262 ## F-statistic: 1313 on 12 and 3300 DF, p-value: &lt; 2.2e-16 And as we can see, our adjusted R2 has gone up even further, to 0.826! Note, by the way, that you can also add interaction terms very explicitly to your model by adding the interaction term (written as Term1:Term2) to the formula with +. In our case, that would look like this: summary(lm(lifeExp ~ log(gdpPercap) + continent + year + (log(gdpPercap)):continent, data = gapminder_unfiltered)) That line of code would give us the same output as using * above. 6.3 Model Predictions We can now use our model to generate predictions! To do so, we first want to assign our model to an object, to make our code a little easier to understand: gapMod &lt;- lm(lifeExp ~ log(gdpPercap) + continent + year + (log(gdpPercap)):continent, data = gapminder_unfiltered) We’ll now clone our dataset into a new dataframe, which is also where we’re going to store predictions. We could skip this step and just add the columns to the gapminder dataset, but I don’t like changing our base dataframe, just in case something goes wrong - I usually prefer to edit copied dataframes. gapPred &lt;- gapminder_unfiltered We’re now ready to make our predictions, which we’ll store in a column called “predict”. We can use the predict() function to do this, which (with linear models) requires two arguements: first, what model you’re using to make your predictions, and second, what data you’re using to make these predictions. The code to do this looks something like this: gapPred &lt;- gapPred %&gt;% mutate(predict = predict(gapMod, newdata = gapPred)) (This, by the way, is also how you’d use your models to generate forecasts, when you don’t actually know the true value you’re predicting. We’re working with complete datasets in this unit to demonstrate how to assess the results of your predictions, but there’s no real difference between this application and the forecasting techniques professionals use these sorts of models for.) We can preview our predictions by peaking at the dataframe - remember, we’re comparing the lifeExp and predict columns: head(gapPred) ## # A tibble: 6 x 7 ## country continent year lifeExp pop gdpPercap predict ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. 48.8 ## 2 Afghanistan Asia 1957 30.3 9240934 821. 50.0 ## 3 Afghanistan Asia 1962 32.0 10267083 853. 51.1 ## 4 Afghanistan Asia 1967 34.0 11537966 836. 51.9 ## 5 Afghanistan Asia 1972 36.1 13079460 740. 52.1 ## 6 Afghanistan Asia 1977 38.4 14880372 786. 53.3 That doesn’t give me a ton of confidence - but remember, we’re only looking at 6 out of more than 3,300 predictions! We could attempt plotting the predictions over the real values: ggplot(gapPred, aes(gdpPercap)) + geom_point(aes(y = lifeExp)) + geom_point(aes(y = predict), color = &quot;blue&quot;, alpha = 0.25) (Remember, alpha = 0.25 made the blue points 75% transparent - or, well, 25% opaque.) But while I can tell that our predictions seem to generally fit the right pattern, I have no way of telling how well any individual prediction performed - it’s possible all our highest points represent the lowest values, for instance. A better way is to calculate the residuals for your model by subtracting the real values (our life expectancy) from the predictions. This number shows how good any given prediction is, and is actually what R2 is based on - a higher R2 means a model has generally lower residuals. We’ll then plot those residuals against our predictions, to see how well we did: gapPred %&gt;% mutate(resid = predict-lifeExp) %&gt;% ggplot(aes(predict, resid)) + geom_point() + annotate(&quot;segment&quot;, x = -Inf, xend = Inf, y = 0, yend = 0, color = &quot;red&quot;) (See what I did there, by using ggplot() with the pipe? Note that you still have to use + to add things to your plots.) So it looks like our model does an okay job predicting life expectancy - most of the predictions are within five years, though some are pretty dramatically off. The plot also exhibits heteroscedasticity and nonlinearity, neither of which I’m going to get into here - check out this post for more information. The short version is that we could certainly improve our model, by getting into a little more advanced statistics - but the model we’ve built does an okay job, and could probably be used to generate predictions. (By the way, you can get a very quick version of this plot - alongside a few other useful model plots - using plot(YourModel) - so in this case, plot(gapMod)). 6.4 Classification These linear models we’ve been building have been good for predicting numeric values - the number of years a person might live, or (in other situations) how many customers will click a link or how many frogs will be in a pond. That’s because we’ve been building regression models, which are useful for these sorts of tasks. However, we often want to predict events or categories, not just a numeric value - we want to know what species a flower might be, whether or not someone will click a link we email them, or if a tree will die this year. For these tasks, we need to use a classification model. To walk through how these work, let’s start using a new dataset included in base R - mtcars: head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 To understand what each of these column abbreviations mean, try typing ?mtcars into the console. What we want to do is predict if cars have either automatic or manual transmissions, which is coded in the am column - 0 means a car is an automatic, while 1 represents a manual - based on the MPG and horsepower of the car. If we went through the same steps as we did for the linear model, we’d then go ahead and make a quick graph to see how well a linear model fits our data: ggplot(mtcars, aes(mpg, am)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = F) Judging based on our past models, this doesn’t look great! Our model isn’t particularly close to any points. That’s a natural occurance when trying to classify data, however. Our model isn’t actually showing us what value a point has - instead, it’s showing us the probability that a data point belongs to the 1 group (in this example, if a car has a manual transmission). You can then decide how certain you want to be before assigning the data point to that 1 group - usually, if the model gives a 50% or more probability to being a 1, we assign it to that group. We can visualize what the results of this process would look like for our dataset with our very simple model only using MPG - the red triangles are misclassified, while the green circles were accurately guessed by the model: So our model got it about 78% right - not bad, for a single variable model! This is a fully acceptable use of a linear model for classification - it’s simple and mostly does the job. If you’ve ever worked with economics research, you’ve probably seen linear models used this way before. 6.5 Logistic Models However, there are a few statistics-based reasons that we don’t want to use that simple straight line for classification problems. If your data has heavy tails - that is, if there are points that are going to have something like 5% or 95% chances of belonging in the 1 group, the linear formula begins to break down somewhat. It also performs badly when your predictors are, well, non-linear - that is, when both high and low values of a predictor make being a 0 more likely, with middle values tending to be 1s. Instead, we want to use a logistic model, which will give us a line that looks more like this: ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; That curvy line works exactly the same way our linear model did - points with higher than 50% probability of being “1”s are assigned a 1, and so on. And you can see that it classified our data slightly better, hitting 81% accuracy with exactly the same model formula as the linear had. Not everyone is in agreement here - for instance, here is a professor arguing that the simplicity of linear models make them the best choice for the majority of data sets. However, I think the two models are in practice as easy to implement and understand, and the logistic model is usually a better classifier. As such, I usually rely on logistic models to classify my datasets. Making these models is a pretty similar process to making linear models. We’ll use the glm() function, rather than lm(), as the logistic model is one of the family of algorithms known as a generalized linear model. We’ll still need to supply the formula and data arguments, just like with lm(). The main difference with glm() is that we also need to specify the argument family. In this case, we want to specify that family = binomial, which will calculate the logistic model for us. With all that in mind, we can build our model with the following line of code: glm(am ~ mpg, data = mtcars, family = binomial) ## ## Call: glm(formula = am ~ mpg, family = binomial, data = mtcars) ## ## Coefficients: ## (Intercept) mpg ## -6.604 0.307 ## ## Degrees of Freedom: 31 Total (i.e. Null); 30 Residual ## Null Deviance: 43.23 ## Residual Deviance: 29.68 AIC: 33.68 And we can see the model summary just like we did with lm(): logmod &lt;- glm(am ~ mpg, data = mtcars, family = binomial) summary(logmod) ## ## Call: ## glm(formula = am ~ mpg, family = binomial, data = mtcars) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.5701 -0.7531 -0.4245 0.5866 2.0617 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -6.6035 2.3514 -2.808 0.00498 ** ## mpg 0.3070 0.1148 2.673 0.00751 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.230 on 31 degrees of freedom ## Residual deviance: 29.675 on 30 degrees of freedom ## AIC: 33.675 ## ## Number of Fisher Scoring iterations: 5 (Go ahead and save the model in logmod, like I just did - we’ll use that in a second). We could go ahead and drill down our model even further, adding other variables and changing the formula just like we did with linear models earlier: summary(glm(am ~ hp + wt, data = mtcars, family=&quot;binomial&quot;)) ## ## Call: ## glm(formula = am ~ hp + wt, family = &quot;binomial&quot;, data = mtcars) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2537 -0.1568 -0.0168 0.1543 1.3449 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 18.86630 7.44356 2.535 0.01126 * ## hp 0.03626 0.01773 2.044 0.04091 * ## wt -8.08348 3.06868 -2.634 0.00843 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.230 on 31 degrees of freedom ## Residual deviance: 10.059 on 29 degrees of freedom ## AIC: 16.059 ## ## Number of Fisher Scoring iterations: 8 In fact, logistic models made with the glm() function work pretty much the same as linear models made with lm(). There are only two big differences that I want to highlight, as far as use cases are concerned. The first of these is when using the model to make predictions. For instance, let’s take that model I just made above and assign it to mtmod: mtmod &lt;- glm(am ~ hp + wt, data = mtcars, family=&quot;binomial&quot;) Remember earlier, how we could generate predictions from our models using predict(model, data)? Let’s look at what happens if we did something similar here: head(predict(mtmod, mtcars)) ## Mazda RX4 Mazda RX4 Wag Datsun 710 Hornet 4 Drive ## 1.6757093 -0.3855769 3.4844067 -3.1339584 ## Hornet Sportabout Valiant ## -2.5961266 -5.2956878 That doesn’t look right at all! By this interpretation, the Valiant has a -530% chance of having a manual transmission - a number which shouldn’t be possible, as probabilities don’t go below 0. The explanation for why this happens, but it boils down to R trying to use our model to make predictions like it’s a linear model, not a logistic one. To fix this, we just have to specify type = &quot;response&quot; in our predict() call: head(predict(mtmod, mtcars, type = &quot;response&quot;)) ## Mazda RX4 Mazda RX4 Wag Datsun 710 Hornet 4 Drive ## 0.842335537 0.404782533 0.970240822 0.041728035 ## Hornet Sportabout Valiant ## 0.069388122 0.004988159 And voila, we generate probabilities. 6.6 Evaluating and Comparing Models The second thing you might have noticed in our logistic models section is that I didn’t mention R2 even once. That’s because R2 doesn’t really work for logistic models - we don’t really have a good equivalent of a “residual” to use to calculate it from. If we’re deadset on comparing our models using R2, we can choose from one of several pseudo-R2 provided by the pR2 function from the pscl package: install.packages(&quot;pscl&quot;) pscl::pR2(mtmod) ## llh llhNull G2 McFadden r2ML r2CU ## -5.0295552 -21.6148666 33.1706228 0.7673104 0.6453351 0.8708970 If you’re going to do this, I reccomend using the McFadden R2, or the r2CU (which stands for Cragg and Uhler, by the way). However, whichever pseudo-R2 you use, make sure you stay consistent - the different pseudo-R2 cannot be compared to one another. None of these can be compared to the R2 you’ll get off a linear model, either - they’re a completely different metric. These pseudo-R2 aren’t quite the most common method used to compare models, however. Much more commonly used is the Akaike Information Criterion, also called AIC. You may have noticed that AIC prints at the bottom of the summary() printouts for logistic models: summary(logmod) ## ## Call: ## glm(formula = am ~ mpg, family = binomial, data = mtcars) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.5701 -0.7531 -0.4245 0.5866 2.0617 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -6.6035 2.3514 -2.808 0.00498 ** ## mpg 0.3070 0.1148 2.673 0.00751 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.230 on 31 degrees of freedom ## Residual deviance: 29.675 on 30 degrees of freedom ## AIC: 33.675 ## ## Number of Fisher Scoring iterations: 5 And if we want, we can get just the AIC by selecting it with $: summary(mtmod)$aic ## [1] 16.05911 It’s important to understand what exactly AIC can be used for, as it’s frequently misapplied in both academia and industry. AIC can be used effectively in three ways: Model selection, where you compare every combination of scientifically justifiable variables possible OR Variable analysis where you either compare your response variable against each predictor variable independently OR Variable analysis where you standardize your predictor variables, put them into a model, and compare their coefficients For all of these, AIC can only be used to compare models of the same type (so logistic to logistic is fine, logistic to linear isn’t), created on the same datasets, created using the same function. Otherwise, your comparisons will be meaningless and misleading - not adjectives you want associated with your work! When comparing models, keep in mind that AIC is an (almost) arbitrary number that gets larger as your dataset gets larger - what’s important is not how large an AIC is overall, but how much larger (or smaller) it is than the AIC of your other models. That difference in AIC values is also known as \\(\\Delta\\)AIC. Your best model is going to be the one with the lowest AIC value - to compare models, subtract the smaller AIC value from the larger and then reference the chart below: library(knitr) library(kableExtra) ## ## Attaching package: &#39;kableExtra&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## group_rows tibble(&quot;If the difference in AIC is:&quot; = c(&quot;0-2&quot;, &quot;2-4&quot;, &quot;4+&quot;), &quot;The model with lowest AIC is:&quot; = c(&quot;Identical to the other one&quot;, &quot;Maybe better&quot;, &quot;Definitely better&quot;)) %&gt;% kable() %&gt;% kable_styling(&quot;striped&quot;) If the difference in AIC is: The model with lowest AIC is: 0-2 Identical to the other one 2-4 Maybe better 4+ Definitely better For our example here, we can just quickly subtract the AIC values for both of our models: summary(logmod)$aic - summary(mtmod)$aic ## [1] 17.61606 As the \\(\\Delta\\)AIC between these models is 17.6, we can very confidently say that our mtmod object is the better of these two models. 6.6.1 Confusion Matrices However, as I mentioned earlier, AIC can’t really be used to compare models created by different functions. That can be a challenge when we want to compare, for instance, a linear and logistic classifier. Luckily enough, there are other ways we can go about solving this issue. Let’s go ahead and create a linear version of our mtmod function, to start off with: linmod &lt;- lm(am ~ hp + wt, data = mtcars) We’ll now make a clone of mtcars called mtcarspred, and create two columns for our predictions - lin for the linear model, and log for the logistic. Note that for both of these I’m wrapping the predict() function in round(, 0) - this will automatically change the probabilities we’re getting from predict() into our 0/1 classification system. mtcarspred &lt;- mtcars mtcarspred$lin &lt;- round(predict(linmod, mtcarspred), 0) mtcarspred$log &lt;- round(predict(mtmod, mtcarspred, type = &quot;response&quot;), 0) We’re now going to make what’s known as a confusion matrix for each of these predictions. This matrix will show us exactly how our model predictions differ from one another, and let us get a good sense of how accurate each model is. To make it, first we have to install the caret package: install.packages(&quot;caret&quot;) And now we can use the confusionMatrix() function to create our matrices. To do this, we pass the function factored versions of our prediction and true value columns - in that order, or else we’ll mess up a few of the calculations. caret::confusionMatrix(factor(mtcarspred$lin), factor(mtcarspred$am)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 17 1 ## 1 2 12 ## ## Accuracy : 0.9062 ## 95% CI : (0.7498, 0.9802) ## No Information Rate : 0.5938 ## P-Value [Acc &gt; NIR] : 0.000105 ## ## Kappa : 0.808 ## ## Mcnemar&#39;s Test P-Value : 1.000000 ## ## Sensitivity : 0.8947 ## Specificity : 0.9231 ## Pos Pred Value : 0.9444 ## Neg Pred Value : 0.8571 ## Prevalence : 0.5938 ## Detection Rate : 0.5312 ## Detection Prevalence : 0.5625 ## Balanced Accuracy : 0.9089 ## ## &#39;Positive&#39; Class : 0 ## caret::confusionMatrix(factor(mtcarspred$log), factor(mtcarspred$am)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 18 1 ## 1 1 12 ## ## Accuracy : 0.9375 ## 95% CI : (0.7919, 0.9923) ## No Information Rate : 0.5938 ## P-Value [Acc &gt; NIR] : 1.452e-05 ## ## Kappa : 0.8704 ## ## Mcnemar&#39;s Test P-Value : 1 ## ## Sensitivity : 0.9474 ## Specificity : 0.9231 ## Pos Pred Value : 0.9474 ## Neg Pred Value : 0.9231 ## Prevalence : 0.5938 ## Detection Rate : 0.5625 ## Detection Prevalence : 0.5938 ## Balanced Accuracy : 0.9352 ## ## &#39;Positive&#39; Class : 0 ## As we can see, that function gives us a lot to work with. One of the most interesting to us is the matrix up top, which compares your predicted values to the actual ones. This, in combination with the Accuracy metric right below it, give us a sense for how our model performs, and exactly what it messes up on. There may be cases where you choose your model based more on this matrix than the actual accuracy - for instance, if a healthcare diagnostic model is slightly less accurate but generates fewer false negatives, erring on the side of caution might actually help save people’s lives. For our example, however, it’s enough to just look at the accuracy metric. And once again, our logistic model outperforms the linear formula - a result that becomes even more common once you’re working with larger and larger datasets. 6.7 Conclusion We’ve now covered the two most common forms of models, and seen applications of modeling for both regression and classification. We’ve gone over how to select your best model, and how to use that best model to predict both values already in your dataset and values you don’t know yet. But even still, we’ve barely even scratched the surface of modeling - this is the sort of thing people get very expensive degrees in. Hopefully this chapter has helped you learn a bit more about models, and how to make them in R. Our next chapter returns to working with ggplot2 and similar packages, in order to help you share all your hard work - after all, what’s the point of an analysis no one else sees? 6.8 Exercises What is the difference between regression and classification? What model types are better suited for each? Find the best model formula for predicting am in the mtcars dataset. Use AIC to compare models, and feel free to transform the data. "],
["achieving-graphical-excellence.html", "7 Achieving Graphical Excellence 7.1 Introduction 7.2 Getting Started 7.3 Themes 7.4 Colors 7.5 Labels 7.6 Animation 7.7 Specialized Visualizations 7.8 Rearranging Groups 7.9 Further Reading", " 7 Achieving Graphical Excellence Figures often beguile me, particularly when I have the arranging of them myself; in which case the remark attributed to Disraeli would often apply with justice and force: ’There are three kinds of lies: lies, damned lies, and statistics. — Mark Twain 7.1 Introduction This chapter is primarily designed to get you thinking about what exactly is possible with graphs made in R. We aren’t going to cover every possible tweak you can make to a graphic, since that’s almost impossible. We won’t showcase every extension and theme you could make use of, because that’s probably truly impossible. But by expanding our understanding about what exactly is possible with these graphics, we can begin to understand how to make the best visualizations with our own data possible. This unit is a bit easier and a bit less involved than the past few - don’t worry, we’ll be back to the hard stuff come unit 9. So far, we’ve used graphs repeatedly to help communicate and understand our data. While the fast graphics we’ve been using are more than sufficient for our own analyses, making graphics for publication or presentation requires a little more finesse. That’s where this unit comes in - we’ll be briefly touching on many of the options you have control over to make your graphics look exactly as you want them to. Before we get started, let’s load the tidyverse: library(tidyverse) ## Registered S3 methods overwritten by &#39;ggplot2&#39;: ## method from ## [.quosures rlang ## c.quosures rlang ## print.quosures rlang ## Registered S3 method overwritten by &#39;rvest&#39;: ## method from ## read_xml.response xml2 ## ── Attaching packages ─────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.1.1 ✔ purrr 0.3.2 ## ✔ tibble 2.1.1 ✔ dplyr 0.8.0.1 ## ✔ tidyr 0.8.3 ✔ stringr 1.4.0 ## ✔ readr 1.3.1 ✔ forcats 0.4.0 ## ── Conflicts ────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() 7.2 Getting Started Look at this scatterplot: Hopefully we can already tell that this isn’t a great graph. The complete lack of text means we have no idea what data are being visualized, or what the takeaway message is supposed to be. Remember, graphs are for storytelling and demonstrating your point, not necessarily for giving exact values - you should include tables in your document if the exact values are important. Even so, we can tell just from this scatterplot which points have larger values than others - they’re the ones further up and to the right. That’s because we’ve been trained to see position as an ordered aesthetic in graphs. Position isn’t the only way to communicate which values are larger than others. For instance, if we want to show the level of a third variable, we can use color: Which of these have a larger value of that third variable? Most people would assume the darker colors have the larger values, due to their higher contrast with the background. If we make the contrast less obvious, it becomes much harder to tell what the color is supposed to convey: But at the same time, even contrast isn’t quite enough for us to automatically interpret a color in a graph. For instance, the rainbow colors have different amounts of contrast against a white background, but when plotted: Which values are higher now? The colors that we use in a particular graph - or, at least, the hue of the colors - don’t really matter. What matters are the shade and intensity of these colors - these are the attributes that humans will use to draw conclusions based on these colors. Usually, darker shades and more intense colors imply a “larger” (or, at any rate, more extreme) value, while the lighter and fainter colors represent less extreme values. Moving away from color, we can also use other aesthetics to communicate a third variable. For instance: Which values are larger? We have one last aesthetic that we can use to show our third variable - the shape of the points: Which values are larger? As we can see, some aesthetics communicate quantitative data very well, while others should only be used for qualitative purposes. We already knew this - we touched on it in our first unit. But getting a sense of what representations are appropriate for our data - and what sorts of things we’re able to do with it - is the first step towards creating worthwhile graphics for whatever business or research purpose you have. For the rest of this unit, we’ll be working through all the various controls that ggplot (and other R packages) give us over our data visualizations. While you’ll still likely have to google some solutions for your own particular problems, this unit should give you a good idea of what’s possible with R graphics. Many of these solutions will make use of ggplot extensions, documented at this website. 7.3 Themes Take, for example, the basic graph: ggplot(mpg, aes(cty, hwy)) + geom_point(aes(color = class)) This graph works fine - it’s not particularly attractive, but we can understand what’s going on decently well. However, if we wanted to further control each element of our graph, we’re more than able to do so using theme(), alongside a few other functions. Below is a demonstration of some of the most commonly used theme elements - but there’s a whole world of possibilities beyond what we’ll get into here. This is a situation where Google is your best friend - googling “how do I ____ ggplot” almost always gets the right answer to your question. Generally speaking, arguments inside of theme() all follow a general pattern. First, you specify what plot element exactly you want to tweak - usually named plot.XX.XX or so on. Then, you specify what type of object it is - element_rect() for rectangular elements, element_line() for lines on the plot, element_text() for, well, text, and element_blank() for anything you want to not be included at all. I’ve explained what each action below does in comments (using ##). You can choose different specifications for almost everything I demonstrated - these are just examples to give you an understanding of what you’re capable of controlling. ## Create the ggplot object ggplot(mpg, aes(cty, hwy)) + scale_color_discrete( ## Change the default name for the legend name = &quot;Vehicle Class&quot;, ## Change the default name for each legend object ## anything you don&#39;t include will become NA labels = c(&quot;Two Seater&quot;, &quot;Compact&quot;, &quot;Midsize&quot;, &quot;Minivan&quot;, &quot;Pickup&quot;, &quot;Subcompact&quot;, &quot;SUV&quot;)) + theme( ## Remove the margins around the plot - useful when embedding the plot in another document ## Numbers are the top/right/bottom/left margin ## Change &quot;in&quot; to use a different unit plot.margin = unit(c(0,0,0,0), &quot;in&quot;), ## Change the background of the larger plot itself plot.background = element_rect(fill = &quot;beige&quot;), ## Replace the grey background with a white one panel.background = element_rect(fill = &quot;white&quot;), ## Add an x axis line axis.line.x.bottom = element_line(color = &quot;black&quot;), ## Add a y axis line axis.line.y.left = element_line(color = &quot;black&quot;), ## Make the text size 10 text = element_text(size = 10), ## Make the axis text size 10 and black axis.text = element_text(size = 10, color = &quot;black&quot;), ## Replace that ugly grey box with a white background legend.key = element_rect(fill = &quot;white&quot;), ## Recolor the legend box&#39;s background legend.background = element_rect(fill = &quot;grey&quot;), ## Move the legend to the top of the graph legend.position = &quot;top&quot;, ## Add gridlines to the graph along the axis major breaks ## Use panel.grid for both major and minor lines ## Or panel.grid.minor to just do minor lines ## This is our last argument in theme() panel.grid.major = element_line(color = &quot;grey90&quot;)) + ## Override the other aesthetics for the color legend ## In this case, make the points larger in the legend than the graph guides(color = guide_legend(override.aes = list(size = 3))) + ## Change the x and y axis labels labs(x = &quot;City Miles per Gallon&quot;, y = &quot;Highway Miles per Gallon&quot;) + ## Create a larger red point behind each compact car to highlight their location geom_point(data = filter(mpg, class == &quot;compact&quot;), size = 3, color = &quot;red&quot;) + ## Plot the data on top of the theme and highlights geom_point(aes(color = class)) + ## Control the x axis ## Expand = how far past the limits to draw the graph ## Limits = where to end the axis ## Breaks = where to draw tick marks and grid lines scale_x_continuous( expand = c(0,0), limits = c(0,41), breaks = c(0, 5, 10, 15, 20, 25, 30, 35, 40)) You can also save your basic preferences into a function of their own, and then add that to your graphs. This makes preparing multiple graphs for publications or presentations much easier - you save a ton of code replication this way. theme_publishable &lt;- theme( plot.background = element_rect(fill = &quot;white&quot;), panel.background = element_rect(fill = &quot;white&quot;), axis.line.x.bottom = element_line(color = &quot;black&quot;), axis.line.y.left = element_line(color = &quot;black&quot;), text = element_text(size = 10), axis.text = element_text(size = 10, color = &quot;black&quot;), legend.key = element_rect(fill = &quot;white&quot;), legend.background = element_rect(fill = &quot;white&quot;), panel.grid.major = element_line(color = &quot;grey90&quot;)) ggplot(mpg, aes(cty, hwy)) + geom_point(data = filter(mpg, class == &quot;compact&quot;), size = 3, color = &quot;red&quot;) + geom_point(aes(color = class)) + scale_color_discrete( name = &quot;Vehicle Class&quot;, labels = c(&quot;Two Seater&quot;, &quot;Compact&quot;, &quot;Midsize&quot;, &quot;Minivan&quot;, &quot;Pickup&quot;, &quot;Subcompact&quot;, &quot;SUV&quot;)) + labs(x = &quot;City Miles per Gallon&quot;, y = &quot;Highway Miles per Gallon&quot;) + guides(color = guide_legend(override.aes = list(size = 3))) + scale_x_continuous( expand = c(0,0), limits = c(0,41), breaks = c(0, 5, 10, 15, 20, 25, 30, 35, 40)) + theme_publishable (As a sidenote, I usually have pretty similar themes for printed and presented graphics. The biggest difference comes in text sizes - for presentations, the text argument becomes size 24, while the axis text becomes size 20.) If this is all a little intimidating, don’t worry - a lot of people have developed packages with handcrafted themes in them for you to use. For instance, in addition to the cowplot package we’ve been using, there’s ggthemes, which includes a number of palettes and themes - I’m only demonstrating one below: install.packages(&quot;ggthemes&quot;) ggplot(mpg, aes(cty, hwy)) + geom_point(aes(color = class)) + labs(x = &quot;City Miles per Gallon&quot;, y = &quot;Highway Miles per Gallon&quot;) + guides(color = guide_legend(override.aes = list(size = 3))) + scale_x_continuous( expand = c(0,0), limits = c(0,41), breaks = c(0, 5, 10, 15, 20, 25, 30, 35, 40)) + ggthemes::theme_pander() 7.4 Colors As we showed above, the colors you use in a graph can really help - or hinder! - communicating your data. Luckily, there’s plenty of aids in R to help you use color effectively. Just remember that more color isn’t always better - for instance, look at this graph: It’s colorful, sure, but the colors don’t add any information to the graph - if anything, they confuse the message. You should be sparing in your use of color - as the old saying goes: Everything should be made as simple as possible - but no simpler. 7.4.1 Viridis One of the most popular color scale packages is the viridis color scale, designed to provide colorblind-friendly color scales for your graphics. More information on the available palettes may be found here. To use the palettes, just load viridis and add scale_color_viridis to your graph, specifying which palette you want with option =. install.packages(&quot;viridis&quot;) ggplot(mpg, aes(cty, hwy)) + geom_point(aes(color = displ)) + viridis::scale_color_viridis(option = &quot;C&quot;) In order to apply the viridis palette to a discrete scale, just specify discrete = TRUE: ggplot(mpg, aes(cty, hwy)) + geom_point(aes(color = class)) + viridis::scale_color_viridis(discrete = TRUE) 7.4.2 Color Brewer If you want a few more options for color scales, the RColorBrewer package offers plenty of choices. Originally designed to help make attractive maps, the Color Brewer paettes offer palettes designed to be printer and colorblind friendly. You can see the full list of palettes at the interactive Color Brewer website. To use the package with discrete values, just type in scale_color_brewer() or scale_fill_brewer() and specify your palette: install.packages(&quot;RColorBrewer&quot;) ggplot(mpg, aes(cty, hwy)) + geom_point(aes(color = class)) + scale_color_brewer(palette = &quot;Dark2&quot;) If you want to use the Brewer scales with continuous values, just use scale_color_distiller(): ggplot(mpg, aes(cty, hwy)) + geom_point(aes(color = displ)) + scale_color_distiller(palette = &quot;PuRd&quot;) 7.4.3 Other Packages Plenty of other packages include color scales for you to use. For instance, the ggthemes package we used earlier has a number of color scales to choose from: ggplot(mpg, aes(cty, hwy)) + geom_point(aes(color = class)) + ggthemes::scale_color_pander() Similarly, ggsci has a lot of journal-specific and academic color scales for use: install.packages(&quot;ggsci&quot;) ggplot(mpg, aes(cty, hwy)) + geom_point(aes(color = class)) + ggsci::scale_color_lancet() 7.4.4 Making Your Own Of course, you aren’t restricted to the scales put together by others! If you want, you can use any of the many options in ggplot to put together scales of your own. In particular, scale_color_manual() lets you specify colors for each level you want colored, while scale_color_gradient() lets you create a gradient by specifying the low and high value colors. Both of these functions have fill versions, as well. ggplot(mpg, aes(cty, hwy)) + geom_point(aes(color = displ)) + scale_color_gradient(low = &quot;orange&quot;, high = &quot;purple&quot;) If you’re looking to specify your own colors by hand, I find sites like ColorSupply to be extremely helpful. 7.5 Labels It can be very useful to label your data, in order to make it obvious exactly what each point represents. ggplot includes two geoms specifically designed for this purpose - geom_text(), which will be on the left below, and geom_label(), on the right. Each needs the aesthetic label = in order to work properly: a &lt;- ggplot(mpg, aes(cty, hwy)) + geom_text(aes(label = model)) b &lt;- ggplot(mpg, aes(cty, hwy)) + geom_label(aes(label = model)) cowplot::plot_grid(a, b, nrow = 1) As we can see, that’s a bit of a mess! Trying to label all of our points just winds up with the labels all overlapping. One method to fix this is to only label the particularly impressive points, like so: a &lt;- ggplot(mpg, aes(cty, hwy)) + geom_point() + geom_text(data = filter(mpg, hwy &gt; 40), aes(label = model)) b &lt;- ggplot(mpg, aes(cty, hwy)) + geom_point() + geom_label(data = filter(mpg, hwy &gt; 40), aes(label = model)) cowplot::plot_grid(a, b, nrow = 1) If we want our labels to be even cleaner, we can make use of the ggrepel package: install.packages(&quot;ggrepel&quot;) a &lt;- ggplot(mpg, aes(cty, hwy)) + geom_point() + ggrepel::geom_text_repel(data = filter(mpg, hwy &gt; 40), aes(label = model)) b &lt;- ggplot(mpg, aes(cty, hwy)) + geom_point() + ggrepel::geom_label_repel(data = filter(mpg, hwy &gt; 40), aes(label = model)) cowplot::plot_grid(a, b, nrow = 1) 7.6 Animation Animated plots are both very cool and also very often ineffective - for instance, here’s a really interesting paper on silencing, or how motion distracts us from what we should actually be paying attention to. That said, the gganimate package still makes it pretty easy to create animated visuals. We won’t go too far into the specifics - you can feel free to explore animated graphs at your own leisure - but here’s an example straight from the gganimate documentation, using our old friend the gapminder data: library(gapminder) library(gganimate) ggplot(gapminder, aes(gdpPercap, lifeExp, size = pop, colour = country)) + geom_point(alpha = 0.7, show.legend = FALSE) + scale_colour_manual(values = country_colors) + scale_size(range = c(2, 12)) + scale_x_log10() + facet_wrap(~continent) + # Here comes the gganimate specific bits labs(title = &#39;Year: {frame_time}&#39;, x = &#39;GDP per capita&#39;, y = &#39;life expectancy&#39;) + transition_time(year) + ease_aes(&#39;linear&#39;) 7.7 Specialized Visualizations We’re going to spend some time now working our way through a number of less-commonly used visualizations, using a variety of other packages. 7.7.1 Stacked Area Plots If you want to show how the distribution of several groups change over time, one method is to use what’s known as a stacked area chart. While very visually appealing, this chart design has a lot of the drawbacks of stacked bar plots - it can be hard to see how exactly values change (for instance, compare A and C in the plot below). However, if you’re working with timeseries data, the changes in your values are relatively obvious, or you care more about aesthetics than usefulness, this plot can be a good choice. Just specify a fill aesthetic for a geom_area() plot: df &lt;- tibble(x = c(1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4), y = c(33, 70, 50, 18, 33, 10, 25, 60, 34, 20, 25, 22), z = c(&quot;A&quot;,&quot;A&quot;,&quot;A&quot;,&quot;A&quot;, &quot;B&quot;,&quot;B&quot;,&quot;B&quot;,&quot;B&quot;, &quot;C&quot;,&quot;C&quot;,&quot;C&quot;,&quot;C&quot;)) df %&gt;% ggplot(aes(x, y, fill = z)) + geom_area(alpha = 0.9, color = &quot;black&quot;) + scale_fill_brewer(palette = &quot;Dark2&quot;) 7.7.2 ggridges A similar aesthetic to the stacked area chart can be found in a ridge plot - particularly, a density ridge plot, as provided by the ggridges package: install.packages(&quot;ggridges&quot;) ggplot(iris, aes(Sepal.Length, Species)) + ggridges::geom_density_ridges(aes(fill = Species)) ## Picking joint bandwidth of 0.181 You can get plenty of cool effects using this package - check out the vignette here for more information. 7.7.3 Maps It’s relatively easy to make attractive maps using R and ggplot. We’re going to be working with the simplest versions of maps possible - while R can do a lot of the same things as GIS softwares, that’s a little more complicated than we want to get today. As such, we’ll be working with data included in two R packages, maps and mapdata. You can find out more about the data in the maps package here, and the mapdata package here. install.packages(&quot;maps&quot;) install.packages(&quot;mapdata&quot;) library(maps) ## ## Attaching package: &#39;maps&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## map library(mapdata) ggplot includes a function specifically designed to extract data from these packages, map_data(). We’re going to extract the usa map from the maps package and store it in usa. usa &lt;- map_data(&quot;usa&quot;) head(usa) ## long lat group order region subregion ## 1 -101.4078 29.74224 1 1 main &lt;NA&gt; ## 2 -101.3906 29.74224 1 2 main &lt;NA&gt; ## 3 -101.3620 29.65056 1 3 main &lt;NA&gt; ## 4 -101.3505 29.63911 1 4 main &lt;NA&gt; ## 5 -101.3219 29.63338 1 5 main &lt;NA&gt; ## 6 -101.3047 29.64484 1 6 main &lt;NA&gt; Now we can plot the data like any other graph - our x axis is the longitude, while the y axis is latitude. We’ll use geom_polygon() for this purpose: ggplot(usa, aes(x = long, y = lat)) + geom_polygon() Gah! There’s three specific things we’re going to have to do to fix this map: We’ll need to get rid of the axes and grid - theme_void() will help with that We’ll need to fix the aspect ratio of the map, to make sure America looks like it should, using coord_fixed(). A default value of 1.3 usually works. Oh, we’re gonna have to get rid of that bigass triangle, too. Our problem is that we forgot to specify our group aesthetic - ggplot drew all the points as a single line, rather than the several lines that the dataset uses to define the country’s borders. By specifying group=group, we can make America whole again: ggplot(usa, aes(long, lat, group=group)) + geom_polygon() + theme_void() + coord_fixed(1.3) Much better! This is just the surface of what you can do with maps in R. For a more in-depth overview, check out this tutorial. 7.7.3.1 Philosophical Tangent If you already know how to use a GIS program, you might be wondering what the point of mapping with R is. There’s a few benefits to this system, but here’s what I think are the top four: R is free and works; most GIS systems are only one of those (if even) R works with a ton of data formats and allows you insane levels of control over every detail of your map R will port anywhere you need, and makes interactive maps and web-enabled systems a breeze Most importantly, any maps - and any map analyses - you use R for are reproducible. GIS systems are largely unknowable black boxes - if you don’t report each step you take, or even if you don’t fully understand the steps you took (or all the settings you set and so on), it’s impossible for anyone to verify your work without repeating your entire methodology. With R, people - be it an advisor, a reviewer, or a peer trying to solve their own problem - can load your code and see exactly how you got to where you are. 7.7.4 Circular Charts Pie charts are much maligned, but do have some benefits over bar plots - for instance, if you’re trying to represent proportional data, pie charts perform better than stacked or dodged column charts. While ggplot doesn’t include a native method for making pie charts, you could use base R’s pie() function: df &lt;- tibble(x = c(33, 33, 33), y = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;)) pie(df$x, df$y) If you want to get even more involved with circular plots, check out the circlize package, documented here. 7.8 Rearranging Groups Say you have a boxplot: ggplot(iris, aes(Species, Sepal.Length)) + geom_boxplot() And you want to rearrange the order the boxes go in. The easiest way to do this is to factor() whatever discrete variable you’re grouping your data by, which will create a ranked order for your variable. You can then specify which levels you want each factor to go in: iris$Species &lt;- factor(iris$Species, levels = c(&quot;setosa&quot;, &quot;virginica&quot;, &quot;versicolor&quot;)) ggplot(iris, aes(Species, Sepal.Length)) + geom_boxplot() Note that factors sometimes don’t play nicely with data analysis tools - to unfactor a variable, run it through as.character() or as.numeric(), depending on which you need: iris$Species &lt;- as.character(iris$Species) 7.9 Further Reading For further reading on graphical excellence, consider the following sources: A study on which graphs are most easily understood, alongside a more recent update to the study Hadley Wickham’s ggplot2 article A Few Practical Rules for the Use of Colors in Graphs Edward Tufte’s Rules for Graphical Excellence "],
["functions-and-scripting.html", "8 Functions and Scripting 8.1 Writing Functions 8.2 About Names… 8.3 Conditional Statements 8.4 Stops 8.5 Function Dependencies 8.6 Saving and Loading Functions 8.7 Loops 8.8 Mapping Functions 8.9 More Information 8.10 Exercises", " 8 Functions and Scripting “How hard can it be? What can go wrong?” —Jack Crenshaw 8.1 Writing Functions By now, we’ve started understanding how to use the prebuilt functions available for us within R. However, those functions don’t cover everything that we might need to do - and so sometimes, we need to build them for ourselves. At first, it might not make sense to spend your time writing functions, when you could just copy and paste the same code snippets to multiple places. But - in addition to this failing our rule that, if you have to repeat something more than twice, there’s a better way - copying and pasting increases the chance of typos, makes the important parts of your code harder to understand, and makes your script or notebook unnecessarily long. Additionally, if the needs of your project change - or you catch a mistake in your code - functions only need to be changed in one place. Plus, once we move into working with others and sharing your code, functions make cleaning data and standardizing analyses between partners much easier. 8.1.1 Our First Function Some functions might seem impossibly complex - coding something like ggplot’s geoms, for instance, is probably a bit beyond us right now. But others really aren’t that hard - for instance, if we wanted to code another function to calculate the mean, our code would look like this: mean2 &lt;- function(x){ MEAN &lt;- sum(x)/length(x) return(MEAN) } Note if we want to run an entire chunk of code while using curly braces ({}), we have to run our code from the very first line - trying to run the code from the middle will only evaluate the section inside of those particular braces. As you can see, there are three steps to this process: We defined the object mean2() as a function (using function()), which takes the argument x Inside the curly braces {}, we coded what the function should do - define MEAN as the sum of x divided by the number of elements in x We told R what our function should return() as an output - in this case, MEAN This is how most functions work! While the internal code can be a lot more complicated than this, at their essence, functions are just objects that manipulate their inputs before returning an output. We can compare the results of our function against base R’s mean(), to make sure we did alright: mean(iris$Sepal.Length) ## [1] 5.843333 mean2(iris$Sepal.Length) ## [1] 5.843333 Not bad! 8.1.2 Returns As a quick sidenote, not all the code you’ll see in the wild will be quite as explicit about what it return()s. For instance, the same code will run exactly like mean2() did, and is a bit shorter: mean3 &lt;- function(x){ sum(x)/length(x) } In general, functions will return the last thing they evaluate. return() is typically used to stop a function early - if it fails a conditional (as we’ll discuss below) or other similar cases. I personally like being explicit about what a function returns, but most developers will let their function return values implicitly instead. 8.1.3 More Complicated Functions Where other functions get more complicated is in the number of arguments they take. In mean2(), we defined x as our only argument. This is the data argument - it tells us what dataset we should use in our function. However, plenty of functions have detail arguments as well - for instance, the na.rm argument in mean(), or the method argument in cor.test(). These allow us to specify exactly how our function is implemented. For instance, if we wanted to code a new function, multistat(), which let us run any function on a dataset x: multistat &lt;- function(x, func = mean){ func(x) } We could now use that function to calculate standard deviation, for instance: multistat(iris$Sepal.Length, sd) ## [1] 0.8280661 But if we left the func argument blank, it would default to what it was defined as - in this case, mean: multistat(iris$Sepal.Length) ## [1] 5.843333 This is a pretty stupid reason to make a function - it doesn’t do anything that the base functions don’t, and makes your code harder to understand - but it’s good for demonstration purposes. You don’t have to give your detail arguments a default value - and it’s often helpful if you don’t, as it makes you be explicit about what you want as an output. However, if you’re expecting that you - or anyone else using your function - will usually want a particular output, you can specify the default using = like we did above. 8.2 About Names… You might remember our discussion in unit 1 about why R occasionally requires quotes around things, while othertimes it doesn’t. This is what our explanation was then: Note the quotes around “tidyverse” when you go to install it, but not when it’s inside of library(). The reason for this is a little complicated - basically, you don’t use quotes for things that are inside of R’s memory, like data, functions, and packages. You use quotes for everything else. This is still true inside of functions, but with a twist - objects that are defined inside of functions are only defined inside of that function. For instance, even though our mean functions have assigned Sepal.Length to x, we can’t then call mean(x). As such, there’s not as much worry about creatively naming objects created inside of functions - while your names should still be short and descriptive, they don’t need to be distinct from names that exist outside of the function. 8.3 Conditional Statements Let’s go back to our mean() copycat functions. Now, to be fully honest, the code for mean() is a little more complicated - the function checks to make sure your vector isn’t a character vector, removes NA values if specified, trims your data, and then calculates the mean. But we’ve come close with our basic functions! In order to come a bit closer, we’d have to make use of something known as a conditional statement. In their simplest form, these statements evaluate whether or not something is true, then return the appropriate output. The most basic version of a conditional is the if() statement. if() evaluates the statement inside of its parentheses, and then returns the result of whatever code is in the {} below it: if(TRUE == 1){ &quot;Yes&quot; } ## [1] &quot;Yes&quot; Typically, you’ll see if() statements paired with else statements, which will run the code below them if the conditional statement is false. else statements should be surrounded by {} brackets for clarity. For instance: if(TRUE == 0){ &quot;Yes&quot; } else { &quot;No&quot; } ## [1] &quot;No&quot; You can even combine the two: if(TRUE == 0){ &quot;Yes&quot; } else if(FALSE == 1){ &quot;Maybe&quot; } else { &quot;No&quot; } ## [1] &quot;No&quot; A shorter version of this format is the ifelse() statement, which works much like if statements in Excel - it evaluates the conditional statement, then returns the first value if the statement is true, or the second if it’s false: ifelse(TRUE == 1, 4, 0) ## [1] 4 If we wanted to get closer to the mean() function, then, we could do something like this: mean4 &lt;- function(x){ if(!is.numeric(x)) { return(&quot;That&#39;s not a number!&quot;) } else{ MEAN &lt;- sum(x)/length(x) return(MEAN) } } This function does exactly as well handling numeric values as our other functions: mean4(iris$Sepal.Length) ## [1] 5.843333 But makes a bit more sense when handed other types of values: mean2(&quot;h&quot;) ## Error in sum(x) : invalid &#39;type&#39; (character) of argument mean4(&quot;h&quot;) ## [1] &quot;That&#39;s not a number!&quot; 8.4 Stops The proper way to handle that sort of error-catching is to use stop() statements. While our mean4() function will ID when it’s given a non-numeric dataset, it will still return a value - in this case, the string “That’s not a number!” The problem with this is that it won’t make it obvious that something’s gone wrong - it lets the mess-up be implicit, instead of explicit. We can fix that by giving the error message to stop() instead of return: mean5 &lt;- function(x){ if(!is.numeric(x)) { stop(&quot;That&#39;s not a number!&quot;) } else{ MEAN &lt;- sum(x)/length(x) return(MEAN) } } mean5(&quot;h&quot;) ## Error in mean5(&quot;h&quot;) : That&#39;s not a number! If we think that this sort of error is worth alerting the user about, but not stopping the entire function, we can use warning() to generate warning messages: mean6 &lt;- function(x){ if(!is.numeric(x)) { warning(&quot;That&#39;s not a number! Returning NA.&quot;) return(NA_real_) } else{ MEAN &lt;- sum(x)/length(x) return(MEAN) } } mean6(&quot;h&quot;) ## Warning in mean6(&quot;h&quot;): That&#39;s not a number! Returning NA. ## [1] NA You might have noticed, by the way, that I haven’t been using descriptive object names for our mean() functions. That’s because it’s hard to come up with short, descriptive names for such similar objects - this is the problem that we solve with %&gt;% for our datasets! But still, I tripped up once or twice while writing this chapter, and accidentally called the wrong function - you should make sure you’re naming your functions much more descriptively than I am! 8.5 Function Dependencies As you’ve seen, we can include functions inside of our functions - and, in fact, most of the most useful functions do exactly this. In that last example, we used mean() and sd() inside of our multistat() function - both of which are included in base R. However, we can even use functions from other libraries if we want. For instance, we can use the describe() function from psych below - but note that we’re preceeding it with psych::, so that it’ll run even if the user doesn’t have the psych library loaded: summary_describe &lt;- function(x){ return(list(Summary = summary(x), Describe = psych::describe(x))) } summary_describe(iris$Sepal.Length) ## $Summary ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.300 5.100 5.800 5.843 6.400 7.900 ## ## $Describe ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 150 5.84 0.83 5.8 5.81 1.04 4.3 7.9 3.6 0.31 -0.61 0.07 The problem with doing this is that if your end user doesn’t have the package installed, your function will fail out. If you want to force your users to download that package, we’d code something like this: summary_describe &lt;- function(x){ if(!require(psych)){ install.packages(&quot;psych&quot;) return(list(Summary = summary(x), Describe = psych::describe(x))) } else{ return(list(Summary = summary(x), Describe = psych::describe(x))) } } summary_describe(iris$Sepal.Length) ## Loading required package: psych ## $Summary ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.300 5.100 5.800 5.843 6.400 7.900 ## ## $Describe ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 150 5.84 0.83 5.8 5.81 1.04 4.3 7.9 3.6 0.31 -0.61 0.07 (We use require() in the place of library() here as it will generate a warning message - letting our function proceed - rather than an error. More info here.) We can also require that packages be loaded in an R session, using code like this: summary_describe &lt;- function(x){ if(!require(psych)){ install.packages(&quot;psych&quot;) library(psych) return(list(Summary = summary(x), Describe = describe(x))) } else{ return(list(Summary = summary(x), Describe = describe(x))) } } summary_describe(iris$Sepal.Length) ## $Summary ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.300 5.100 5.800 5.843 6.400 7.900 ## ## $Describe ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 150 5.84 0.83 5.8 5.81 1.04 4.3 7.9 3.6 0.31 -0.61 0.07 This isn’t a great practice, though - you should only load a library inside a function if you use that library often enough that using :: becomes impractical. Otherwise, your code becomes hard for outsiders to understand - you might understand where describe() comes from, but if you have a ton of packages loaded for a function, other users will have to go digging. Also, make sure to be careful with how many other packages your functions depend upon - the more packages, the more chances something breaks when one of them updates! 8.5.0.1 Sidenote: Note that I had to use “list” in summary_describe() above to return more than one object - and that both objects are written with a $ in front of their name. One cool side effect of this is that I can ask R to return only one of the outputs: summary_describe(iris$Sepal.Length)$Describe ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 150 5.84 0.83 5.8 5.81 1.04 4.3 7.9 3.6 0.31 -0.61 0.07 This has some actual applications when using tidyverse functions - for instance, summarise() can only use single-output functions. We can get around this by doing the following: library(tidyverse) ## Registered S3 methods overwritten by &#39;ggplot2&#39;: ## method from ## [.quosures rlang ## c.quosures rlang ## print.quosures rlang ## Registered S3 method overwritten by &#39;rvest&#39;: ## method from ## read_xml.response xml2 ## ── Attaching packages ─────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.1.1 ✔ purrr 0.3.2 ## ✔ tibble 2.1.1 ✔ dplyr 0.8.0.1 ## ✔ tidyr 0.8.3 ✔ stringr 1.4.0 ## ✔ readr 1.3.1 ✔ forcats 0.4.0 ## ── Conflicts ────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ ggplot2::%+%() masks psych::%+%() ## ✖ ggplot2::alpha() masks psych::alpha() ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() iris %&gt;% group_by(Species) %&gt;% summarise(Estimate = cor.test(Sepal.Length, Sepal.Width)$estimate, pValue = cor.test(Sepal.Length, Sepal.Width)$p.value) ## # A tibble: 3 x 3 ## Species Estimate pValue ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 0.743 6.71e-10 ## 2 versicolor 0.526 8.77e- 5 ## 3 virginica 0.457 8.43e- 4 This is another workaround to the map functions we described in unit 4. Map functions have a lot of advantages to this approach - for instance, they don’t require repeating yourself as much, and they run faster - but there’s nothing wrong with doing it this way. However, not all functions return lists, so you may have some challenges if you only rely on this approach. 8.6 Saving and Loading Functions If you want to use a function in multiple scripts or notebooks, you have to save it in its own .r file - that is, its own R script file. After doing so, you’ll be able to load the function in other scripts and notebooks using the source() command, with the filename (in quotes, with the extension, case sensitive) as the only argument. Once you do that, you’ll be able to use the function as normal. Saving and loading functions in this way has a lot of the same benefits as writing functions in the first place - it cuts down on repetition, makes editing and debugging easier, and makes your code easier for other people to understand. 8.7 Loops You may have noticed a theme throughout this reader - repetition is bad. Repeating your code makes it easy to make mistakes, and makes it harder to edit things as needed. As such, there’s a concept in R - and most other programming languages - called looping, designed to cut down on repetitions. For instance, say we had a tibble: df1 &lt;- tibble(a = c(1,1,1,1), b = c(2,2,2,2)) If we wanted to multiply each column by 2, we could do the following: df1$a &lt;- df1$a * 2 df1$b &lt;- df1$b * 2 df1 ## # A tibble: 4 x 2 ## a b ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2 4 ## 2 2 4 ## 3 2 4 ## 4 2 4 But as usual, we want to find a way to reduce that repetition. The best tool in R for this sort of thing is what’s known as a for loop, which will repeat an action a specified number of times. To complete this task, we’d write a for loop that looked something like this: df1 &lt;- data.frame(a = c(1,1,1,1), b = c(2,2,2,2)) for (i in seq_along(df1)){ df1[[i]] &lt;- df1[[i]] * 2 } df1 ## a b ## 1 2 4 ## 2 2 4 ## 3 2 4 ## 4 2 4 Alright, so let’s break that down. The for() statement initializes the loop, telling R that we’re going to want to do something repeatedly. Inside the parentheses, the i in seq_along(df1) defines how many times we want to loop the code. This will make a little more sense if we see what we get from running seq_along(df1) by itself: seq_along(df1) ## [1] 1 2 seq_along() returns the position of each column of a dataframe or each element in a vector - so for a dataframe with n columns, we’ll get the output “1, 2, …, n”. While this isn’t particularly useful by itself, it becomes powerful when used to make loops. The code i in seq_along(df1) then tells our code to repeat itself as many times as there are positions. It does this by incrementing i by 1 each time the code is run, with the first iteration having a value of 1. I should note that it doesn’t matter what you use to represent i - while i is extremely common, you could use almost any object name. We then are able to act on each column of our dataframe by selecting it using [[i]]. Since i increases by 1 each time the code is looped, we keep selecting the next column in order, until we’re completely done with our loop. If we want to make a new dataframe with our output, we have to be careful to initalize it with the proper number of columns before we start our loop - if we don’t, our code will slow down significantly. Say we wanted our function to return the median of each column. Doing this the right way looks something like this: df1 &lt;- data.frame(a = c(1,1,1,1), b = c(2,2,2,2)) out &lt;- vector(&quot;numeric&quot;, length(df1)) out[1] &lt;- df1[1] for (i in seq_along(df1)){ out[[i]] &lt;- median(df1[[i]]) } out ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 2 By the way - you may have heard or read that for loops are extremely slow in R. That’s not true, anymore. While they’re slower than Python’s loops by a good margin, that’s because everything is slower than Python by a good margin - but R is pretty middle-of-the-road when it comes to loop speed. That being said, most programmers in R avoid using for loops when possible. That’s both because loops were significantly slower in R than other languages not that long ago, but also because the nature of R makes using functions more popular and efficient than loops. But there are certain times loops are unavoidable - and we’ll be using them in the next unit! For more on for loops, look at Hadley Wickham’s coverage of the subject. For our purposes, we don’t have to go much further than we already have on the subject. 8.8 Mapping Functions One way to avoid the use of loops is to replace them with map functions from purrr, as we did in the past unit. While the lists returned by map() last unit worked fine for our purposes, there are actually map functions for each type of output you might want: map() for lists map_lgl() for logicals (true/false values) map_int() for integers (whole-number numerics) map_dbl() for doubles (numerics with decimal places) map_chr() for character strings For instance, we could replace our last for loop with the following: map_dbl(df1, median) ## a b ## 1 2 Which, with the pipe, looks like this: df1 %&gt;% map_dbl(median) ## a b ## 1 2 Which is a lot cleaner to read, and easier to write! 8.9 More Information There’s a world of depth to constructing functions, with mountains of methods to make your functions work cleaner and more efficiently. The majority of R users don’t think of themselves as software developers, who need to be concerned with speed and efficiency - instead, they want things to work, preferably with as little time spent coding as possible. However, if you ever want to write functions for other people to use - or use R for more complex tasks in your own work - it’s worth understanding how to code efficiently in R. Here’s a good primer on that subject. 8.10 Exercises Remember in unit 3 that there’s no standard error function in base R. Write one, naming it std.err. Write a function that will say “Hi!” if given the input “hi”, “Bye!” if it gets the input “bye”, and “How rude!” if the input isn’t either of those. Use a loop to find the mean and median of each column of this dataframe: df &lt;- data.frame(x = c(57, 24, 245, 3526), y = c(67, 234, 574, 57)) Replace that loop with map functions. Write a program that prints the numbers from 1 to 100. But for multiples of three print “Fizz” instead of the number and for the multiples of five print “Buzz”. For numbers which are multiples of both three and five print “FizzBuzz”. "],
["more-complicated-analyses.html", "9 More Complicated Analyses 9.1 Other Datasets 9.2 Logistic Models 9.3 Modelling Metrics 9.4 More Complicated Analyses 9.5 Relational Data 9.6 Exercises", " 9 More Complicated Analyses Computers are cheap, and thinking hurts. —Uwe Ligges 9.1 Other Datasets 9.1.1 Importing Your Own Data So far in this course, we’ve been working exclusively with the data pre-available in R and a few of the packages we’ve taken advantage of. While this is a good way to learn R, at some point you may want to use your own data for analysis. Luckily, R has a number of functions used to import data from external files. To demonstrate these, I’ll be using datasets located on GitHub, in the “Unit 6 Data” folder. You don’t necessarily need to download these files - but we’ll be using these for demonstration throughout this unit. These datasets are from the website Kaggle, where a number of data professionals share methods and datasets. Specifically, we’ll be working with two datasets concerning all Olympic athletes from the first 120 years of the game. Additionally, we’ll use data on NBA athletes from the 2014-2015 season. I’m not a huge sports guy myself, but there’s a huge amount of sports-related data available publicly online, so it’s a good tool for our analyses. We’re going to be assuming that your data are located in the same folder as your script - this is usually the easiest way to manage datasets. Otherwise, you’ll have to wrestle a little with using relative pathways and directory names - Hadley Wickham explains these in more detail here. In order to do this, we’ll be using the readr package, which is included in the base tidyverse: library(tidyverse) ## Registered S3 methods overwritten by &#39;ggplot2&#39;: ## method from ## [.quosures rlang ## c.quosures rlang ## print.quosures rlang ## Registered S3 method overwritten by &#39;rvest&#39;: ## method from ## read_xml.response xml2 ## ── Attaching packages ─────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.1.1 ✔ purrr 0.3.2 ## ✔ tibble 2.1.1 ✔ dplyr 0.8.0.1 ## ✔ tidyr 0.8.3 ✔ stringr 1.4.0 ## ✔ readr 1.3.1 ✔ forcats 0.4.0 ## ── Conflicts ────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() All the datasets included in our example use different delimiters - the character that tells R (or Excel, or whatever program you’re using) where one column stops and the next one begins. No matter what your delimiter is, we can parse the file using read_delim() - the first argument is the filename to read, while the second is the delimiter used. For instance, our text file is tab delimited - and since tabs are represented in R as \\t, we’ll tell read_delim() to use that as a delimiter: NOCRegions &lt;- read_delim(&quot;noc_regions.txt&quot;, &quot;\\t&quot;) ## Parsed with column specification: ## cols( ## NOC = col_character(), ## region = col_character(), ## notes = col_character() ## ) NOCRegions ## # A tibble: 230 x 3 ## NOC region notes ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 AFG Afghanistan &lt;NA&gt; ## 2 AHO Curacao Netherlands Antilles ## 3 ALB Albania &lt;NA&gt; ## 4 ALG Algeria &lt;NA&gt; ## 5 AND Andorra &lt;NA&gt; ## 6 ANG Angola &lt;NA&gt; ## 7 ANT Antigua Antigua and Barbuda ## 8 ANZ Australia Australasia ## 9 ARG Argentina &lt;NA&gt; ## 10 ARM Armenia &lt;NA&gt; ## # … with 220 more rows We can do the same thing with documents that have comma separated values (known as CSVs): AthleteEvents &lt;- read_delim(&quot;athlete_events.csv&quot;, &quot;,&quot;) ## Parsed with column specification: ## cols( ## ID = col_double(), ## Name = col_character(), ## Sex = col_character(), ## Age = col_double(), ## Height = col_double(), ## Weight = col_double(), ## NOC = col_character(), ## Games = col_character(), ## Year = col_double(), ## Season = col_character(), ## Sport = col_character(), ## Event = col_character(), ## Medal = col_character() ## ) AthleteEvents ## # A tibble: 271,116 x 13 ## ID Name Sex Age Height Weight NOC Games Year Season Sport ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 A Di… M 24 180 80 CHN 1992… 1992 Summer Bask… ## 2 2 A La… M 23 170 60 CHN 2012… 2012 Summer Judo ## 3 3 Gunn… M 24 NA NA DEN 1920… 1920 Summer Foot… ## 4 4 Edga… M 34 NA NA DEN 1900… 1900 Summer Tug-… ## 5 5 Chri… F 21 185 82 NED 1988… 1988 Winter Spee… ## 6 5 Chri… F 21 185 82 NED 1988… 1988 Winter Spee… ## 7 5 Chri… F 25 185 82 NED 1992… 1992 Winter Spee… ## 8 5 Chri… F 25 185 82 NED 1992… 1992 Winter Spee… ## 9 5 Chri… F 27 185 82 NED 1994… 1994 Winter Spee… ## 10 5 Chri… F 27 185 82 NED 1994… 1994 Winter Spee… ## # … with 271,106 more rows, and 2 more variables: Event &lt;chr&gt;, Medal &lt;chr&gt; (You may have noticed that took a second to load - at least, if your computer is middle-of-the-road. This dataset has 271,116 rows - even more than our diamonds dataset - so analyses might take slightly longer with it. Imagine how long “big data” problems would take on your computer, when datasets have several million rows of observations!) However, readr also includes a pretty good tool specifically for CSV files: AthleteEvents &lt;- read_csv(&quot;athlete_events.csv&quot;) ## Parsed with column specification: ## cols( ## ID = col_double(), ## Name = col_character(), ## Sex = col_character(), ## Age = col_double(), ## Height = col_double(), ## Weight = col_double(), ## NOC = col_character(), ## Games = col_character(), ## Year = col_double(), ## Season = col_character(), ## Sport = col_character(), ## Event = col_character(), ## Medal = col_character() ## ) AthleteEvents ## # A tibble: 271,116 x 13 ## ID Name Sex Age Height Weight NOC Games Year Season Sport ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 A Di… M 24 180 80 CHN 1992… 1992 Summer Bask… ## 2 2 A La… M 23 170 60 CHN 2012… 2012 Summer Judo ## 3 3 Gunn… M 24 NA NA DEN 1920… 1920 Summer Foot… ## 4 4 Edga… M 34 NA NA DEN 1900… 1900 Summer Tug-… ## 5 5 Chri… F 21 185 82 NED 1988… 1988 Winter Spee… ## 6 5 Chri… F 21 185 82 NED 1988… 1988 Winter Spee… ## 7 5 Chri… F 25 185 82 NED 1992… 1992 Winter Spee… ## 8 5 Chri… F 25 185 82 NED 1992… 1992 Winter Spee… ## 9 5 Chri… F 27 185 82 NED 1994… 1994 Winter Spee… ## 10 5 Chri… F 27 185 82 NED 1994… 1994 Winter Spee… ## # … with 271,106 more rows, and 2 more variables: Event &lt;chr&gt;, Medal &lt;chr&gt; Now, readr doesn’t have native support for more complicated files, like Excel files. Philosphically, you shouldn’t store data in Excel format for long periods of time - we have no idea how long Microsoft will be around for, and the encoding used by Excel may someday disappear and take your data with it. CSVs are generally preferred for long-term data storage, as they’re easy to understand visually and are easily parsed by computers. However, data entry is much easier in Excel, and plenty of data professionals still use the format. Luckily, there’s a package - readxl - designed to parse these types of files: install.packages(&quot;readxl&quot;) library(readxl) NBAStats &lt;- read_excel(&quot;players_stats.xlsx&quot;) NBAStats ## # A tibble: 490 x 34 ## Name `Games Played` MIN PTS FGM FGA `FG%` `3PM` `3PA` `3P%` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AJ P… 26 324 133 51 137 37.2 15 57 26.3 ## 2 Aaro… 82 1885 954 344 817 42.1 121 313 38.7 ## 3 Aaro… 47 797 243 93 208 44.7 13 48 27.1 ## 4 Adre… 32 740 213 91 220 41.4 1 9 11.1 ## 5 Al H… 76 2318 1156 519 965 53.8 11 36 30.6 ## 6 Al J… 65 1992 1082 486 1010 48.1 2 5 40 ## 7 Alan… 74 1744 545 195 440 44.3 73 210 34.8 ## 8 Alec… 27 899 374 121 300 40.3 26 68 38.2 ## 9 Alex… 5 14 4 1 4 25 0 0 0 ## 10 Alex… 69 1518 432 179 353 50.7 1 3 33.3 ## # … with 480 more rows, and 24 more variables: FTM &lt;dbl&gt;, FTA &lt;dbl&gt;, ## # `FT%` &lt;dbl&gt;, OREB &lt;dbl&gt;, DREB &lt;dbl&gt;, REB &lt;dbl&gt;, AST &lt;dbl&gt;, STL &lt;dbl&gt;, ## # BLK &lt;dbl&gt;, TOV &lt;dbl&gt;, PF &lt;dbl&gt;, EFF &lt;dbl&gt;, `AST/TOV` &lt;dbl&gt;, ## # `STL/TOV` &lt;dbl&gt;, Age &lt;dbl&gt;, Birth_Place &lt;chr&gt;, Birthdate &lt;dttm&gt;, ## # Collage &lt;chr&gt;, Experience &lt;chr&gt;, Height &lt;dbl&gt;, Pos &lt;chr&gt;, Team &lt;chr&gt;, ## # Weight &lt;dbl&gt;, BMI &lt;dbl&gt; You’ll also notice that RStudio has an “import dataset” button in the top right corner, which makes use of both the readr and readxl packages. This button lets you point and click your way through data import, and then copy the code into your script. It’s a great resource for beginners! 9.1.2 Exporting Data Writing data to a file is also pretty painless using readr. There are as many options for delimiters as before - you can use write_delim() to specify which you want to use with your data - but more commonly data is imported and exported as CSV files using write_csv() write_csv(AthleteEvents, &quot;athlete_events.csv&quot;) Note that, like all other tidyverse functions, the dataset is the first thing you specify. 9.1.3 Data Exploration Let’s put NBAStats off to the side for a moment, and look at our other two datasets. AthleteEvents is a list of all olympic competitors from 1892 to 2016, including basic statistics about each and any medals they may have won. NOCRegions, meanwhile, maps codes used by the National Olympic Committee (NOC) to the countries they represent. We can get a sense of the variables this dataset measures using psych::describe() psych::describe(AthleteEvents) ## Warning in psych::describe(AthleteEvents): NAs introduced by coercion ## Warning in psych::describe(AthleteEvents): NAs introduced by coercion ## Warning in psych::describe(AthleteEvents): NAs introduced by coercion ## Warning in psych::describe(AthleteEvents): NAs introduced by coercion ## Warning in psych::describe(AthleteEvents): NAs introduced by coercion ## Warning in psych::describe(AthleteEvents): NAs introduced by coercion ## Warning in psych::describe(AthleteEvents): NAs introduced by coercion ## Warning in psych::describe(AthleteEvents): NAs introduced by coercion ## Warning in FUN(newX[, i], ...): no non-missing arguments to min; returning ## Inf ## Warning in FUN(newX[, i], ...): no non-missing arguments to min; returning ## Inf ## Warning in FUN(newX[, i], ...): no non-missing arguments to min; returning ## Inf ## Warning in FUN(newX[, i], ...): no non-missing arguments to min; returning ## Inf ## Warning in FUN(newX[, i], ...): no non-missing arguments to min; returning ## Inf ## Warning in FUN(newX[, i], ...): no non-missing arguments to min; returning ## Inf ## Warning in FUN(newX[, i], ...): no non-missing arguments to min; returning ## Inf ## Warning in FUN(newX[, i], ...): no non-missing arguments to min; returning ## Inf ## Warning in FUN(newX[, i], ...): no non-missing arguments to max; returning ## -Inf ## Warning in FUN(newX[, i], ...): no non-missing arguments to max; returning ## -Inf ## Warning in FUN(newX[, i], ...): no non-missing arguments to max; returning ## -Inf ## Warning in FUN(newX[, i], ...): no non-missing arguments to max; returning ## -Inf ## Warning in FUN(newX[, i], ...): no non-missing arguments to max; returning ## -Inf ## Warning in FUN(newX[, i], ...): no non-missing arguments to max; returning ## -Inf ## Warning in FUN(newX[, i], ...): no non-missing arguments to max; returning ## -Inf ## Warning in FUN(newX[, i], ...): no non-missing arguments to max; returning ## -Inf ## vars n mean sd median trimmed mad min max ## ID 1 271116 68248.95 39022.29 68205 68290.17 50019.96 1 135571 ## Name* 2 271116 NaN NA NA NaN NA Inf -Inf ## Sex* 3 271116 NaN NA NA NaN NA Inf -Inf ## Age 4 261642 25.56 6.39 24 24.87 4.45 10 97 ## Height 5 210945 175.34 10.52 175 175.31 10.38 127 226 ## Weight 6 208241 70.70 14.35 70 69.90 13.34 25 214 ## NOC* 7 271116 NaN NA NA NaN NA Inf -Inf ## Games* 8 271116 NaN NA NA NaN NA Inf -Inf ## Year 9 271116 1978.38 29.88 1988 1981.65 29.65 1896 2016 ## Season* 10 271116 NaN NA NA NaN NA Inf -Inf ## Sport* 11 271116 NaN NA NA NaN NA Inf -Inf ## Event* 12 271116 NaN NA NA NaN NA Inf -Inf ## Medal* 13 39783 NaN NA NA NaN NA Inf -Inf ## range skew kurtosis se ## ID 135570 0.00 -1.20 74.94 ## Name* -Inf NA NA NA ## Sex* -Inf NA NA NA ## Age 87 1.75 6.27 0.01 ## Height 99 0.02 0.18 0.02 ## Weight 189 0.80 2.02 0.03 ## NOC* -Inf NA NA NA ## Games* -Inf NA NA NA ## Year 120 -0.82 -0.21 0.06 ## Season* -Inf NA NA NA ## Sport* -Inf NA NA NA ## Event* -Inf NA NA NA ## Medal* -Inf NA NA NA Wow, R didn’t like that! R didn’t know how to calculate most of the fields in describe() for character vectors. We can try that process again with only the numeric columns by using select_if() from dplyr: psych::describe(select_if(AthleteEvents, is.numeric)) ## vars n mean sd median trimmed mad min max ## ID 1 271116 68248.95 39022.29 68205 68290.17 50019.96 1 135571 ## Age 2 261642 25.56 6.39 24 24.87 4.45 10 97 ## Height 3 210945 175.34 10.52 175 175.31 10.38 127 226 ## Weight 4 208241 70.70 14.35 70 69.90 13.34 25 214 ## Year 5 271116 1978.38 29.88 1988 1981.65 29.65 1896 2016 ## range skew kurtosis se ## ID 135570 0.00 -1.20 74.94 ## Age 87 1.75 6.27 0.01 ## Height 99 0.02 0.18 0.02 ## Weight 189 0.80 2.02 0.03 ## Year 120 -0.82 -0.21 0.06 When I see this output, three things catch my eye: We knew we were missing observations, but wow, we’re missing observations. Over 60,000 athletes have no weight listed, while over 50,000 don’t have a height. Mean height is 175.34 while mean weight is 70.7, suggesting that we’re using metric units here Looking at mean, skew and kurtosis, it seems like we have a lot more athletes in recent years - the distribution is centered around 1978, almost 100 years into our 120 year dataset - and that while an incredible number of athletes are young (25, with a kurtosis of 6.27!), we have plenty of older competitors. If we want to get a sense of correlations in the data, we can try using pairs() again - but be warned, this one might take a while due to the size of the data. pairs(select_if(AthleteEvents, is.numeric)) (Pop quiz for the history nerds: what are those vertical bars in the Year column?) That’s a little chaotic, but we can still see trends nicely enough. It looks like height and weight have increased in variance over the years, probably due to the increased number of athletes overall. Height and weight look tightly correlated, as do age and the other statistics, interestingly enough. If we look at the actual correlation coefficients, we can see these trends numerically. We’re going to have to remove the NAs manually, first, using dplyr’s drop_na AthleteEvents %&gt;% select_if(is.numeric) %&gt;% drop_na() %&gt;% cor() ## ID Age Height Weight Year ## ID 1.000000000 -0.002100308 -0.01219033 -0.009038648 0.007066518 ## Age -0.002100308 1.000000000 0.14168449 0.212040722 0.089142499 ## Height -0.012190329 0.141684490 1.00000000 0.796572579 0.048141525 ## Weight -0.009038648 0.212040722 0.79657258 1.000000000 0.022175179 ## Year 0.007066518 0.089142499 0.04814153 0.022175179 1.000000000 So we’re right in thinking that height and weight are correlated, but maybe not age. It is interesting to see that year is more correlated with age than any other variable! Moving back to the full dataset, I have a few other questions I want to ask. First off, when did women start competing? It would be useful to know if our gender distributions have significantly different lengths. Let’s count() the number of male and female athletes per year: AthleteEvents %&gt;% count(Year, Sex) ## # A tibble: 69 x 3 ## Year Sex n ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 1896 M 380 ## 2 1900 F 33 ## 3 1900 M 1903 ## 4 1904 F 16 ## 5 1904 M 1285 ## 6 1906 F 11 ## 7 1906 M 1722 ## 8 1908 F 47 ## 9 1908 M 3054 ## 10 1912 F 87 ## # … with 59 more rows The first women appear in 1900! That’s very cool. I do wonder if the male or female athletes have it harder - that is, are there more male or female medal recipients, as a proportion of the whole? For instance, if fewer countries send female participants, each participant might be more statistically more likely to win. We can eyeball this by calculating the percentage of each gender who hold each type of medal, out of the total number of athletes who have participated. To do so, we’ll count the number of recipients of each type of medal, rename the NA medal value “none”, group our data by sex, and then divide the number of people who have received each medal by the total number of contestants: AthleteEvents %&gt;% count(Sex, Medal) %&gt;% replace_na(list(Medal = NA)) %&gt;% group_by(Sex) %&gt;% mutate(PercentReceiving = n / sum(n)) ## # A tibble: 8 x 4 ## # Groups: Sex [2] ## Sex Medal n PercentReceiving ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 F &lt;NA&gt; 63269 0.849 ## 2 F Bronze 3771 0.0506 ## 3 F Gold 3747 0.0503 ## 4 F Silver 3735 0.0501 ## 5 M &lt;NA&gt; 168064 0.855 ## 6 M Bronze 9524 0.0484 ## 7 M Gold 9625 0.0490 ## 8 M Silver 9381 0.0477 So it looks like women have a tiny edge, but generally speaking, both men and women have a 15% chance of medalling at the Olympics. 9.1.4 Modeling Winners Personally, I’m interested in predicting if someone will win a medal at the Olympics. We could do this a number of inferential ways - for instance, dropping all the values where an athlete didn’t win, and looking at the summary statistics of the athletes who did: AthleteEvents %&gt;% filter(!is.na(Medal)) %&gt;% summary(object = .) ## ID Name Sex Age ## Min. : 4 Length:39783 Length:39783 Min. :10.00 ## 1st Qu.: 36494 Class :character Class :character 1st Qu.:22.00 ## Median : 68990 Mode :character Mode :character Median :25.00 ## Mean : 69407 Mean :25.93 ## 3rd Qu.:103462 3rd Qu.:29.00 ## Max. :135563 Max. :73.00 ## NA&#39;s :732 ## Height Weight NOC Games ## Min. :136.0 Min. : 28.00 Length:39783 Length:39783 ## 1st Qu.:170.0 1st Qu.: 63.00 Class :character Class :character ## Median :178.0 Median : 73.00 Mode :character Mode :character ## Mean :177.6 Mean : 73.77 ## 3rd Qu.:185.0 3rd Qu.: 83.00 ## Max. :223.0 Max. :182.00 ## NA&#39;s :8711 NA&#39;s :9327 ## Year Season Sport Event ## Min. :1896 Length:39783 Length:39783 Length:39783 ## 1st Qu.:1952 Class :character Class :character Class :character ## Median :1984 Mode :character Mode :character Mode :character ## Mean :1974 ## 3rd Qu.:2002 ## Max. :2016 ## ## Medal ## Length:39783 ## Class :character ## Mode :character ## ## ## ## This doesn’t help us that much - it appears that these athletes are slightly older, taller, and heavier than the whole dataset, but that’s not a lot to go off of when making predictions - no one wants to bet on the oldest and heaviest high jumper. One way we could try to do this is using our old friend, the lm() tool, with all the data we have. We can make a new binary column Winner, which will be 1 if the athlete won a medal, and then try to model it: AthleteEvents$Winner &lt;- AthleteEvents$Medal AthleteEvents$Winner[which(!is.na((AthleteEvents$Winner)))] &lt;- 1 AthleteEvents$Winner[which(is.na(AthleteEvents$Winner))] &lt;- 0 AthleteEvents$Winner &lt;- as.numeric(AthleteEvents$Winner) AthleteEvents %&gt;% replace_na(list(Winner = 0)) %&gt;% lm(Winner ~ ID * Name * Sex * Age * Height * Weight * NOC * Games * Year * Season * Sport * Event, data = .) ## Error: cannot allocate vector of size 72.4 Gb But R doesn’t like that much. This process is pretty excessive - we’re asking R to calculate a lot of stuff - and also has the downside of probably being too specific. After all, if we tell R what ID each athlete has, plus the year and the event, it can tell 100% of the time who won the medal! Instead, we want to be able to guess what traits make someone more likely to win. As such, we should only test the variables that we think are scientifically relevant - most likely, things like their age, height, and weight. If we make a model more like that: AthleteLinearModel &lt;- lm(Winner ~ Age * Height * Weight, data = AthleteEvents) summary(AthleteLinearModel) ## ## Call: ## lm(formula = Winner ~ Age * Height * Weight, data = AthleteEvents) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.5264 -0.1570 -0.1327 -0.1113 0.9327 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.833e-01 2.156e-01 1.314 0.18884 ## Age 1.125e-02 8.985e-03 1.252 0.21047 ## Height -2.668e-03 1.278e-03 -2.088 0.03676 * ## Weight -1.735e-03 3.201e-03 -0.542 0.58783 ## Age:Height -4.743e-06 5.274e-05 -0.090 0.92834 ## Age:Weight -3.489e-04 1.294e-04 -2.696 0.00701 ** ## Height:Weight 3.429e-05 1.791e-05 1.915 0.05549 . ## Age:Height:Weight 1.221e-06 7.231e-07 1.689 0.09125 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3517 on 206157 degrees of freedom ## (64951 observations deleted due to missingness) ## Multiple R-squared: 0.01027, Adjusted R-squared: 0.01024 ## F-statistic: 305.6 on 7 and 206157 DF, p-value: &lt; 2.2e-16 9.2 Logistic Models Wow! Every single term in our model is significant, but the R2 is horrible! As for why that might be, let’s visualize our response variable against one of its predictors - again, this might take a second to run: ggplot(AthleteEvents, aes(Height, Winner)) + geom_point() + geom_smooth(method = &quot;lm&quot;) ## Warning: Removed 60171 rows containing non-finite values (stat_smooth). ## Warning: Removed 60171 rows containing missing values (geom_point). This here is the root of our problem - we’re trying to model a binary outcome (a yes or no, a medal or not) with a straight line, which is useless. This sort of problem comes up all the time in real world situations - in science, whether or not a trap will catch something, or in business, whether or not a customer will click. These problems are known as classification problems, where the outcome is a categorical variable, and linear models are really bad at them. Instead, we can choose to use a logistic model, one of the types of generalized linear models from chapter 3. These models serve to measure the probability of a binary event - while a linear model will tell you the value of your response variable, logistic models will tell you the probability your response variable is 1. R convieniently has a function glm() for doing exactly this: AthleteLogisticModel &lt;- glm(Winner ~ Age * Height * Weight, data = AthleteEvents, family = &quot;binomial&quot;) summary(AthleteLogisticModel) ## ## Call: ## glm(formula = Winner ~ Age * Height * Weight, family = &quot;binomial&quot;, ## data = AthleteEvents) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.3873 -0.5831 -0.5351 -0.4870 2.3375 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.023e+00 1.815e+00 -2.216 0.02668 * ## Age 1.228e-01 7.358e-02 1.669 0.09514 . ## Height -2.067e-03 1.062e-02 -0.195 0.84561 ## Weight 3.083e-02 2.616e-02 1.179 0.23849 ## Age:Height -2.261e-04 4.275e-04 -0.529 0.59699 ## Age:Weight -3.099e-03 1.036e-03 -2.990 0.00279 ** ## Height:Weight 1.595e-05 1.447e-04 0.110 0.91224 ## Age:Height:Weight 1.160e-05 5.731e-06 2.023 0.04303 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 171694 on 206164 degrees of freedom ## Residual deviance: 169703 on 206157 degrees of freedom ## (64951 observations deleted due to missingness) ## AIC: 169719 ## ## Number of Fisher Scoring iterations: 4 Note that family = &quot;binomial&quot; is necessary to make R compute a logistic model - you can find other possible generalized linear model formats by typing ?family. 9.3 Modelling Metrics Well, our terms are all still significant, which is a good thing - but you’ll notice that we now have no R2! That’s because R2 terms don’t really exist for logistic models, for reasons we won’t go too far into - you can read more on the topic here. But what we will get into are other methods of evaluating logistic models. One of the more common methods of comparing logistic models is to use what’s known as a pseudo R2 value. Unfortunately, there are plenty of different methods to compute these, and you can’t compare R2 calculated with different formulas to one another. 9.3.1 Pseudo-R2 One package that gives pretty decent results is the pscl package by Simon Jackman. After we install it, we can use the pR2() function to give us some pseudo-R2 values: install.packages(&quot;pscl&quot;) pscl::pR2(AthleteLogisticModel) ## llh llhNull G2 McFadden r2ML ## -8.485169e+04 -1.130577e+05 5.641209e+04 2.494835e-01 2.393834e-01 ## r2CU ## 3.594066e-01 The McFadden R2 is given as the fourth output from this function, while the Cragg-Uhler R2 is the last value in the list. Again, these numbers can only be compared against other pseudo-R2 following the same formula, which limits their versatility. 9.3.2 Area Under the ROC Curve (AUC) More common in data analytics is to find the area under the receiver operating curve - abbreviated as AUC. To understand how we get there, let’s first take a look at our model object: AthleteLogisticModel ## ## Call: glm(formula = Winner ~ Age * Height * Weight, family = &quot;binomial&quot;, ## data = AthleteEvents) ## ## Coefficients: ## (Intercept) Age Height ## -4.023e+00 1.228e-01 -2.067e-03 ## Weight Age:Height Age:Weight ## 3.083e-02 -2.261e-04 -3.099e-03 ## Height:Weight Age:Height:Weight ## 1.595e-05 1.160e-05 ## ## Degrees of Freedom: 206164 Total (i.e. Null); 206157 Residual ## (64951 observations deleted due to missingness) ## Null Deviance: 171700 ## Residual Deviance: 169700 AIC: 169700 Wow! R stores models as list objects, containing a number of elements of different data types. We can get a sense of what’s going on under the hood using str() - that is, asking R to show us the structure of the data: str(AthleteLogisticModel) ## List of 31 ## $ coefficients : Named num [1:8] -4.023117 0.122794 -0.002067 0.030834 -0.000226 ... ## ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;(Intercept)&quot; &quot;Age&quot; &quot;Height&quot; &quot;Weight&quot; ... ## $ residuals : Named num [1:206165] -1.19 -1.14 -1.21 -1.21 -1.21 ... ## ..- attr(*, &quot;names&quot;)= chr [1:206165] &quot;1&quot; &quot;2&quot; &quot;5&quot; &quot;6&quot; ... ## $ fitted.values : Named num [1:206165] 0.158 0.122 0.171 0.171 0.173 ... ## ..- attr(*, &quot;names&quot;)= chr [1:206165] &quot;1&quot; &quot;2&quot; &quot;5&quot; &quot;6&quot; ... ## $ effects : Named num [1:206165] 277.54 11.65 -40.07 -11.55 -2.84 ... ## ..- attr(*, &quot;names&quot;)= chr [1:206165] &quot;(Intercept)&quot; &quot;Age&quot; &quot;Height&quot; &quot;Weight&quot; ... ## $ R : num [1:8, 1:8] -160 0 0 0 0 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:8] &quot;(Intercept)&quot; &quot;Age&quot; &quot;Height&quot; &quot;Weight&quot; ... ## .. ..$ : chr [1:8] &quot;(Intercept)&quot; &quot;Age&quot; &quot;Height&quot; &quot;Weight&quot; ... ## $ rank : int 8 ## $ qr :List of 5 ## ..$ qr : num [1:206165, 1:8] -1.60e+02 2.05e-03 2.36e-03 2.36e-03 2.37e-03 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:206165] &quot;1&quot; &quot;2&quot; &quot;5&quot; &quot;6&quot; ... ## .. .. ..$ : chr [1:8] &quot;(Intercept)&quot; &quot;Age&quot; &quot;Height&quot; &quot;Weight&quot; ... ## ..$ rank : int 8 ## ..$ qraux: num [1:8] 1 1 1 1 1 ... ## ..$ pivot: int [1:8] 1 2 3 4 5 6 7 8 ## ..$ tol : num 1e-11 ## ..- attr(*, &quot;class&quot;)= chr &quot;qr&quot; ## $ family :List of 12 ## ..$ family : chr &quot;binomial&quot; ## ..$ link : chr &quot;logit&quot; ## ..$ linkfun :function (mu) ## ..$ linkinv :function (eta) ## ..$ variance :function (mu) ## ..$ dev.resids:function (y, mu, wt) ## ..$ aic :function (y, n, mu, wt, dev) ## ..$ mu.eta :function (eta) ## ..$ initialize: expression({ if (NCOL(y) == 1) { if (is.factor(y)) y &lt;- y != levels(y)[1L] n &lt;- rep.int(1, nobs) y[weights =| __truncated__ ## ..$ validmu :function (mu) ## ..$ valideta :function (eta) ## ..$ simulate :function (object, nsim) ## ..- attr(*, &quot;class&quot;)= chr &quot;family&quot; ## $ linear.predictors: Named num [1:206165] -1.67 -1.98 -1.58 -1.58 -1.57 ... ## ..- attr(*, &quot;names&quot;)= chr [1:206165] &quot;1&quot; &quot;2&quot; &quot;5&quot; &quot;6&quot; ... ## $ deviance : num 169703 ## $ aic : num 169719 ## $ null.deviance : num 171694 ## $ iter : int 4 ## $ weights : Named num [1:206165] 0.133 0.107 0.142 0.142 0.143 ... ## ..- attr(*, &quot;names&quot;)= chr [1:206165] &quot;1&quot; &quot;2&quot; &quot;5&quot; &quot;6&quot; ... ## $ prior.weights : Named num [1:206165] 1 1 1 1 1 1 1 1 1 1 ... ## ..- attr(*, &quot;names&quot;)= chr [1:206165] &quot;1&quot; &quot;2&quot; &quot;5&quot; &quot;6&quot; ... ## $ df.residual : int 206157 ## $ df.null : int 206164 ## $ y : Named num [1:206165] 0 0 0 0 0 0 0 0 0 0 ... ## ..- attr(*, &quot;names&quot;)= chr [1:206165] &quot;1&quot; &quot;2&quot; &quot;5&quot; &quot;6&quot; ... ## $ converged : logi TRUE ## $ boundary : logi FALSE ## $ model :&#39;data.frame&#39;: 206165 obs. of 4 variables: ## ..$ Winner: num [1:206165] 0 0 0 0 0 0 0 0 0 0 ... ## ..$ Age : num [1:206165] 24 23 21 21 25 25 27 27 31 31 ... ## ..$ Height: num [1:206165] 180 170 185 185 185 185 185 185 188 188 ... ## ..$ Weight: num [1:206165] 80 60 82 82 82 82 82 82 75 75 ... ## ..- attr(*, &quot;terms&quot;)=Classes &#39;terms&#39;, &#39;formula&#39; language Winner ~ Age * Height * Weight ## .. .. ..- attr(*, &quot;variables&quot;)= language list(Winner, Age, Height, Weight) ## .. .. ..- attr(*, &quot;factors&quot;)= int [1:4, 1:7] 0 1 0 0 0 0 1 0 0 0 ... ## .. .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. .. .. ..$ : chr [1:4] &quot;Winner&quot; &quot;Age&quot; &quot;Height&quot; &quot;Weight&quot; ## .. .. .. .. ..$ : chr [1:7] &quot;Age&quot; &quot;Height&quot; &quot;Weight&quot; &quot;Age:Height&quot; ... ## .. .. ..- attr(*, &quot;term.labels&quot;)= chr [1:7] &quot;Age&quot; &quot;Height&quot; &quot;Weight&quot; &quot;Age:Height&quot; ... ## .. .. ..- attr(*, &quot;order&quot;)= int [1:7] 1 1 1 2 2 2 3 ## .. .. ..- attr(*, &quot;intercept&quot;)= int 1 ## .. .. ..- attr(*, &quot;response&quot;)= int 1 ## .. .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; ## .. .. ..- attr(*, &quot;predvars&quot;)= language list(Winner, Age, Height, Weight) ## .. .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:4] &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; ## .. .. .. ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;Winner&quot; &quot;Age&quot; &quot;Height&quot; &quot;Weight&quot; ## ..- attr(*, &quot;na.action&quot;)= &#39;omit&#39; Named int [1:64951] 3 4 27 28 30 36 37 38 39 40 ... ## .. ..- attr(*, &quot;names&quot;)= chr [1:64951] &quot;3&quot; &quot;4&quot; &quot;27&quot; &quot;28&quot; ... ## $ na.action : &#39;omit&#39; Named int [1:64951] 3 4 27 28 30 36 37 38 39 40 ... ## ..- attr(*, &quot;names&quot;)= chr [1:64951] &quot;3&quot; &quot;4&quot; &quot;27&quot; &quot;28&quot; ... ## $ call : language glm(formula = Winner ~ Age * Height * Weight, family = &quot;binomial&quot;, data = AthleteEvents) ## $ formula :Class &#39;formula&#39; language Winner ~ Age * Height * Weight ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; ## $ terms :Classes &#39;terms&#39;, &#39;formula&#39; language Winner ~ Age * Height * Weight ## .. ..- attr(*, &quot;variables&quot;)= language list(Winner, Age, Height, Weight) ## .. ..- attr(*, &quot;factors&quot;)= int [1:4, 1:7] 0 1 0 0 0 0 1 0 0 0 ... ## .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. .. ..$ : chr [1:4] &quot;Winner&quot; &quot;Age&quot; &quot;Height&quot; &quot;Weight&quot; ## .. .. .. ..$ : chr [1:7] &quot;Age&quot; &quot;Height&quot; &quot;Weight&quot; &quot;Age:Height&quot; ... ## .. ..- attr(*, &quot;term.labels&quot;)= chr [1:7] &quot;Age&quot; &quot;Height&quot; &quot;Weight&quot; &quot;Age:Height&quot; ... ## .. ..- attr(*, &quot;order&quot;)= int [1:7] 1 1 1 2 2 2 3 ## .. ..- attr(*, &quot;intercept&quot;)= int 1 ## .. ..- attr(*, &quot;response&quot;)= int 1 ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; ## .. ..- attr(*, &quot;predvars&quot;)= language list(Winner, Age, Height, Weight) ## .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:4] &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; ## .. .. ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;Winner&quot; &quot;Age&quot; &quot;Height&quot; &quot;Weight&quot; ## $ data :Classes &#39;spec_tbl_df&#39;, &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 271116 obs. of 14 variables: ## ..$ ID : num [1:271116] 1 2 3 4 5 5 5 5 5 5 ... ## ..$ Name : chr [1:271116] &quot;A Dijiang&quot; &quot;A Lamusi&quot; &quot;Gunnar Nielsen Aaby&quot; &quot;Edgar Lindenau Aabye&quot; ... ## ..$ Sex : chr [1:271116] &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; ... ## ..$ Age : num [1:271116] 24 23 24 34 21 21 25 25 27 27 ... ## ..$ Height: num [1:271116] 180 170 NA NA 185 185 185 185 185 185 ... ## ..$ Weight: num [1:271116] 80 60 NA NA 82 82 82 82 82 82 ... ## ..$ NOC : chr [1:271116] &quot;CHN&quot; &quot;CHN&quot; &quot;DEN&quot; &quot;DEN&quot; ... ## ..$ Games : chr [1:271116] &quot;1992 Summer&quot; &quot;2012 Summer&quot; &quot;1920 Summer&quot; &quot;1900 Summer&quot; ... ## ..$ Year : num [1:271116] 1992 2012 1920 1900 1988 ... ## ..$ Season: chr [1:271116] &quot;Summer&quot; &quot;Summer&quot; &quot;Summer&quot; &quot;Summer&quot; ... ## ..$ Sport : chr [1:271116] &quot;Basketball&quot; &quot;Judo&quot; &quot;Football&quot; &quot;Tug-Of-War&quot; ... ## ..$ Event : chr [1:271116] &quot;Basketball Men&#39;s Basketball&quot; &quot;Judo Men&#39;s Extra-Lightweight&quot; &quot;Football Men&#39;s Football&quot; &quot;Tug-Of-War Men&#39;s Tug-Of-War&quot; ... ## ..$ Medal : chr [1:271116] NA NA NA &quot;Gold&quot; ... ## ..$ Winner: num [1:271116] 0 0 0 1 0 0 0 0 0 0 ... ## ..- attr(*, &quot;spec&quot;)= ## .. .. cols( ## .. .. ID = col_double(), ## .. .. Name = col_character(), ## .. .. Sex = col_character(), ## .. .. Age = col_double(), ## .. .. Height = col_double(), ## .. .. Weight = col_double(), ## .. .. NOC = col_character(), ## .. .. Games = col_character(), ## .. .. Year = col_double(), ## .. .. Season = col_character(), ## .. .. Sport = col_character(), ## .. .. Event = col_character(), ## .. .. Medal = col_character() ## .. .. ) ## $ offset : NULL ## $ control :List of 3 ## ..$ epsilon: num 1e-08 ## ..$ maxit : num 25 ## ..$ trace : logi FALSE ## $ method : chr &quot;glm.fit&quot; ## $ contrasts : NULL ## $ xlevels : Named list() ## - attr(*, &quot;class&quot;)= chr [1:2] &quot;glm&quot; &quot;lm&quot; There’s a ton going on in there! Luckily enough, we don’t have to worry about most of it. The two elements I do want to point out, though, are the y and fitted.values columns. y contains our response variable - whether or not an athlete medaled - as a binary value of 1 or 0. fitted.values, meanwhile, stores the probability our model gives of y being 1. We can take advantage of this using the package pROC, which will let us calculate how close our model got. Let’s first install and load pROC: install.packages(&quot;pROC&quot;) library(pROC) ## Type &#39;citation(&quot;pROC&quot;)&#39; for a citation. ## ## Attaching package: &#39;pROC&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## cov, smooth, var What we’re going to do now is to build a receiver operating characteristic curve - a ROC - in order to understand graphically exactly what we’re computing. pROC makes that easy, using the function roc(): LogModelROC &lt;- roc(AthleteLogisticModel$y, AthleteLogisticModel$fitted.values) We can then plot our ROC object using plot.roc(): plot.roc(LogModelROC) This is cool! What the hell is it? This is what we mean when we say receiver operating characteristic curve. That light grey line in the middle represents what would happen if we just randomly guessed whether or not each athlete got a medal, using a 50/50 chance. The black line represents how well our model did - everywhere that it’s higher than the grey line, we were more accurate than random chance. We don’t need to worry about exactly what the axes mean (basically, the x axis represents how confident our model is in guessing someone medaled, while the y is how surprised it is about the result), but you can read more for yourself here. The curve is cool and all, but is hard to interpret. Luckily for us, there’s a single value - similar to R2 for linear models - which we can use to identify how well our model did. That number is the area under the curve (AUC, sometimes called the c-statistic) that I mentioned earlier, which we can calculate with pROC by printing out our ROC object: LogModelROC ## ## Call: ## roc.default(response = AthleteLogisticModel$y, predictor = AthleteLogisticModel$fitted.values) ## ## Data: AthleteLogisticModel$fitted.values in 175984 controls (AthleteLogisticModel$y 0) &lt; 30181 cases (AthleteLogisticModel$y 1). ## Area under the curve: 0.5753 (Note that, if I didn’t assign the ROC object to a name in the first place, this value would have just printed out.) That number - 0.57 - is our single statistic for how accurate our model is. Generally speaking, models with an AUC of 0.7 are thought of as good models, with 0.8 as great and 0.9 incredible. Ecology can have slightly fuzzier models - predicting the natural world is hard, yo - but even the “random chance” line has an AUC of 0.5 - if your model is close to or below that, it’s pretty much useless. 9.3.3 Model Comparisons 9.3.3.1 AUC So while our model is doing better than chance, it’s still not doing great - 0.57 leaves a lot of room to improve. For instance, since we know the sex of each athlete in our dataset, what would happen if we added that variable to our formula? We can calculate the new formula following all the steps above. I’m also going to use pscl and pROC to find the pseduo-R2 and AUC of our new model, as well: AthleteLogisticModel2 &lt;- glm(Winner ~ Age * Height * Weight * Sex, data = AthleteEvents, family = &quot;binomial&quot;) pscl::pR2(AthleteLogisticModel2) ## llh llhNull G2 McFadden r2ML ## -8.413737e+04 -1.130577e+05 5.784073e+04 2.558018e-01 2.446360e-01 ## r2CU ## 3.672928e-01 LogModel2ROC &lt;- roc(AthleteLogisticModel2$y, AthleteLogisticModel2$fitted.values) LogModel2ROC ## ## Call: ## roc.default(response = AthleteLogisticModel2$y, predictor = AthleteLogisticModel2$fitted.values) ## ## Data: AthleteLogisticModel2$fitted.values in 175984 controls (AthleteLogisticModel2$y 0) &lt; 30181 cases (AthleteLogisticModel2$y 1). ## Area under the curve: 0.6051 So, under each metric, our new model seems to be a slightly better fit to the data. The benefits of working with AUC - and, specifically, with pROC - is that testing to see if one model is better than the other is a piece of cake with roc.test: roc.test(LogModelROC, LogModel2ROC) ## ## DeLong&#39;s test for two correlated ROC curves ## ## data: LogModelROC and LogModel2ROC ## Z = -21.937, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in AUC is not equal to 0 ## sample estimates: ## AUC of roc1 AUC of roc2 ## 0.5752987 0.6051203 And so we’re able to conclude that yes, our second model is significantly better than the first - including sex in our model made it significantly more predictive. 9.3.3.2 AIC While comparing model AUCs is effective, it isn’t the most popular method to analyze model performance. That honor likely goes to the Akaike Information Criterion, more commonly known as the AIC. We discussed AIC back in chapter 6, AIC can’t be used to rank random models against one another without actual experimental design. More rants about this may be found here. As such, our example here - only comparing two models, with no strong hypothesis as to why these are the two that deserve to be compared - isn’t the best way to demonstrate the use of AIC. We’ll follow up with a more clear example in a moment. AIC can be calculated using the broom package’s glance() on any model object you have: library(broom) glance(AthleteLogisticModel) ## # A tibble: 1 x 7 ## null.deviance df.null logLik AIC BIC deviance df.residual ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 171694. 206164 -84852. 169719. 169801. 169703. 206157 glance(AthleteLogisticModel2) ## # A tibble: 1 x 7 ## null.deviance df.null logLik AIC BIC deviance df.residual ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 171694. 206164 -84137. 168307. 168471. 168275. 206149 The number we’re interested in right now is AIC, located halfway across the dataframe. There’s a very simple rule for comparing models using AIC: \\(\\Delta\\)AIC between two models &gt; 4? The model with the smaller AIC is better. \\(\\Delta\\)AIC between 2 and 4? Odds are the model with the smaller AIC is better, but it’s shakier. \\(\\Delta\\)AIC &lt; 2? The two models are effectively identical. In this case, our second model has an AIC of 168576.3, while the first model’s AIC value is 169872.5. As such, the models have a \\(\\Delta\\)AIC of 1296.2, which is a little bit bigger than 4. Note that the actual AIC number is unitless and arbitrary - the AIC isn’t so much a metric for how well the model fits the data, in the way that R2 and AUC are, but rather a way to tell which of your models better fits the data. More commonly reported than AIC values are the \\(\\Delta\\)AIC values, the number of parameters in a model (represented as k), and the Aikake Weights of each model - information on calculating those here. 9.4 More Complicated Analyses So far, we haven’t really earned our chapter title - while what we’ve been doing so far has been more in depth, it hasn’t necessarily been more complicated. This is the section that’ll step things up a little. If you got a strong understanding of functional programming in unit 4, and then understood making your own functions in unit 5, this part shouldn’t be particularly hard. However, it is one of the more involved blocks of code we’ve worked through so far - so don’t feel bad if it takes a little time before things work right, or before you fully understand what we’re doing. So, as we mentioned above, using AIC to compare two random models isn’t exactly what the tool is designed for. While it isn’t technically wrong, so long as the models are both of the same type (i.e., logistic) and modeling the same dataset, it isn’t useful to science or to you. We don’t particularly care which models are slightly better than others - we want to know which ones are right! You can either do this by comparing a small set of models that you have strong reasons to believe are accurate - this is a form of hypothesis testing, but using model AICs rather than p values - or by comparing all possible models which use the same set of scientifically sound predictor variables. Note the “scientifically sound” - you can’t throw millions of predictors at the wall and hope that they stick; you have to be able to justify their inclusion. 9.4.1 Model Selection Model improvement doesn’t make it into papers for the same reason people don’t go around introducing you to their ex-wives. — Andrew Gelman Lets say with our Olympic athlete dataset I can justify expecting sex, age, height, and weight as predictors for the model, as well as the interactions between each of these terms. After all, we can expect age, height, and weight to vary together somewhat - and each of these likely differs between male and female athletes. One way I could compare the combinations of these variables would be to glance() at each of them individually, like so: ## Glancing at the full model glance(glm(Winner ~ Sex * Age * Height * Weight, data = AthleteEvents, family = &quot;binomial&quot;)) ## # A tibble: 1 x 7 ## null.deviance df.null logLik AIC BIC deviance df.residual ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 171694. 206164 -84137. 168307. 168471. 168275. 206149 However, this is inefficient, requires a lot of typing, and makes it really hard to compare AICs directly - or to export them to somewhere like Excel, for inclusion in a document. A somewhat better way is to define each model as its own function, taking the argument df: FullMod &lt;- function(df){ glm(Winner ~ Sex * Age * Height * Weight, data = df, family = &quot;binomial&quot;) } One &lt;- function(df){ glm(Winner ~ Sex * Age * Height, data = df, family = &quot;binomial&quot;) } Two &lt;- function(df){ glm(Winner ~ Sex * Age * Weight, data = df, family = &quot;binomial&quot;) } Three &lt;- function(df){ glm(Winner ~ Sex * Height * Weight, data = df, family = &quot;binomial&quot;) } Four &lt;- function(df){ glm(Winner ~ Sex * Height, data = df, family = &quot;binomial&quot;) } Five &lt;- function(df){ glm(Winner ~ Sex * Age, data = df, family = &quot;binomial&quot;) } Six &lt;- function(df){ glm(Winner ~ Sex * Weight, data = df, family = &quot;binomial&quot;) } Seven &lt;- function(df){ glm(Winner ~ Age * Height * Weight, data = df, family = &quot;binomial&quot;) } Eight &lt;- function(df){ glm(Winner ~ Age * Height, data = df, family = &quot;binomial&quot;) } Nine &lt;- function(df){ glm(Winner ~ Age * Weight, data = df, family = &quot;binomial&quot;) } Ten &lt;- function(df){ glm(Winner ~ Height * Weight, data = df, family = &quot;binomial&quot;) } Eleven &lt;- function(df){ glm(Winner ~ Sex, data = df, family = &quot;binomial&quot;) } Twelve &lt;- function(df){ glm(Winner ~ Age, data = df, family = &quot;binomial&quot;) } Thirteen &lt;- function(df){ glm(Winner ~ Height, data = df, family = &quot;binomial&quot;) } Fourteen &lt;- function(df){ glm(Winner ~ Weight, data = df, family = &quot;binomial&quot;) } NullMod &lt;- function(df){ glm(Winner ~ 1, data = df, family = &quot;binomial&quot;) } That last item in the list is what’s known as the null model - it’s a similar concept to the null hypothesis in hypothesis testing. Its purpose is to serve as a baseline for judging the rest of our models - we’re hoping it has a \\(\\Delta\\)AIC &gt; 4! Now that we have our models defined, we’re able to run each of them against the dataset. Fair warning, this step will take a long time to complete - we’re asking R to compute estimates for something like 4.8 million observations by doing this. We’ll then overwrite the columns we just created, replacing them with the values from glance() so we can make model comparisons: LogModelGlances &lt;- AthleteEvents %&gt;% nest() %&gt;% mutate(FullMod = map(data, FullMod), FullMod = map(FullMod, glance), One = map(data, One), One = map(One, glance), Two = map(data, Two), Two = map(Two, glance), Three = map(data, Three), Three = map(Three, glance), Four = map(data, Four), Four = map(Four, glance), Five = map(data, Five), Five = map(Five, glance), Six = map(data, Six), Six = map(Six, glance), Seven = map(data, Seven), Seven = map(Seven, glance), Eight = map(data, Eight), Eight = map(Eight, glance), Nine = map(data, Nine), Nine = map(Nine, glance), Ten = map(data, Ten), Ten = map(Ten, glance), Eleven = map(data, Eleven), Eleven = map(Eleven, glance), Twelve = map(data, Twelve), Twelve = map(Twelve, glance), Thirteen = map(data, Thirteen), Thirteen = map(Thirteen, glance), Fourteen = map(data, Fourteen), Fourteen = map(Fourteen, glance), NullMod = map(data, NullMod), NullMod = map(NullMod, glance)) LogModelGlances ## # A tibble: 1 x 17 ## data FullMod One Two Three Four Five Six Seven Eight Nine Ten ## &lt;lis&gt; &lt;list&gt; &lt;lis&gt; &lt;lis&gt; &lt;lis&gt; &lt;lis&gt; &lt;lis&gt; &lt;lis&gt; &lt;lis&gt; &lt;lis&gt; &lt;lis&gt; &lt;lis&gt; ## 1 &lt;tib… &lt;tibbl… &lt;tib… &lt;tib… &lt;tib… &lt;tib… &lt;tib… &lt;tib… &lt;tib… &lt;tib… &lt;tib… &lt;tib… ## # … with 5 more variables: Eleven &lt;list&gt;, Twelve &lt;list&gt;, Thirteen &lt;list&gt;, ## # Fourteen &lt;list&gt;, NullMod &lt;list&gt; (This is why you need to have smart hypotheses about which variables matter - imagine having to wait for thirty-two models to process, or even more!) We then want to get from this wide dataframe to a tidy dataframe, by gather()ing the model types into a column, paired with the dataframe their column contains: LogModelGlances &lt;- LogModelGlances %&gt;% gather(Model, Value) %&gt;% slice(-1) I also used slice(-1) to drop the first row containing our data, since we don’t need it anymore. Now all that’s left is to unlist our value columns. I’m also going to arrange the table by AIC, to show us which models performed best (and worst!) LogModelGlances %&gt;% unnest(Value, .drop = T) %&gt;% arrange(AIC) ## # A tibble: 16 x 8 ## Model null.deviance df.null logLik AIC BIC deviance df.residual ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 FullM… 171694. 206164 -84137. 1.68e5 1.68e5 168275. 206149 ## 2 Three 171965. 206852 -84360. 1.69e5 1.69e5 168719. 206845 ## 3 Seven 171694. 206164 -84852. 1.70e5 1.70e5 169703. 206157 ## 4 Ten 171965. 206852 -85050. 1.70e5 1.70e5 170100. 206849 ## 5 Two 172939. 207378 -85059. 1.70e5 1.70e5 170118. 207371 ## 6 Six 173321. 208240 -85311. 1.71e5 1.71e5 170622. 208237 ## 7 Nine 172939. 207378 -85632. 1.71e5 1.71e5 171265. 207375 ## 8 Fourt… 173321. 208240 -85878. 1.72e5 1.72e5 171755. 208239 ## 9 One 175987. 210067 -86619. 1.73e5 1.73e5 173239. 210060 ## 10 Four 176348. 210944 -86860. 1.74e5 1.74e5 173720. 210941 ## 11 Eight 175987. 210067 -87154. 1.74e5 1.74e5 174308. 210064 ## 12 Thirt… 176348. 210944 -87364. 1.75e5 1.75e5 174728. 210943 ## 13 Five 220518. 261641 -110124. 2.20e5 2.20e5 220249. 261638 ## 14 Twelve 220518. 261641 -110185. 2.20e5 2.20e5 220370. 261640 ## 15 Eleven 226115. 271115 -113050. 2.26e5 2.26e5 226101. 271114 ## 16 NullM… 226115. 271115 -113058. 2.26e5 2.26e5 226115. 271115 Tada! Looks like all our models outperformed the null model by a good margin - and our full model is the best model of the set, with no other model getting particularly close to it. If we wanted, we could now measure just how accurate our best model is, using either AUC or a pseudo-R2: ROCMod &lt;- glm(Winner ~ Sex * Age * Height * Weight, data = AthleteEvents, family = &quot;binomial&quot;) pROC::roc(ROCMod$y, ROCMod$fitted.values) ## ## Call: ## roc.default(response = ROCMod$y, predictor = ROCMod$fitted.values) ## ## Data: ROCMod$fitted.values in 175984 controls (ROCMod$y 0) &lt; 30181 cases (ROCMod$y 1). ## Area under the curve: 0.6051 pscl::pR2(glm(Winner ~ Sex * Age * Height * Weight, data = AthleteEvents, family = &quot;binomial&quot;)) ## llh llhNull G2 McFadden r2ML ## -8.413737e+04 -1.130577e+05 5.784073e+04 2.558018e-01 2.446360e-01 ## r2CU ## 3.672928e-01 0.6 AUC and 0.255 McFadden’s pseudo-R2. Even after all that, it seems like our model could be improved quite a bit! 9.4.1.1 Detour: Another Way Now, obviously, this entire section breaks our rule that if you have to type it more than twice, there’s a better way. That’s true here, too - but the better way is significantly more complicated, so I introduced the longer form first. The better way is in the next section of this unit. Typing models out this way does become prohibitive as you add more variables - for every k variables you want to include, you wind up having to type out 2k formulas. Even greater than the time tax that puts on you is the amount of time it will take to compute those models and fit them to your data. Generally speaking, you shouldn’t have massive numbers of variables in your regression equations. Remember the old saying: All models are wrong, but some models are useful. Or, alternately: The perfect is the enemy of the pretty good. Lots of practicioners want to use as much data as they can to make the most accurate and precise model formula possible, no matter how efficient or practical that formula may be. That’s because a lot of people lose track of the whole purpose of the model - someone should be able to take this crazy thing you’ve created and go do something with it. Models with fifty parameters make that impossible, unless practicioners are going to be able to get as much data as you happened to collect. Plus, longer formulas may be better fits to your data - but they’re also more likely to be overfits, worse at describing the outside world. Finally - and this is the challenge most relevant to business applications, as well - models with more terms are extremely costly to use, requiring either huge amounts of computing power or time to work properly. As such, a better practice is to figure out which variables are most likely to matter before you start building your models - as we did, with our correlation plots and tables. This method is a lot more sound scientifically, as well - rather than throwing things at the wall until you got one to stick, you’re better able to explain your analysis methods, and the reason those terms matter. However, if you want to throw things at the wall anyway, the statistically preferred way to do so is stepwise selection (one informative link here, another - this one including information on the true “stepwise” bidirectional selection - here, and a third - this one explaining things better, but including coding tips for non-R programs, here). The basic process to do this is to fit two models - a null model and a full model - and then pass them to the step() function. First, though, we have to drop all the rows where our predictor values have a value of NA: StepWiseData &lt;- AthleteEvents %&gt;% drop_na(3:6, 14) ## Numbers specify columns to drop NA in We can choose which type of stepwise selection we want by specifying certain parameters within this function - for instance, given our models: StepWiseFullMod &lt;- glm(Winner ~ Sex * Age * Height * Weight, data = StepWiseData, family = &quot;binomial&quot;) StepWiseNullMod &lt;- glm(Winner ~ 1, data = StepWiseData, family = &quot;binomial&quot;) We can perform a backwards selection by typing: step(StepWiseFullMod) A forward selection via: step(StepWiseNullMod, scope = list(lower = formula(StepWiseNullMod), upper = formula(StepWiseFullMod)), direction = &quot;forward&quot;) And a bi-directional bothways function via the following: step(StepWiseNullMod, scope = list(lower = formula(StepWiseNullMod), upper = formula(StepWiseFullMod)), direction = &quot;both&quot;) I’m not running them here, because they take a ton of space. Note that these methods give us slightly different formulas, due to how each calculates which terms to include. This method is becoming disfavored, as it doesn’t require you to think about why you want a given variable in the model. But I can’t stop you from doing whatever you’re gonna do. 9.4.1.2 The Better (…Faster) Way Before we launch into this, note that this section is a little more techy and a little more obscure. It shouldn’t be that hard to follow, if you fully understand how for loops and map functions work. But if this is a little above your level right now, that’s totally fine - think of this section as more of a template, showing how we can combine tools to make our code faster and more efficient. There is a way to programmatically generate all the models, using the map() family of functions. Before we get to that, though, let’s create a smaller dataframe - Predictors - that only contains our predictor variables. We’ll also get the names() of all our predictors, and store that in our column Cols: Predictors &lt;- data.frame(Sex = AthleteEvents$Sex, Age = AthleteEvents$Age, Height = AthleteEvents$Height, Weight = AthleteEvents$Weight) Cols &lt;- names(Predictors) We’re now going to find the length() of our Cols vector and store it in n: n &lt;- length(Cols) Alright, here’s where things get a little more complicated. What we want to do is use the combn() function to find all the combinations of our variables, so that we can use those to programmatically build formulas. The way we do this is to build what’s known as an anonymous function inside of combn() by typing ~, using our . pronoun to iterate through a dataset. We’ll also set simplify inside of the combn() function to FALSE. If we pass this to map(), we get the following output: map(1:n, ~ combn(1:n, ., simplify = FALSE)) ## [[1]] ## [[1]][[1]] ## [1] 1 ## ## [[1]][[2]] ## [1] 2 ## ## [[1]][[3]] ## [1] 3 ## ## [[1]][[4]] ## [1] 4 ## ## ## [[2]] ## [[2]][[1]] ## [1] 1 2 ## ## [[2]][[2]] ## [1] 1 3 ## ## [[2]][[3]] ## [1] 1 4 ## ## [[2]][[4]] ## [1] 2 3 ## ## [[2]][[5]] ## [1] 2 4 ## ## [[2]][[6]] ## [1] 3 4 ## ## ## [[3]] ## [[3]][[1]] ## [1] 1 2 3 ## ## [[3]][[2]] ## [1] 1 2 4 ## ## [[3]][[3]] ## [1] 1 3 4 ## ## [[3]][[4]] ## [1] 2 3 4 ## ## ## [[4]] ## [[4]][[1]] ## [1] 1 2 3 4 Which is very close to what we want. The issue is, each of these combinations is buried inside a list - while we want a list of combinations, what we have is a list of lists of combinations. To get rid of that higher-level list, we can use the unlist() function - specifying recursive = FALSE to make sure we only get rid of the top-level list. I’m going to assign that to id: id &lt;- unlist(map(1:n, ~ combn(1:n, ., simplify = FALSE)), recursive = FALSE) We now have everything we need to find our formulas - the names of our predictor variables and all the combinations we want of them. We’re now going to use map_chr() to paste() together each of our formulas - by indexing Cols with [.] in our anonymous function, we can call each combination of column names we have specified in id. collapse = &quot;*&quot; will separate each of our predictors with the * operator: Formulas &lt;- map_chr(id, ~ paste(&quot;Winner ~&quot;, paste(Cols[.], collapse=&quot;*&quot;))) Formulas ## [1] &quot;Winner ~ Sex&quot; &quot;Winner ~ Age&quot; ## [3] &quot;Winner ~ Height&quot; &quot;Winner ~ Weight&quot; ## [5] &quot;Winner ~ Sex*Age&quot; &quot;Winner ~ Sex*Height&quot; ## [7] &quot;Winner ~ Sex*Weight&quot; &quot;Winner ~ Age*Height&quot; ## [9] &quot;Winner ~ Age*Weight&quot; &quot;Winner ~ Height*Weight&quot; ## [11] &quot;Winner ~ Sex*Age*Height&quot; &quot;Winner ~ Sex*Age*Weight&quot; ## [13] &quot;Winner ~ Sex*Height*Weight&quot; &quot;Winner ~ Age*Height*Weight&quot; ## [15] &quot;Winner ~ Sex*Age*Height*Weight&quot; Now, my desired outputs here are two dataframes: one of the outputs from tidy(), and one of the outputs from glance(). As such, I’m going to initalize two dataframes, Glances and Tidies, with the outputs from those functions. I’m also going to create a column in Tidies to store which formula the output is from: Glances &lt;- glance(glm(Winner ~ 1, data = AthleteEvents, family = &quot;binomial&quot;)) Tidies &lt;- tidy(glm(Winner ~ 1, data = AthleteEvents, family = &quot;binomial&quot;)) Tidies$FormNum &lt;- 0 And now we want to actually calculate these models for our data. For this, we have to use a for loop, running as many times as we have formulas. Rather than explain everything in the loop up here, I’ve commented the code below, with explanations following the ##: for(i in seq_along(Formulas)){ ## Convert the current (index i) formula into a logistic model CurrentModel &lt;- glm(as.formula(Formulas[[i]]), data = AthleteEvents, family = &quot;binomial&quot;) ## Store the glance of the current model in CurrentGlance CurrentGlance &lt;- glance(CurrentModel) ## Make row i of Glances the current model&#39;s glance Glances[i, ] &lt;- as.vector(CurrentGlance) ## Store the tidy of the current model, label it with the model number, and convert it to a vector CurrentTidy &lt;- tidy(CurrentModel) CurrentTidy$FormNum &lt;- i CurrentTidy &lt;- as.vector(CurrentTidy) ## Append the current tidy to your table with all tidy outputs Tidies &lt;- rbind(Tidies, CurrentTidy) } And if everything worked properly, we can print out our dataframes to make sure we got everything right: Glances ## # A tibble: 15 x 7 ## null.deviance df.null logLik AIC BIC deviance df.residual ## * &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 226115. 271115 -113050. 226105. 226126. 226101. 271114 ## 2 220518. 261641 -110185. 220374. 220395. 220370. 261640 ## 3 176348. 210944 -87364. 174732. 174752. 174728. 210943 ## 4 173321. 208240 -85878. 171759. 171780. 171755. 208239 ## 5 220518. 261641 -110124. 220257. 220299. 220249. 261638 ## 6 176348. 210944 -86860. 173728. 173769. 173720. 210941 ## 7 173321. 208240 -85311. 170630. 170671. 170622. 208237 ## 8 175987. 210067 -87154. 174316. 174357. 174308. 210064 ## 9 172939. 207378 -85632. 171273. 171314. 171265. 207375 ## 10 171965. 206852 -85050. 170108. 170149. 170100. 206849 ## 11 175987. 210067 -86619. 173255. 173337. 173239. 210060 ## 12 172939. 207378 -85059. 170134. 170216. 170118. 207371 ## 13 171965. 206852 -84360. 168735. 168817. 168719. 206845 ## 14 171694. 206164 -84852. 169719. 169801. 169703. 206157 ## 15 171694. 206164 -84137. 168307. 168471. 168275. 206149 9.5 Relational Data Okay, this next section should be easier and more relevant to your daily life. Before we get into it, I’m going to make two edits to the Glances table: I’m going to convert the row names (those unlabeled numbers on the left) into a column called FormNum I’m going to only select our columns of FormNum and AIC: Glances &lt;- Glances %&gt;% mutate(FormNum = as.numeric(row.names(.))) %&gt;% select(FormNum, AIC) Glances ## # A tibble: 15 x 2 ## FormNum AIC ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 226105. ## 2 2 220374. ## 3 3 174732. ## 4 4 171759. ## 5 5 220257. ## 6 6 173728. ## 7 7 170630. ## 8 8 174316. ## 9 9 171273. ## 10 10 170108. ## 11 11 173255. ## 12 12 170134. ## 13 13 168735. ## 14 14 169719. ## 15 15 168307. I also want to point out one important difference between our dataframes - the Glances dataframe doesn’t include our null model, while Tidies has it as FormNum == 0. We can check to make sure by factor()ing our dataframes’ FormNum fields, which identifies each unique level in the vector. We can then find all the levels() contained in that factor, confirming that 0 is only present in the Tidies dataframe: levels(factor(Glances$FormNum)) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; &quot;10&quot; &quot;11&quot; &quot;12&quot; &quot;13&quot; &quot;14&quot; ## [15] &quot;15&quot; levels(factor(Tidies$FormNum)) ## [1] &quot;0&quot; &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; &quot;10&quot; &quot;11&quot; &quot;12&quot; &quot;13&quot; ## [15] &quot;14&quot; &quot;15&quot; What we’ll be doing now is working on joining datasets, combining two dataframes into a single output. All of these functions take the form f(x, y), where f() is the function name and x, y are the arguments. What this function does is find columns with the same name - known as keys - and combine your dataframes based on the values in those columns. For our examples, our key column will be the FormNum column. 9.5.1 Inner Join An inner join will preserve only the rows in your data which have keys present in both datasets - so, in this case, the null model will be dropped: inner_join(Glances, Tidies) 9.5.2 Left Join A left join will preserve all the values in your x dataset - here, in Glances. left_join(Glances, Tidies) 9.5.3 Right Join A right join will preserve all the values in your y dataset - here in Tidies. As you can see, the AIC for the null model is left as NA, but the null model stays in the dataframe. right_join(Glances, Tidies) 9.5.4 Full Join A full join will save all values in both your x and y dataframes. full_join(Glances, Tidies) 9.5.5 Semi Join A semi join returns a row from x if that key value has a match in y. semi_join(Glances, Tidies) 9.5.6 Anti Join An anti join returns a row from x if that key value doesn’t have a match in y: anti_join(Glances, Tidies) 9.5.7 Specifying Key Columns Sometimes it’s helpful to specify which columns you want to join your data by - particularly when you have columns with the same name but different values. In those situations, use by = to specify which columns you want to join by: full_join(Glances, Tidies, by = &quot;FormNum&quot;) 9.5.8 Merging Multiple Dataframes In order to merge more than two dataframes at the same time, use reduce() with any of the above joins - all your dataframes have to be in a list() inside the reduce() call: ## Creating example dataframes: x &lt;- data.frame(key = c(&quot;A&quot;,&quot;A&quot;,&quot;B&quot;,&quot;B&quot;), value = c(3,4,3,4)) y &lt;- data.frame(key = c(&quot;A&quot;,&quot;A&quot;,&quot;B&quot;,&quot;B&quot;), value = c(124,12,524,43)) z &lt;- data.frame(key = c(&quot;A&quot;,&quot;A&quot;,&quot;B&quot;,&quot;B&quot;), value = c(31, 4, 14, 124)) ## Using &quot;reduce&quot; to merge all dataframes: reduce( list( x, y, z ), full_join, by = &quot;key&quot; ) 9.5.9 Binding Dataframes Sometimes, however, we don’t have a key column to work with. In these cases, we usually just want to combine all of our data. Consider the dataframes: df1 &lt;- tibble(Name = c(&quot;Alice&quot;, &quot;Bob&quot;), EyeColor = c(&quot;Blue&quot;, &quot;Brown&quot;)) df2 &lt;- tibble(BestFriend = c(&quot;Bob&quot;, &quot;Alice&quot;), HairColor = c(&quot;Brown&quot;, &quot;Blue&quot;)) If the rows of each dataframe represent the same observation - that is, row 1 in dataframe 1 is the same as row 1 in dataframe 2 - we can combine the tables using cbind(): combineddf &lt;- cbind(df1, df2) But say we forgot someone, whose records are stored in dataframe 3: df3 &lt;- tibble(Name = &quot;Eve&quot;, EyeColor = &quot;Grey&quot;, BestFriend = &quot;Alice&quot;, HairColor = &quot;Grey&quot;) If we want to add her to the table, we can use rbind() - so long as she has all the same columns as our original table! rbind(combineddf, df3) ## Name EyeColor BestFriend HairColor ## 1 Alice Blue Bob Brown ## 2 Bob Brown Alice Blue ## 3 Eve Grey Alice Grey 9.6 Exercises Try out the types of join on the Olympic datasets AthleteEvents and NOCRegions. What differences do you notice between the join types? Explore the NBAStats dataset. Can you predict the number of points (PTS) players will score based on information available to you? Make multiple linear models (lm()) for each position (Pos) in the dataset. "],
["playing-nicely-with-others.html", "10 Playing Nicely With Others 10.1 R Markdown 10.2 LaTex 10.3 Git(Hub) 10.4 Commenting Code 10.5 Further Reading", " 10 Playing Nicely With Others Any fool can write code that a computer can understand. Good programmers write code that humans can understand. — Martin Fowler So far, we’ve gotten ourselves to a point where we can explore our data, perform some analyses, and graph our results intelligently. That’s all great, and are incredibly important skills. However, even more important than the hard coding skills we’ve been building is the ability to communicate our results - to our peers, bosses, and the larger outside world. We’ll be using a lot of non-R technologies this unit, and focusing mostly on how to use these technologies in a workflow that resembles those used in the modern scientific world. Plenty of scientists don’t use R Markdown. Even more don’t use Git. Some may have never heard of LaTeX. But these are skills and tools which are growing in importance and acceptance throughout the scientific and business worlds, so getting used to them now will make your life significantly better. But even moreso than that, understanding how to learn tools like these - at least to the depths you need them - will help you pick up new tools and tech as the world continues to advance. We aren’t going super deep into any tool - but we’ll go deep enough to make their benefits obvious. 10.1 R Markdown Bus Factor: The number of developers who have to be hit by a bus before your project is doomed. The Markdown programming language was developed in 2004 to let developers write HTML in a way that is human-readable and easily writeable. R Markdown is an implementation of that language in R, which lets you embed R code and graphs seamlessly into your documents. To get more information on R Markdown, check out Yihui Xie’s R Markdown: The Definitive Guide, written by the guy who made the format. We can start a new Markdown document in the same place we make new scripts and notebooks - click File -&gt; New File -&gt; R Markdown. This will open up a window with some default templates - Markdown can be used to make web apps, presentations, and even entire websites. You’ll also see that a number of journal formats are pre-loaded in the templates option. For now, let’s just open a blank document, and have it output as an HTML file. You’ll notice a lot of pre-filled in the document. At the very top (inside two lines of ---) is what’s known as the YAML header. This is where you can set a number of top-level options for your document, including font and margin sizes. This is also where you should specify your document title, author name, and date (using the ISO format: YYYY-MM-DD). We won’t mess too much with the header right now, but I do want to point out one important difference between YAML and the R code we’ve gotten used to. In R, we use parentheses to specify which arguments belong to which function. In YAML, we use spacing, with an indentation of two spaces taking the place of parentheses. For instance, if we wanted to add a table of contents to our HTML document, we could specify that with toc = TRUE in R as follows: html_document(toc = TRUE) In YAML, those parentheses get replaced by spacing: output: html_document: toc: true So the number of spaces you use matter in YAML, while they don’t in R. This still trips me up, which is why I emphasize it now! Now, you might notice that the Markdown document looks a lot like the R Notebooks we’ve been experimenting with throughout the course. That’s because they’re actually the same thing - notebooks are a specialized implementation of Markdown, which let you run chunks of code faster and easier than a markdown document. As such, you can write an entire document in a notebook, and then move it to Markdown once you’re looking to share the document around. All of the options we’ll cover below still apply to notebook documents, and you can still make chunks of code in Markdown by pressing Ctrl+Alt+I. The Markdown format lets you stylize your text in a way that doesn’t impede readability and won’t require reformatting every time page numbers change, for instance. It does take a second to learn, but luckily there are a number of cheatsheets and reference guides available as resources. There’s a ton of tools available for stylizing your documents. Here, we’re going to quickly demonstrate the most commonly used ones: # with a space after it makes headers - the more # you have in a row, the smaller the header. * with a space after it makes a bulleted list Numbers with a . or ) after them start a numbered list. You can also use #) or #. if you don’t want to worry about numbering the list yourself. For both types of list, you need an empty line of whitespace before the first bullet to make a list. By the way, putting four spaces before the * lets you make an indented bullet. Two spaces at the end of a line of text start a new paragraph. If you only use one space, both lines will print continuously on a single line. You can put spaces between paragraphs by entering a single line of white space. One set of _ or * on both sides of a word makes it italicized Two sets of __ or ** on both sides of a word make it bold Superscripts can be made using ^ around text Subscripts can be made using ~ around text Links are made following the format [text](link) You can make block quotes using &gt; before the text You can also use HTML directly in the body of your Markdown documents, if you need more specialized formats. When you want to view the output of your document, press the Knit button on the toolbar to turn your Markdown file into whatever output file type you’ve specified. You can then share the resulting file - in our case, an HTML document - with whatever stakeholder needs to see it. There’s plenty more depth to be discovered in how to best format your R Markdown documents - for instance, there are extensions to automatically add bibliographies and figure captions to your entire document - but we’ll leave those for another reader to cover. For now, the point in us going over how to use R Markdown is to enable you to communicate your code better, by making documents that explain what each of your code chunks is doing - and, more importantly, why. By having documents that clearly label and show each of the pieces of your code next to their outputs, researchers and programmers are better able to communicate their results and methodologies with one another. 10.1.1 Kable Now that we know how to generate pretty figures and text, it’s time we learn how to make attractive tables, too. We’ll be using the iris dataset as an example of how to use the kableExtra package to do just that. Note that most of this information comes from the kableExtra documentation, which I’ve found very easy to use. First off, let’s load the tidyverse: library(tidyverse) ## Registered S3 methods overwritten by &#39;ggplot2&#39;: ## method from ## [.quosures rlang ## c.quosures rlang ## print.quosures rlang ## Registered S3 method overwritten by &#39;rvest&#39;: ## method from ## read_xml.response xml2 ## ── Attaching packages ─────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.1.1 ✔ purrr 0.3.2 ## ✔ tibble 2.1.1 ✔ dplyr 0.8.0.1 ## ✔ tidyr 0.8.3 ✔ stringr 1.4.0 ## ✔ readr 1.3.1 ✔ forcats 0.4.0 ## ── Conflicts ────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() When you print out tables in a Markdown document, the generic output looks a lot like the plain text I’ve been using in the past units: head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa However, there happens to be a better way. Included in the knitr package is the function kable(), which makes attractive-ish tables: install.packages(&quot;knitr&quot;) library(knitr) head(iris) %&gt;% kable() Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa In my mind, though, a more attractive option is provided by the kable_styling() function from the kableExtra package: install.packages(&quot;kableExtra&quot;) library(kableExtra) ## ## Attaching package: &#39;kableExtra&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## group_rows head(iris) %&gt;% kable() %&gt;% kable_styling() Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa Once we have that table to work from, we’re able to customize it with a number of different options. For instance, if we want our table to have striped rows, which get darker when you hover a cursor over them, are less wide (more condensed) than usual, and are responsive to how big the user’s screen is, and we don’t want our table to take up the full screen width allowed, we can specify all those as options: head(iris) %&gt;% kable() %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;, &quot;responsive&quot;), full_width = FALSE) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa Heck, if we wanted to show all our data, we can include a scroll bar, to keep the printout from being overwhelming: iris %&gt;% kable() %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;, &quot;responsive&quot;), full_width = FALSE) %&gt;% scroll_box(height = &quot;250px&quot;) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa 5.4 3.4 1.7 0.2 setosa 5.1 3.7 1.5 0.4 setosa 4.6 3.6 1.0 0.2 setosa 5.1 3.3 1.7 0.5 setosa 4.8 3.4 1.9 0.2 setosa 5.0 3.0 1.6 0.2 setosa 5.0 3.4 1.6 0.4 setosa 5.2 3.5 1.5 0.2 setosa 5.2 3.4 1.4 0.2 setosa 4.7 3.2 1.6 0.2 setosa 4.8 3.1 1.6 0.2 setosa 5.4 3.4 1.5 0.4 setosa 5.2 4.1 1.5 0.1 setosa 5.5 4.2 1.4 0.2 setosa 4.9 3.1 1.5 0.2 setosa 5.0 3.2 1.2 0.2 setosa 5.5 3.5 1.3 0.2 setosa 4.9 3.6 1.4 0.1 setosa 4.4 3.0 1.3 0.2 setosa 5.1 3.4 1.5 0.2 setosa 5.0 3.5 1.3 0.3 setosa 4.5 2.3 1.3 0.3 setosa 4.4 3.2 1.3 0.2 setosa 5.0 3.5 1.6 0.6 setosa 5.1 3.8 1.9 0.4 setosa 4.8 3.0 1.4 0.3 setosa 5.1 3.8 1.6 0.2 setosa 4.6 3.2 1.4 0.2 setosa 5.3 3.7 1.5 0.2 setosa 5.0 3.3 1.4 0.2 setosa 7.0 3.2 4.7 1.4 versicolor 6.4 3.2 4.5 1.5 versicolor 6.9 3.1 4.9 1.5 versicolor 5.5 2.3 4.0 1.3 versicolor 6.5 2.8 4.6 1.5 versicolor 5.7 2.8 4.5 1.3 versicolor 6.3 3.3 4.7 1.6 versicolor 4.9 2.4 3.3 1.0 versicolor 6.6 2.9 4.6 1.3 versicolor 5.2 2.7 3.9 1.4 versicolor 5.0 2.0 3.5 1.0 versicolor 5.9 3.0 4.2 1.5 versicolor 6.0 2.2 4.0 1.0 versicolor 6.1 2.9 4.7 1.4 versicolor 5.6 2.9 3.6 1.3 versicolor 6.7 3.1 4.4 1.4 versicolor 5.6 3.0 4.5 1.5 versicolor 5.8 2.7 4.1 1.0 versicolor 6.2 2.2 4.5 1.5 versicolor 5.6 2.5 3.9 1.1 versicolor 5.9 3.2 4.8 1.8 versicolor 6.1 2.8 4.0 1.3 versicolor 6.3 2.5 4.9 1.5 versicolor 6.1 2.8 4.7 1.2 versicolor 6.4 2.9 4.3 1.3 versicolor 6.6 3.0 4.4 1.4 versicolor 6.8 2.8 4.8 1.4 versicolor 6.7 3.0 5.0 1.7 versicolor 6.0 2.9 4.5 1.5 versicolor 5.7 2.6 3.5 1.0 versicolor 5.5 2.4 3.8 1.1 versicolor 5.5 2.4 3.7 1.0 versicolor 5.8 2.7 3.9 1.2 versicolor 6.0 2.7 5.1 1.6 versicolor 5.4 3.0 4.5 1.5 versicolor 6.0 3.4 4.5 1.6 versicolor 6.7 3.1 4.7 1.5 versicolor 6.3 2.3 4.4 1.3 versicolor 5.6 3.0 4.1 1.3 versicolor 5.5 2.5 4.0 1.3 versicolor 5.5 2.6 4.4 1.2 versicolor 6.1 3.0 4.6 1.4 versicolor 5.8 2.6 4.0 1.2 versicolor 5.0 2.3 3.3 1.0 versicolor 5.6 2.7 4.2 1.3 versicolor 5.7 3.0 4.2 1.2 versicolor 5.7 2.9 4.2 1.3 versicolor 6.2 2.9 4.3 1.3 versicolor 5.1 2.5 3.0 1.1 versicolor 5.7 2.8 4.1 1.3 versicolor 6.3 3.3 6.0 2.5 virginica 5.8 2.7 5.1 1.9 virginica 7.1 3.0 5.9 2.1 virginica 6.3 2.9 5.6 1.8 virginica 6.5 3.0 5.8 2.2 virginica 7.6 3.0 6.6 2.1 virginica 4.9 2.5 4.5 1.7 virginica 7.3 2.9 6.3 1.8 virginica 6.7 2.5 5.8 1.8 virginica 7.2 3.6 6.1 2.5 virginica 6.5 3.2 5.1 2.0 virginica 6.4 2.7 5.3 1.9 virginica 6.8 3.0 5.5 2.1 virginica 5.7 2.5 5.0 2.0 virginica 5.8 2.8 5.1 2.4 virginica 6.4 3.2 5.3 2.3 virginica 6.5 3.0 5.5 1.8 virginica 7.7 3.8 6.7 2.2 virginica 7.7 2.6 6.9 2.3 virginica 6.0 2.2 5.0 1.5 virginica 6.9 3.2 5.7 2.3 virginica 5.6 2.8 4.9 2.0 virginica 7.7 2.8 6.7 2.0 virginica 6.3 2.7 4.9 1.8 virginica 6.7 3.3 5.7 2.1 virginica 7.2 3.2 6.0 1.8 virginica 6.2 2.8 4.8 1.8 virginica 6.1 3.0 4.9 1.8 virginica 6.4 2.8 5.6 2.1 virginica 7.2 3.0 5.8 1.6 virginica 7.4 2.8 6.1 1.9 virginica 7.9 3.8 6.4 2.0 virginica 6.4 2.8 5.6 2.2 virginica 6.3 2.8 5.1 1.5 virginica 6.1 2.6 5.6 1.4 virginica 7.7 3.0 6.1 2.3 virginica 6.3 3.4 5.6 2.4 virginica 6.4 3.1 5.5 1.8 virginica 6.0 3.0 4.8 1.8 virginica 6.9 3.1 5.4 2.1 virginica 6.7 3.1 5.6 2.4 virginica 6.9 3.1 5.1 2.3 virginica 5.8 2.7 5.1 1.9 virginica 6.8 3.2 5.9 2.3 virginica 6.7 3.3 5.7 2.5 virginica 6.7 3.0 5.2 2.3 virginica 6.3 2.5 5.0 1.9 virginica 6.5 3.0 5.2 2.0 virginica 6.2 3.4 5.4 2.3 virginica 5.9 3.0 5.1 1.8 virginica I’m not walking through all the options as in-depth as I usually will, because I think the documentation for this package is extremely well-done. I just wanted to give you the skills to control what your tables look like as much as you may want. 10.2 LaTex LaTeX (pronounced Lay-Tek) is another document creation language, much like Markdown. We won’t go too far into the minutae of LaTex, as it can be a little intimidating. We’ll only be looking at the parts of LaTex that we can use directly in Markdown, simply by putting dollar signs ($) around the LaTex command. Most commonly, these commands are used to make attractive equations or include special symbols, such as Greek letters and mathematical operators. Those operators are drawn by putting a \\ before the name of the symbol - for instance, the formula $$\\pi * 2^4 * \\sqrt{27}$$ would draw the equation: \\[\\pi * 2^4 * \\sqrt{27}\\] For the capitalized version of Greek letters, just capitalize the first letter of the name - $\\delta$ generates \\(\\delta\\), versus $\\Delta$ generating \\(\\Delta\\), for instance. Two dollar signs will start the equation (or symbol) centered on the next line, while one will generate the equation or symbol continuing on the same line. That’s about all the Latex we’ll have a need to go over in this course. If you’re interested in learning more, there’s a pretty good tutorial available on OverLeaf, and a publicly available cheatsheet. 10.3 Git(Hub) Git is a version control system originally developed in 2005 to let a lot of extremely talented coders work together on one of the largest, most complex open source projects the world has ever seen. It is a nightmare to learn, and every company does it slightly differently. However, it lets you document the changes you make to your code, your reasons for making those changes, and lets you revert to any historical state of the project - which can save tons of work reconstructing past states, if you find out one of your most recent changes breaks an important part of the software! GitHub is a company founded in 2008, designed to provide streamlined servers and plugins to allow teams and non-professionals to use Git on various projects. The tools it provides are less powerful than Git itself, but much faster to learn. For that reason, we’ll be using GitHub - and the tools developed specifically for using Git with GitHub - exclusively in this unit. There’s really no reason to learn Git by itself right now; it’s a good skill to pick up eventually, but not while busy learning another computer language. If you’re interested in a tutorial written specifically for Git, check out this blog by Karl Broman. My preferred workflow involves using tools that are built specifically for GitHub - I’ve personally switched to GitKraken recently, though GitHub Desktop provides a solid experience for new users. The instructions throughout this section will assume you’re also working with GitKraken - while you can use any other service (or just pure Git), there are no directions for those alternative services. Once you’ve installed one of these softwares, you’ll be asked to connect it to your account at GitHub. The biggest advantage GitHub gives (as opposed to Git by itself) is the ability to share your work with a broader community, and for that community to contribute to the development of your product. At the same time, that’s the biggest disadvantage - unless you’re paying for a premium account, your code is public for everyone to see. While it’s not particularly likely that anyone will find your code unless you advertise it - think of how many hits the average YouTube video gets, for instance - it’s still advisable to not host company secrets or unpublished data on the service. Luckily, Git supports hosting your project locally, rather than putting it out onto GitHub. While this loses the collaborative advantages of GitHub - and won’t serve as a backup, should your computer kick the bucket - it will still let you track your project and revert to past stages. 10.3.1 My First Repository It’s time to make your first repository! A repository - or, more commonly, a repo - is the folder where your project lives. Git will track the changes you make to every single file within the repo, and you’ll be able to revert to past commits any time you want. In GitKraken, we can make a new repo by clicking “Start a local repository” or via File-&gt;Init Repo. Let’s call this repository “GitExample”. You can choose a license if you want - I’m not going to get into the details of what each license entails - and save the repo wherever makes sense. Your GitKraken should now look something like this: Let’s make a new project R project now through R Studio. We want to create a project in an existing directory, and select our repository as the folder to create the project in. Once your new project has loaded, open a new script file (Ctrl/Cmd+Shift+N) and type in the following code: library(tidyverse) ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() We should know what this code will give us by now - the same iris scatterplot we’ve been working with for most of this reader. Save your script using whatever name makes sense to you (I went with Scatterplot.R). If we open GitKraken now, we should see a screen that looks like this: You’ll notice that the left-hand side of the screen has changed, and our new files are listed as “unstaged” changes. To understand what this means, let’s back up a little and talk about what a Git workflow looks like: 10.3.1.1 Commits The most recent version of your code - the one that anyone with access to your repository should see, that works and is up-to-date - is what’s known as your most recent commit. Commits are versions of the code where you’ve decided to confirm that you like your changes, and want to save them in your project’s development history. In order to do that, you have to be very specific in telling Git which files you want to include in that commit. Git keeps track of every file you’ve changed since your last commit, and will include them in that “Unstaged Files” window you see our files in. For those changes to be saved, however, we need to stage our changes, and then commit them. GitKraken makes that easy enough - we just click “Stage All Changes”, add a commit message, and then click “Commit Changes”. A note on commit messages, by the way - these are the main ways you’ll identify what each change actually did to your code when you’re looking at the history, and will be super important if you have to revert to a prior state! As such, try and keep each commit message short but explanatory - “did stuff” is much less helpful than “model selection algorithim now supports AIC”, for instance! On a similar note, you should commit whenever it makes sense to you to do so - when your code is in a place you may want to get back to. Generally this means that whatever you’ve written works, or is very close to working, or was enough effort to do that you don’t want to bother retyping it should you accidentally break it. There isn’t a hard and fast rule as to when you should commit - but the general rule is “early and often”. Anyway, go ahead and commit all your changes now. Personally, my message was “made the base scatterplot”. 10.3.1.2 Branches In the top bar of GitKraken, you should notice a button labeled “branch”. Clicking this will open a box that tells you to “enter branch name”. Right now, I’m naming this branch redplot. A “branch” is a copy of the code at the time that it’s created - so it “branches” off of the master code (sometimes referred to as the “trunk”). This way, multiple people can work on different parts of the project at the same time - and if you leave an idea half-baked for a while, it won’t disturb the main body of code. GitHub has a nice explanation of how branches work here, if you want more detail. Now that we’re working in the redplot branch, let’s change our code so that the points are red: library(tidyverse) ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point(color = &quot;red&quot;) We can now commit this change to our branch, updating the code there without disturbing the code in the master branch. Let’s do this now! You’ll notice now that you have two labels to your commit messages - master and redplot. If we’ve decided we like our plots red, we can merge the branch into the trunk by right clicking on master and selecting “merge redplot into master”. Tada! Our branches have merged, our change to the code has become incorporated in the master repository, and we’re understanding the basics of Git. The only thing left to do in this unit is to talk about working with GitHub. 10.3.1.3 Push + Pull Again, GitHub is an online service which makes working with others using Git much easier than most other services. It will host your repositories online and let others access them. To do that, we first have to make an online repository. Log into GitHub (or create an account, if you still haven’t) and then click the “+” sign next to your profile photo, then click “New Repository”. Give a name that makes sense (I chose “GitExample”) and don’t check any boxes, then click “Create Repository”. Copy the URL inside the blue box (labeled “Quick Setup”). Then, in GitKraken, hover over the “0/0” displayed next to “Remote” and click the “+” sign that appears. Type your repository’s name into the “Name” field, then paste the URL into the “Pull URL” field. Once you click “add remote” you’ll be connected to your online repository! We can now upload our code to this repository, using the “Push” button (located next to “Branch”) and then clicking “Submit”. Your code is now online (reload the GitHub page to see it), and accessible by collaborators! This is called “pushing” your code, and is the next step up from commits - if you’re committing early and often, you should be pushing every time you’ve got something new that works. The other side of “pushing” is “pulling”, done through the “pull” button in the top bar. If your collaborators have pushed code that impacts your work, you should make sure to pull it to your machine - otherwise, you might be introducing bugs into the system! This is a super basic introduction to Git, but should be enough to get you started using version control on your own projects. Once you do, it becomes much, MUCH easier to interact with other developers, and to collaborate on projects with people working anywhere in the world. 10.4 Commenting Code Commenting your code is like cleaning your bathroom - you never want to do it, but it really does create a more pleasant experience for you and your guests. — Ryan Campbell When you insert a hashtag (#) before a line in your code, R won’t try to run it. This can be used to explain what you’re doing at each step of a complicated process, and is referred to as commenting your code. For instance: ## Comments like this usually explain the purpose of a block of code ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() # Whereas comments like this usually explain a specific line It’s a best practice to continuously comment your code - both so that any collaborators you may have can understand what you’ve done, and so that you understand your own work when you come back to it in the future. However, you shouldn’t write comments only explaining what your code does - everyone touching your project should be able to understand that. Instead, good comments explain why you’re doing a particular thing - what your end goal and motivations for each step of the process are, so that others can follow your line of thinking. 10.5 Further Reading I had originally planned to write a section of this chapter on the basics of making R packages, in order to quickly and easily share functions and code with a wide variety of people. However, I think that may be slightly outside of the scope of this introduction level text - and there’s already a very good textbook on the subject written by people far more qualified than I. For a full explanation of why (and, to an extent, how) you should build packages, check out the Leek Group guide to developing R packages. Additionally, if you’re interested in getting further into Git, check out Pro Git available for free online. "],
["working-with-text.html", "11 Working with Text 11.1 Working with Stringr 11.2 Regular Expressions 11.3 Case Study 11.4 Further Reading 11.5 Exercises", " 11 Working with Text “If it is not written down, it does not exist.” — Philippe Kruchten So far, all of our work has dealt with numeric data. We’ve even gone so far as to convert categorical data (such as the type of medal won in the Olympics) into numeric formats when we want to include it in our analyses. There’s a few reasons for this. First off, the majority of data used in business and scientific applications can be treated as numeric in nature, meaning it makes the most sense for the front of this book to focus on that more common application. But mostly we started off with numeric data due to how much harder text can be to work with - it doesn’t lend itself to nice rectangular dataframes as easily as numeric data does, and tends to be much messier to wrangle than easy numeric formats. However, much data originates in text format, whether or not it will be used that way, and as such data scientists have to understand how to interpret and process it as easily as numeric data. As such, this chapter will teach you how to wrangle and manipulate text data into more usable formats for your analyses. This is, I believe, one of the hardest topics in this book, but a good understanding of how to use these tools will allow you to handle a much wider variety of datasets than numerical methods alone. Before we begin, I want to highlight that the title of this chapter is Working with Text, as opposed to Text-Based Analyses. While text-based analyses are commonly used in some spheres - for instance, sentiment analysis is popular in both the digital humanities and political science, and is gaining acceptance as a business tool - we’re more focused on the tools used to work with text than the following analyses. For good resources on those topics, check out the Text Mining with R book. 11.1 Working with Stringr Let’s say we have some text in a vector - known as a string: string &lt;- c(&quot;lzGDHgldkh3orange2o5ghte&quot;) (I never said it would be a pretty string.) We’re going to be working with this data using the stringr package, conveniently located in the tidyverse suite. As such, let’s load the tidyverse now: library(tidyverse) ## Registered S3 methods overwritten by &#39;ggplot2&#39;: ## method from ## [.quosures rlang ## c.quosures rlang ## print.quosures rlang ## Registered S3 method overwritten by &#39;rvest&#39;: ## method from ## read_xml.response xml2 ## ── Attaching packages ─────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.1.1 ✔ purrr 0.3.2 ## ✔ tibble 2.1.1 ✔ dplyr 0.8.0.1 ## ✔ tidyr 0.8.3 ✔ stringr 1.4.0 ## ✔ readr 1.3.1 ✔ forcats 0.4.0 ## ── Conflicts ────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() stringr contains a bunch of useful tools for analyzing and working with strings via the use of patterns - that is, the things you want to detect in the string itself. For instance, you may have noticed that there’s the word “orange” hidden in our string of text. If we just wanted to know if that word was there, we could do the following: str_detect(string, &quot;orange&quot;) ## [1] TRUE (By the way, all these functions that start with str_ are provided by the stringr package. While we’ll use a few functions from base R, stringr simplifies a lot of the hassle of working with strings, and so we’ll primarily be using those.) If we wanted to know where in the string it was, we could try this instead: str_locate(string, &quot;orange&quot;) ## start end ## [1,] 12 17 We can use this information to extract the word from our string using str_sub(): str_sub(string, 12, 17) ## [1] &quot;orange&quot; Or even use the str_extract() function to do so: str_extract(string, &quot;orange&quot;) ## [1] &quot;orange&quot; We can also replace the fruit, if we so desire: str_replace(string, &quot;orange&quot;, &quot;apple&quot;) ## [1] &quot;lzGDHgldkh3apple2o5ghte&quot; (Note that if we wanted to update our string object, we’d have to assign the new value using string &lt;- str_replace().) You can find a listing of all the other useful stringr functions on the package cheatsheet or the accompanying vignette. 11.2 Regular Expressions Some people, when confronted with a problem, think “I know, I’ll use regular expressions.” Now they have two problems. — Jamie Zawinski This is all pretty neat, but what happens if we didn’t know exactly what we’re looking for - just specific parts of the string that we want? For instance, GPS data is often stored in a messy text file, where the latitude and longitude - that is, the data we care about - are specifically marked with start and end brackets or similar markings. In those cases, we can’t type out each datapoint that we want - by introducing human errors and taking a long time to complete, that would seem to defeat the entire purpose of coding in the first place! In this case, we’d have to begin wading into the world of regular expressions, also known as regex or regexp. These are concise statements that let us find specific chunks of strings that we’re interested in by generalizing our problems slightly. For instance, the . operator in regex lets us select any character: str_extract_all(string, &quot;o....e&quot;) ## [[1]] ## [1] &quot;orange&quot; &quot;o5ghte&quot; Note that we’re using str_extract_all() here, so that our regex doesn’t just pick up the first match and terminate. While that would actually help in this situation - what’s that ugly thing doing in our output? - there’s better ways to only get what we’re interested in. For instance, the \\\\D operator will select any character that isn’t a number - so switching it out for . should do the trick: str_extract_all(string, &quot;o\\\\D\\\\D\\\\D\\\\De&quot;) ## [[1]] ## [1] &quot;orange&quot; If we wanted to select the ugly thing instead (for instance, to correct it to whatever it should be), we can use the \\\\d operator, which will select any numeric digit. str_extract_all(string, &quot;o\\\\d\\\\D\\\\D\\\\De&quot;) ## [[1]] ## [1] &quot;o5ghte&quot; You’ll notice this pattern throughout regex expressions, by the way - a lowercase letter will select any of the things it stands for (so \\\\d selects any digit, while \\\\s will select any blank space) while an uppercase letter will select everything BUT that thing (so \\\\D doesn’t select digits, \\\\S will erase blank spaces, and so on). Another commonly used operator is the * function, which means “as many times as it shows up”. For instance, say we had a string that looked like this: string2 &lt;- &quot;aeeeeeeeeeeeaeee&quot; If we wanted to select everything from the first a to the second, we could type out all the e’s. But rather than trying to make sure we typed the right number, it’s a lot easier for us to do something like this: str_extract(string2, &quot;ae*a&quot;) ## [1] &quot;aeeeeeeeeeeea&quot; This function also works with other operators - for instance, you can type .* to select all the characters within your other specifications: str_extract_all(string, &quot;o.*e&quot;) ## [[1]] ## [1] &quot;orange2o5ghte&quot; Note, though, that * always looks for the longest string it can find. To make it select the shortest string instead, add ? after the *: str_extract_all(string, &quot;o.*?e&quot;) ## [[1]] ## [1] &quot;orange&quot; &quot;o5ghte&quot; So now * splits the strings at each e, rather than continuing on to the last e in the string. This becomes helpful if you’re trying to extract multiple strings with the same pattern - for instance, let’s say we had a string with multiple o_e words in it, like this: string3 &lt;- c(&quot;lzGDHodegldopenkh3orange2o5ghte&quot;) We can get each of our words out by combining the operators we’ve discussed above: str_extract_all(string3, &quot;o\\\\D*?e&quot;) ## [[1]] ## [1] &quot;ode&quot; &quot;ope&quot; &quot;orange&quot; And if we want to incorporate those words into a dataframe, it’s as easy as unlist()ing them: tibble(x = 1:3, y = unlist(str_extract_all(string3, &quot;o\\\\D*?e&quot;))) ## # A tibble: 3 x 2 ## x y ## &lt;int&gt; &lt;chr&gt; ## 1 1 ode ## 2 2 ope ## 3 3 orange Hopefully you can start to see how these techniques can be useful - we can start turning chunks of text into usable dataframes! We’ve only covered the basics of regular expressions here - there are plenty of further specifications you can use, to make your code ever more efficient. A good reference moving forward is the regex cheatsheet, which contains an at-a-glance overview of the operators available for your use. 11.3 Case Study For this example, we’re going to be using data located in the GitHub data repository for this book, in the “Unit 9 Data” folder. This file - named LatLongFile.txt contains a good number of GPS data points, and runs approximately 1154 lines, with 190 individual datapoints. Looking at the file (in a text editor) shows me that each datapoint is stored as follows: wpt lat=&quot;43.485087&quot; lon=&quot;-74.949118&quot; Since these are consistently labeled, I can extract them from the text file using my sick new regex skills. First, I’ll load the file into R: LatLong &lt;- read_file(&quot;LatLongFile.txt&quot;) I’m now going to work on the regex expressions, starting with the one for latitude. Since our latitude is always signified by lat = &quot;, that’s how I’ll start my expression. However, remember that R will assume the &quot; means you’ve finished entering your pattern, and will throw an error about anything following the quote mark. As such, we’ll have to escape that character using \\ - writing \\&quot; in the place of &quot; will tell R to interpret the quote as part of the string, rather than as the end of the pattern. After the quote comes a whole bunch of data - the digits of our latitude - and then the string ends with another quote mark. As such, we can represent all the data inside the quotes using .*? - we can’t just use \\\\d*?, since we need to extract a period as well, and we need *? instead of * alone, in order to not just select the majority of the document text. We now have a regex that looks something like lat=\\&quot;.*?, which is very close to being finished. The last thing we’re going to do is tell R what the last character in the string is - another quotation mark. As such, we add \\&quot; to the end of our regex, for a final product of lat=\\&quot;.*?\\&quot;. Even though it looks like something a comic book character might swear at someone, that little expression is powerful enough to extract every single latitude in our file. We can actually use the exact same formula to get our longitudes - we just have to swap “lon” for “lat” in the expression. In practice, this looks something like: lat &lt;- str_extract_all(LatLong, &quot;lat=\\&quot;.*?\\&quot;&quot;) lon &lt;- str_extract_all(LatLong, &quot;lon=\\&quot;.*?\\&quot;&quot;) If we want to then convert those objects into vectors, we can use unlist: lat &lt;- unlist(lat) lon &lt;- unlist(lon) And then we can look at our results by making a tibble: head(tibble(lon = lon, lat = lat)) ## # A tibble: 6 x 2 ## lon lat ## &lt;chr&gt; &lt;chr&gt; ## 1 &quot;lon=\\&quot;-74.949134\\&quot;&quot; &quot;lat=\\&quot;43.485033\\&quot;&quot; ## 2 &quot;lon=\\&quot;-74.949118\\&quot;&quot; &quot;lat=\\&quot;43.485087\\&quot;&quot; ## 3 &quot;lon=\\&quot;-74.948893\\&quot;&quot; &quot;lat=\\&quot;43.485093\\&quot;&quot; ## 4 &quot;lon=\\&quot;-74.948766\\&quot;&quot; &quot;lat=\\&quot;43.485156\\&quot;&quot; ## 5 &quot;lon=\\&quot;-74.948748\\&quot;&quot; &quot;lat=\\&quot;43.485245\\&quot;&quot; ## 6 &quot;lon=\\&quot;-74.948725\\&quot;&quot; &quot;lat=\\&quot;43.485268\\&quot;&quot; It looks like we have a little work left for us! If we want to use the latitude and longitude for anything, we’re going to need to extract those numbers out from the text they’re in. For latitude, we want our expression to start at the first digit and select everything it can until the last digit in the cell. For longitude, we want our expression to start at the negative sign (since all our longitudes are negative - I recorded this data in the Western Hemisphere) and select everything it can until the last digit in the cell. As such, both of our patterns will make use of .*\\\\d - that is, “select everything until the last digit”. The only difference is that latitude will start with \\\\d, while longitude will start with -. lat2 &lt;- unlist(str_extract_all(lat, &quot;\\\\d.*\\\\d&quot;)) lon2 &lt;- unlist(str_extract_all(lon, &quot;-.*\\\\d&quot;)) Let’s see what that looks like: LatLonTib &lt;- tibble(lon = lon2, lat = lat2) head(LatLonTib) ## # A tibble: 6 x 2 ## lon lat ## &lt;chr&gt; &lt;chr&gt; ## 1 -74.949134 43.485033 ## 2 -74.949118 43.485087 ## 3 -74.948893 43.485093 ## 4 -74.948766 43.485156 ## 5 -74.948748 43.485245 ## 6 -74.948725 43.485268 While we could have done this all in one step, I was feeling like those regex were getting complicated enough as they were - I decided splitting things up might make it easier to learn. Either way, our output is a much neater table than we started out with! You might notice that our columns are still character vectors, though. That’s an easy enough fix, using mutate_all from dplyr: LatLonTib &lt;- LatLonTib %&gt;% mutate_all(as.numeric) head(LatLonTib) ## # A tibble: 6 x 2 ## lon lat ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -74.9 43.5 ## 2 -74.9 43.5 ## 3 -74.9 43.5 ## 4 -74.9 43.5 ## 5 -74.9 43.5 ## 6 -74.9 43.5 11.4 Further Reading As I mentioned earlier in this chapter, I view text-based analyses as beyond the scope of this book, and am only covering ways of wranging text data into tidy formats. For more information on text mining and text-based analyses in R, check out the tm package vignette, as well as the prior-linked Text Mining with R book. 11.5 Exercises Each datapoint in the LatLonFile also has an elevation and a name associated with it - try extracting those as well, and making a dataframe with columns for all four variables. Note that you won’t be able to use mutate_all() once the name column is added - either add it (with cbind()) after converting the columns to numeric, or use mutate_at(-name, as.numeric) to select all the other columns. What’s the longest regex you can come up with to extract the times for each point? What’s the shortest? "],
["working-with-dates-and-times.html", "12 Working with Dates and Times 12.1 Dates in R 12.2 Converting To Dates 12.3 Extracting From Dates 12.4 Math with Dates 12.5 Time Zones", " 12 Working with Dates and Times How did it get so late so soon? It’s night before it’s afternoon. December is here before it’s June. My goodness how the time has flewn. — Dr. Seuss Before we get started, let’s load the lubridate package. While part of the tidyverse (so you’ve already got it installed), we have to load it separately, since it’s not as commonly used as the other ones: library(tidyverse) ## Registered S3 methods overwritten by &#39;ggplot2&#39;: ## method from ## [.quosures rlang ## c.quosures rlang ## print.quosures rlang ## Registered S3 method overwritten by &#39;rvest&#39;: ## method from ## read_xml.response xml2 ## ── Attaching packages ─────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.1.1 ✔ purrr 0.3.2 ## ✔ tibble 2.1.1 ✔ dplyr 0.8.0.1 ## ✔ tidyr 0.8.3 ✔ stringr 1.4.0 ## ✔ readr 1.3.1 ✔ forcats 0.4.0 ## ── Conflicts ────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(lubridate) ## ## Attaching package: &#39;lubridate&#39; ## The following object is masked from &#39;package:base&#39;: ## ## date Time series data is essential to a huge number of analysis tasks, now that data readers can collect information every hour - or every second. Knowing how a process changes over time - whether it’s how warm soil is, how many animals a camera has seen, or how often people have clicked on your link - allows insight into a large variety of questions. However, dates and times can be surprisingly hard to deal with. For instance, the date the last time I compiled this book was 2019-06-19 - that’s YYYY-MM-DD format, which we’ll talk about in a minute. At first, this format makes a lot of sense - every 24 hours, we add a 1 to the day; after a certain number of days, we add a 1 to the month; and after 12 months (or 365 days), we add a 1 to the year. It’s just a slightly different form of numerical data. But there’s a lot of messiness under those easy rules. For instance, not every year has 365 days, and not every day has 24 hours - thanks to the leap year’s lesser-known cousin, the leap-second, but also due to daylight savings time, where we sometimes have 23 or 25 hours in a day. There’s really no standardization of how many days a month has. Things are complicated. Plus, unlike other numerical data, there’s no good concept of what 0 means for dates. There’s no month 0, there’s no day 0, and - confusingly enough - there’s no year 0, with year 1 separating the BC and CE eras. And all that confusion comes before the fact that we use two different calendar systems to refer to years pre- and post-1582. Luckily enough, you usually won’t have a ton of data from before 1582. But it’s still worth realizing that dates are just numbers on a kinda weird numberline - or, a perfectly normal numberline, where our labels are a little bit weird. Due to how tricky numbers can be, this chapter is going to focus entirely on manipulating and wrangling date and time data, without any attempt at analyses. Statistical analyses of time-series data can be extremely complex, and as such are outside the scope of this book; a useful resource on implementing time-series analyses in R may be found here. 12.1 Dates in R R is capable of understanding pure numbers as dates. For instance, if we use the as_date() function from lubridate, we can convert numbers to dates as such: as_date(9910) ## [1] &quot;1997-02-18&quot; To help understand this conversion, it might make sense to call as_date(0): as_date(0) ## [1] &quot;1970-01-01&quot; So here we can see what R uses as the 0 for its numberline - instead of being January 1, 1 CE, it chooses January 1, 1970, for… reasons. R then adds or subtracts the number of days inside the parentheses to that “0” in order to give us an output date: as_date(-678) ## [1] &quot;1968-02-23&quot; Importantly, as_date() is doing the fundamentally same thing that as_character() or any of the similar functions do - it’s converting a numeric object into a date object. We can demonstrate this for ourselves by calling class() on an object created by as_date(): DateData &lt;- as_date(9910) class(9910) ## [1] &quot;numeric&quot; class(DateData) ## [1] &quot;Date&quot; You can do math with date objects the same way you can with numeric ones - the only difference being that these objects will use the weird timeline we use for dates, incorporating leap years and so on. We can also work with date-times, objects which include (you guessed it) both a date and a time. Unlike dates, where adding 1 increments the value by a day, adding 1 to a date-time increments by a second. as_datetime(1) ## [1] &quot;1970-01-01 00:00:01 UTC&quot; as_datetime(9910) ## [1] &quot;1970-01-01 02:45:10 UTC&quot; Date-times are incredibly more complicated than dates, due to the need to manage time zones. Generally speaking, it’s best to use the date format unless your data require times be used as well. 12.2 Converting To Dates Of course, most data won’t store dates as integers - we usually store dates in more human readable forms, like 02/18/1997 or 1997-02-18. Now, the best format to store dates in is the YYYY-MM-DD format, known as ISO 8601. This format - which is an internationally agreed upon standard - is clear and unambiguous, and lets newcomers to your data know automatically what they’re looking at. This format requires you use dashes (-) instead of slashes (/), and that you always use the long form of each number - for instance, January 9, 2018 would be 2018-01-09, never 18-1-8. To parse strings formatted like this, use the lubridate command ymd() - an acronym for how your data is formatted: year-month-day: ymd(&quot;2018-02-18&quot;) ## [1] &quot;2018-02-18&quot; Unfortunately, you won’t always be in control of how your data is formatted. Luckily, lubridate has a family of functions for understanding dates - just rearrange the letters of the acronym as needed: dmy(&quot;18/02/1997&quot;) ## [1] &quot;1997-02-18&quot; mdy(&quot;2/18/97&quot;) ## [1] &quot;1997-02-18&quot; The same family of function can also be used with datetimes, with an underscore and some combination of “h” “m” “s” added: ymd_hms(&quot;2018-02-18 5:20:05 pm&quot;) ## [1] &quot;2018-02-18 17:20:05 UTC&quot; Note that you can either use 24-hour time or specify if a date is AM/PM - R will convert to 24-hour time either way. We can also build a time out of components stored in different columns, using the make_date() or make_datetime() commands: TimeTable &lt;- tibble(year = 2018, day = 18, hour = 5, month = 2, seconds = 5, minute = 20) TimeTable %&gt;% mutate(date = make_date(year, month, day), time = make_datetime(year, month, day, hour, minute, seconds)) ## # A tibble: 1 x 8 ## year day hour month seconds minute date time ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; &lt;dttm&gt; ## 1 2018 18 5 2 5 20 2018-02-18 2018-02-18 05:20:05 (Note that the inputs in the table are integers, not character vectors like we fed ymd() et. al.) 12.3 Extracting From Dates Say we have the following date: OurDate &lt;- ymd_hms(&quot;2018-02-18 5:20:05 pm&quot;) If we only want specific components, we can extract them one by one using a few helper functions: OurDateExploded &lt;- tibble(year = year(OurDate), month = month(OurDate), day = day(OurDate), hour = hour(OurDate), second = second(OurDate), dayOfWeek = wday(OurDate), dayOfMonth = mday(OurDate), dayOfYear = yday(OurDate)) OurDateExploded ## # A tibble: 1 x 8 ## year month day hour second dayOfWeek dayOfMonth dayOfYear ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2018 2 18 17 5 1 18 49 (There’s no difference between day() and mday(), as best as I can tell - but you’ll see both used in example code online.) We can also use these commands to change an individual component faster than retyping the whole datetime: year(OurDate) &lt;- 2020 OurDate ## [1] &quot;2020-02-18 17:20:05 UTC&quot; We can also use update() to change multiple parts of our date at once: update(OurDate, year = 2021, hour = 19, mday = 20) ## [1] &quot;2021-02-20 19:20:05 UTC&quot; 12.4 Math with Dates Sometimes, rather than just updating our times by hand, we’ll have to do a few more complicated things with them. For instance, if we need to truncate or round our date, we can use one of a family of functions: floor_date() will round the date down to the unit specified round_date() will round the date as appropriate to the specified unit ceiling_date() will round the date up to the unit specified rollback() will roll the date back to the last day of the previous month For instance, using each of these with our date looks like this: floor_date(OurDate, unit = &quot;month&quot;) ## [1] &quot;2020-02-01 UTC&quot; round_date(OurDate, unit = &quot;month&quot;) ## [1] &quot;2020-03-01 UTC&quot; ceiling_date(OurDate, unit = &quot;month&quot;) ## [1] &quot;2020-03-01 UTC&quot; rollback(OurDate) ## [1] &quot;2020-01-31 17:20:05 UTC&quot; More complicated math than that gets a little weird. For instance, say we have a date that’s right next to a leap year: leap &lt;- ymd(&quot;2019-03-01&quot;) Now, if we wanted to add a year to this date, what would we expect as an output? Probably March 1, 2020, which is what we get if we add years(1) to our date: leap + years(1) ## [1] &quot;2020-03-01&quot; The problem is, this answer only makes sense if you care more about the unit than what it represents. That is, we just added a year to our date - but that year had 366 days, rather than the 365 we expect a year to represent. As such, there are two different families of functions to do math with dates in R, depending on what you want as an output: If you want to ignore the irregularities of the time numberline, you’re working with periods - so adding a year to March 1, 2019, gives us March 1, 2020. If you care about the actual passage of time - so adding a year means adding 365 days, no matter what - you’re working with durations. Luckily, the functions to work with each are pretty similar - just add a “d” in front of any function that works with periods and you’ll be working with durations. For instance: leap + dyears(1) ## [1] &quot;2020-02-29&quot; The same problem happens every year with daylight savings time - which is further complicated by the fact that it only happens in certain time zones. Luckily, the same family of functions works in these situations: lap &lt;- ymd_hms(&quot;2018-11-04 00:30:00&quot;,tz=&quot;US/Eastern&quot;) lap + minutes(90) ## [1] &quot;2018-11-04 02:00:00 EST&quot; lap + dminutes(90) ## [1] &quot;2018-11-04 01:00:00 EST&quot; Note, by the way, there’s no dmonths() function - every year has 12 months, so that one’s a lot less likely to trip you up. Lastly, if you’re trying to represent the span in between two times, you can do so using interval() - just specify the starting and ending dates in the function: interval(leap, leap + dyears(1)) ## [1] 2019-03-01 UTC--2020-02-29 UTC 12.5 Time Zones Time zones are complicated as hell, since they’re more political boundaries than any sort of real feature of how time passes or any other natural constant. There are over 600 time zones built into R, some with identical names (you’d be shocked how many countries have “Eastern standard time”), and which zone applies to a particular area changes based on time of year and political whims. There’s been some attempt to standardize how we work with time zones, most importantly by the Internet Assigned Numbers Authority. The IANA specifies names for each zone, generally using Continent/City format (US/Eastern, Pacific/Auckland). You can check your computer’s timezone using the Sys.timezone() function: Sys.timezone() ## [1] &quot;America/New_York&quot; Or see the full list using OlsonNames() (warning, it’s a long list). By default, lubridate uses UTC, which is the standard for most computing: OurDate ## [1] &quot;2020-02-18 17:20:05 UTC&quot; But time zones don’t really control anything other than how a date is printed - after all, R is just storing the date as a number, like we showed at the start of this chapter. But if you need to change what time zone your data is stored as, there are two methods. First, if you want the same time represented in a different timezone, use with_tz(): with_tz(OurDate, tz = &quot;America/New_York&quot;) ## [1] &quot;2020-02-18 12:20:05 EST&quot; OurDate == with_tz(OurDate, tz = &quot;America/New_York&quot;) ## [1] TRUE Secondly, if your data has been mislabeled and you need to change the time zone (and the actual time with it), we can use force_tz(): force_tz(OurDate, tz = &quot;America/New_York&quot;) ## [1] &quot;2020-02-18 17:20:05 EST&quot; OurDate == force_tz(OurDate, tz = &quot;America/New_York&quot;) ## [1] FALSE With these functions, you should be all set to start wrangling date and time data with R. By the way, the lubridate package has a wild amount of edge-case functions built into it - I highly recommend referencing the cheatsheet available online for functions that we aren’t covering here. "],
["what-next.html", "13 What Next 13.1 Machine Learning Methods 13.2 Leaflet Maps 13.3 FlexDashboard 13.4 Bookdown 13.5 Blogdown 13.6 Shiny", " 13 What Next At this point, you’ve got a pretty solid understanding of the basics of coding in R. Depending on your purposes for learning the language, there’s a number of different topics that you could focus in on next. The purpose of this chapter, near the end of our book, is to list a number of these possible areas with resources to learn about them, so that you may have an easier time discovering new topics of interest. 13.1 Machine Learning Methods Machine learning is a complicated set of tools used by analysts to make more accurate predictions than classical regression and classification methods allow. R is pretty good at implementing these algorithms, using packages such as caret and keras. While a full discussion of machine learning is outside the scope of this text, there are a number of useful resources available online for further learning. This book is the best resource I’ve found for free, but there are a number of (not-free) courses available online after a quick Google. 13.2 Leaflet Maps Leaflet is, in its original form, a JavaScript library for making interactive maps. The implementation in R, via the leaflet package, allows you to embed these maps in R Markdown documents and other web-based formats. For a great tutorial on how to use this package, check out the RStudio leaflet website. 13.3 FlexDashboard knitr::include_graphics(&quot;Dash.png&quot;) FlexDashboard is a system for making dashboards via R Markdown, combining text and visualizations into highly effective data-driven dashboards well suited to communicate your results to any stakeholders. These boards can be split across multiple pages, scroll, and incorporate all sorts of useful packages, making this package a wonderful resource for communicating your results. The best way to learn more is on the FlexDashboard website. 13.4 Bookdown If you’re writing a book which incorporates R code, outputs, and can be formatted using R Markdown, one of the best packages for your needs is the Bookdown package. By formatting your book as a collection of R Markdown documents, you’re able to write your entire book in the RStudio environment, and see how your graphics and code will look to the reader while you write. The best resource for starting with Bookdown is the Bookdown book, with this blog by Sean Kross another useful reference. (And yes, this book was entirely written in Bookdown). 13.5 Blogdown Similar to the Bookdown package is Blogdown, which lets you use R Markdown to create websites. While it’s slightly more complicated - it requires you to fuss with Hugo somewhat, and understand a little about web design - it’s pretty easy to install a template and produce a professional looking website, with minimal coding required. The best place to learn more about Blogdown is the Blogdown book, with this blog by Alison Hill also useful in getting started. 13.6 Shiny People really, really like interactive graphics. Being able to click their way through a series of visualizations - or maybe just alter the aesthetic choices of a final output - satisfies a lot of non-data-professionals in a big way. I think interactive graphics are usually a waste of resources - while they’re cool, they take away your ability to narrate a story and effectively demonstrate your arguments, which are the main purposes of communicative graphics. While interactive graphics are more useful for data exploration, the amount of time they take to generate and polish makes them cost-prohibitive for this type of use. There are better tools for making these sorts of interactives, and they aren’t particularly useful in the printable types of documents most readers of this book will find themselves making. With that screed out of the way, it’s worth noting that your boss (and clients) likely think interactives are cool, whether or not you like it. And so R’s shiny package does a pretty good job of letting people make interactive charts and HTML widgets with minimal pain - and some jobs do view proficency with Shiny a plus. You can find a list of resources for learning Shiny at the official website, including written tutorials and a free DataCamp course. "],
["basic-statistics-using-r.html", "14 Basic Statistics (Using R) 14.1 Purpose of the Unit 14.2 Definitions 14.3 Exercises", " 14 Basic Statistics (Using R) You ever hear the joke that a data scientist is someone who knows more computer science than a statistician and more statistics than a data scientist? (I never said it was a good joke.) — Joel Gruss 14.1 Purpose of the Unit This isn’t a statistics book. However, R is a statistical computing language, and many of the functions built into R are designed for statistical purposes. As such, we’re going to very quickly go over some statistical terms and a few of the statistical functions built into R. That way, you’ll have a better understanding of what exactly it is that we’re doing in later chapters. 14.2 Definitions 14.2.1 Data Concepts We’ve already discussed some data concepts in this course, such as the ideas of rectangular and tidy data. However, those discussions are buried in the text of the last chapter, so are hard to refer to - and I want to make sure these concepts are all contained in the same place, for a clean reference section. Vector Sequence of data elements of the same type Each element of the vector are also called components, members, or values Created in R using c() Dataframe A list of vectors of identical lengths Example: iris Variable A trait or condition that can exist in different quantities or types We measure the impacts of independent predictor variables on dependent response variables Continuous Data Numeric data which is not restricted to certain values - there are an infinite number of possible values Discrete Data Numeric data which is restricted to certain values - for example, number of kids (or trees, or animals) has to be a whole integer Categorical Data Data which can only exist as one of a specific set of values - for example, house color or zip code Binned numeric data (e.g. “between 1 and 2 inches”) is typically categorical Binary Data Categorical data where the only values are 0 and 1 Often used in situations where a “hit” - an animal getting trapped, a customer clicking a link, etc - is a 1, and no hit is a 0 Ordinal Data A type of categorical data where each value is assigned a level or rank Useful with binned data, but also in graphing to rearrange the order categories are drawn Referred to in R as “factors” Unstructured Data Data without a strict format, typically composed of text R used to deal with unstructured data by converting it to factors; while this isn’t necessary anymore, some functions still require text data to be in factor form Data Distribution How often every possible value occurs in a dataset Usually shown as a curved line on a graph, or a histogram Normal Distribution Data where mean = median, 2/3 of the data are within one standard deviation of the mean, 95% of the data are within two SD and 97% are within 3. Many statistical analyses assume your data are normally distributed Many datasets - especially in nature - aren’t Skewed Distribution Data where the median does not equal the mean A left-skewed distribution has a long tail on the left side of the graph, while a right-skewed distribution has a long tail to the right Named after the tail and not the peak of the graph, as values in that tail occur more often than would be expected with a normal distribution 14.2.2 Statistical Terms Estimate A statistic calculated from your data Called an estimate as we are approximating population-level values from sample data Synonynm: metric Hypothesis Testing Comparing the null hypothesis (typically, that two quantities are equivalent) to an alternative hypothesis The alternative hypothesis in a two-tailed test is that the quantities are different, while the alternative hypothesis in a one-tailed test is that one quantity is larger or smaller than the other Almost never used in business, as the important question is usually not does x cause y but can x predict y p Value: The probability of seeing an effect of the same size as our results given a random model High p values often mean your independent variables are irrelevant, but low p values don’t mean they’re important - that judgement requires a rational justification, and examining the effect size and importance. Otherwise you’re just equating correlation and causation. The 0.05 thing is from a single sentence, taken out of context, from a book published in 1925. There’s no reason to set a line in the sand for “significance” - 0.05 means that there’s a 1 in 20 probability your result could be random chance, and 0.056 means it’s 1 in 18. Those are almost identical odds. Some journals have banned their use altogether, but others still will only accept “significant” results Statement from the American Statistical Association: A p value, or statistical significance, does not measure the size of an effect or the importance of a result. By itself, a p value does not provide a good measure of evidence about a model or a hypothesis. “Robust” A term meaning an estimate is less susceptible to outliers Means are not robust, while medians are, for instance. Regression A method to analyze the impacts of independent variables on a dependent variable ANOVA and models are both types of regression analyses General Linear Model Formulas representing the expected value of a response variable for given values of one or more predictors The typical y = mx + b format of model Sometimes abbreviated GLM; R uses lm() to construct these Generalized Linear Model Depending who you ask, these may or may not be linear models - they tweak the normal formula in one way or another to measure outcomes that general linear models can’t address In this course, we’ll only be using logistic models Sometimes abbreviated GLM; R uses glm() to construct these 14.2.2.1 Estimates and Statistics n The number of observations of a dataset or level of a categorical. In R, run nrow(dataframe) or length(Vector) to calculate. To calculate by group, run count(Data, GroupingVariable) Examples: nrow(iris), length(iris$Sepal.Length), count(iris, Species) Mean The average of a dataset, defined as the sum of all observations divided by the number of observations. In R, run mean(Vector) to calculate. Example: mean(iris$Sepal.Length) Trimmed Mean The mean of a dataset with a certain proportion of data not included The highest and lowest values are trimmed - for instance, the 10% trimmed mean will use the middle 80% of your data mean(Vector, trim = 0.##) mean(iris$Sepal.Length, trim = 0.10) Variance A measure of the spread of your data. var(Vector) var(iris$Sepal.Length) Standard Deviation The amount any observation can be expected to differ from the mean. sd(Vector) sd(iris$Sepal.Length) Standard Error The error associated with a point estimate (e.g. the mean) of the sample. If you’re reporting the mean of a sample variable, use the SD. If you’re putting error bars around means on a graph, use the SE. No native function in R - run sd(Vector)/sqrt(length(Vector)) to calculate. sd(iris$Sepal.Length)/sqrt(length(iris$Sepal.Length)) Median Absolute Deviation from the Median Average distance between each datapoint and - in R - the median A measure of spread in your data Note that MAD often means mean absolute deviation from the mean, mean absolute deviation from the median, and a few other less common things - check your acronyms before using! mad(Vector) mad(iris$Sepal.Length) Median A robust estimate of the center of the data. median(Vector) median(iris$Sepal.Length) Minimum The smallest value. min(Vector) min(iris$Sepal.Length) Maximum The largest value. max(Vector) max(iris$Sepal.Length) Range The maximum minus the minimum. max(Vector) - min(Vector) max(iris$Sepal.Length) - min(iris$Sepal.Length) Quantile The n quantile is the value at which, for a given vector, n percent of the data is below that value. Ranges from 0-1. Quantile * 100 = percentile. Quartiles are the 0.25, 0.5, and 0.75 quantiles quantile(Vector, c(quantiles that you want)) quantile(iris$Sepal.Length, c(0.25, 0.5, 0.75)) Interquartile Range The middle 50% of the data, contained between the 0.25 and 0.75 quantiles IQR(Vector) IQR(iris$Sepal.Length) Skew The relative position of the mean and median. At 0, mean = median, and the data is normally distributed. Not included in base R. Kurtosis The size of the tails in a distribution. In R, values much different from 0 are non-normally distributed. Not included in base R. 14.2.3 Models and Tests Note that most tests discussed here default to a 95% confidence level and a two-tailed test. If you want to learn how to change those for any function, type ?FunctionName(). Functions not applicable to the iris() dataset do not currently have examples underneath them. Correlation How closely related two variables are Pearson’s test assumes your data is normally distributed and measures linear correlation Spearman’s test does not assume normality and measures non-linear correlation Kendall’s test also does not assume normality and measures non-linear correlation, and is a more robust test - but it is harder to compute by hand, and as such is less commonly seen You cannot compare results from one type of test to another - Kendall’s results are always 20-40% lower than Spearman’s, for instance cor(Vector1, Vector2) provides correlation coefficients, while cor.test(Vector1, Vector2) performs the statistical test, giving test statistics, p values, and other outputs. Both perform the Pearson test by default, but can be changed by providing the argument method = &quot;spearman&quot; or method = &quot;kendall&quot; cor(iris$Sepal.Length, iris$Sepal.Width, method = &quot;pearson&quot;) cor.test(iris$Sepal.Length, iris$Sepal.Width, method = &quot;pearson&quot;) t Test A method of comparing the means of two groups If your group variable has more than two levels, don’t use a t test - use an ANOVA instead t.test(Vector1, Vector2) Chi-squared Test A test to see if two categorical variables are related The null hypothesis is that both variables are independent from one another For more information - particularly on what form your data should be in - please see this site. chisq.test(Vector1, Vector2) Linear Models A type of regression which predicts the value of a response variable at given values of independent predictor variables If you need help understanding linear models, here’s a lecture from Penn State. The most important thing for our purposes is understanding that models are made up of terms, which are the predictor variables you give it, and coefficients, which are multiplied by those terms to calculate the value of our response. Larger coefficients generally mean that variable is more important than others in the model, but only if the variables share units - one (much argued-over) way to compare coefficients between variables with different units is discussed here Linear models also include an intercept term, which isn’t multiplied with any variable. This term being “significant” at p = whatever means literally nothing, and if it’s your only significant term, your model is useless. You can add multiple predictor vectors using +, and include interaction terms using + Vector1:Vector2. lm(ResponseVector ~ PredictorVectors, data) lm(Sepal.Length ~ Species, data = iris) Logistic Models A form of generalized linear model where the predictor variable is a binary vector Extremely common in science and business to predict events - if a tree or animal will die, if a sale will be made, etc glm(ResponseVector ~ PredictorVectors, data, family = &quot;binomial&quot;) ANOVA A test to identify the impacts of one or more categorical variables on one or more numeric response variables An altered form of the linear model This blog post does an interesting treatment on how to interpret ANOVA results with significant interactions anova(lm(ResponseVector ~ PredictorVectors, data)) Normally, you’ll save your model to an object using &lt;-, then run anova() on that object - so model &lt;- lm(Sepal.Length ~ Species, data = iris), then anova(model) - though nesting your functions as above returns the same result 14.2.4 How We’ll Compare Models There are plenty of different ways to compare models, each with their own proponents and detractors. Rather than wade into those arguments, we’re going to use three of the most common metrics. Other metrics (RMSE, PRESS, BIC, etc) can also be generated from R’s modeling functions, but we won’t go into depth on those. R2 The percentage of variance in your data explained by your regression The more independent predictor variables in a model, the higher the R2, all else being equal The adjusted R2 is a better estimate of regression goodness-of-fit, as it adjusts for the number of variables in a model summary(lm(ResponseVector ~ PredictorVector, data)) model &lt;- lm(Sepal.Length ~ Species, iris) then summary(model) RMSE Root-mean square error Standard deviation of the residuals - so tells you how accurate your model is Lower == better Receiver Operating Characteristic Curve R2 isn’t applicable to logistic models Instead, we calculate the area under the ROC curve, with the area under the curve being abbreviated AUC AUC represents the accuracy of your model, with random guessing having an AUC of 0.5 and a perfect model having an AUC of 1 AUC is sometimes referred to as the c-statistic AIC: Akaike Information Criterion For a single dataset, the model with the smallest AIC is your best model But models with a \\(\\Delta\\)AIC (the difference between their two AICs) of &lt; 2 (or 4, depending who you ask) are statistically identical If your model has a \\(\\Delta\\)AIC of &lt; 2 with the null model, it’s useless Null model: lm(ResponseVariable ~ 1) Cross Validation: A method in which data is split into separate training and testing datasets, to evaluate the performance of the model on new data The gold standard for examining model performance is for a model to be trained and tuned using the training datasets, then tested once and only once by generating predictions and calculating accuracy for the test dataset. You then report the statistics from this test. A variation is k-fold cross validation, where the dataset is split into k sections. Models are then trained using every combination of k-1 sections (so if k = 3, three separate models would be trained on the combined 1st and 2nd, 2nd and 3rd, and 1st and 3rd datasets) and tested on the one section not included. More information on cross validation and its implementation in R here Confusion Matrices: A table showing how often your model predicted an outcome correctly and how often it performed incorrectly This provides both a sense of overall accuracy and how exactly the model is inaccurate - for instance, which category it’s most likely to misclassify Provides a way of quantitatively and qualitatively comparing different model formulas 14.3 Exercises All examples here use the Orange dataset, which is automatically included in R. Note that the O is capitalized! Look at Orange using either head or as.tibble() (you’ll have to run library(tidyverse) for that second option). What type of data are each of the columns? Find the mean, standard deviation, and standard error of tree circumference. Make a linear model which describes circumference (the response) as a function of age (the predictor). Save it as an object with &lt;-, then print the object out by typing its name. What do those coefficients mean? Make another linear model describing age as a function of circumference. Save this as a different object. Call summary() on both of your model objects. What do you notice? Does this mean that trees growing makes them get older? Does a tree getting older make it grow larger? Or are these just correlations? Does the significant p value prove that trees growing makes them get older? Why not? "],
["other-resources.html", "15 Other Resources Infographics Courses Textbooks Blog Links Data Sources Graphing Aids", " 15 Other Resources This book should give you a decent introduction to the basics of R, and give you a taste of the more involved applications the technology can tackle. But there are plenty of things I didn’t bother tackling, and plenty of topics we barely brushed on. Below are links to other resources which may help you further your learning, once you have the basics from this course. All of these resources are free-as-in-beer - the links will go to the full, open-access text. Infographics R Cheatsheets contain information-dense infographics for many of the packages we’ve used in this course, and plenty other useful tools you may need in your own work. Courses These links go to other courses on R and related topics. Note that most of the packages we use were developed c. 2017 - courses older than that probably use different methods to do the same tasks, as a result. Reproducible Research from 2014 by Eric C. Anderson is a good primer on basic R Advanced Data Science from 2018 at John Hopkins University is a nice introduction to more advanced R coding. Textbooks R for Data Science covers many of the same topics we’ve touched on in this course, but is structured differently and may explain some points better than I have or chose to do. Introduction to Statistical Learning provides a much more thorough introduction to machine learning methods than our brief overview. Text Mining with R serves as an introduction to text-based analyses (such as sentiment analysis and n-grams). Hands-on Programming with R by Garrett Grolemund approaches R from a more coding-oriented perspective, as opposed to our output-focused method. Some parts of the subject are already well-covered in this course, while others - the sections on objects and environments, for instance - we barely touch on. Advanced R by Hadley Wickham similarly addresses many coding-focused aspects of R that we’ve glossed over in this course. This book might be a bit harder to get started with, but will make you a better programmer. The R Inferno by Patrick Burns provides a humorous look at many of the most common mistakes made by R users. The R Markdown Textbook by Yihui Xie provides a comprehensive overview of writing markdown documents. The bookdown textbook by Yihui Xie gives you all the information you need to start putting together your own book documents. The blogdown book by Yihui Xie will get your website up and running in no time at all. Pro Git covers a lot of the basics of Git itself, in case you have a need to move beyond depending upon Github. [Git Guide] by Karl Broman offers a minimalist tutorial to the basics of Git. Blog Links Data Organization by Karl Broseman is a must-read on how to format your raw datasheets - and here’s a citation. 10 things any new grad student should do GitHub: a primer for researchers (in my opinion, should be named “why use GitHub?” - it isn’t a tutorial) An Intro to Git and Github for Beginners Interpreting residual plots to improve your regression The Leek Group Guide to Developing R Packages I’m a really big fan of the Leek Group tutorials on scientific skills. Here are the non-programming related resources they’ve produced: Data Sharing Giving Talks Reading Papers Data Sources UC Irvine’s Machine Learning Repository Graphing Aids ColorSupply is a helpful tool for selecting colors ColorBrewer.org lets you see all the palettes available in the RColorBrewer package "],
["frequently-asked-questions.html", "16 Frequently Asked Questions 16.1 Why R? 16.2 Why is my code broken? 16.3 Why is (X package) named that?", " 16 Frequently Asked Questions Here are the answers to some assorted questions I’ve been asked, which are either not common, important, or interesting enough to put in the main text of the reader. 16.1 Why R? This has to be one of the most common questions people new to data ask - and for good reason! No one wants to waste their time learning how to use the wrong tool, after all. As a result, this is also one of the most answered questions online. Most articles draw comparisons between R and its closest competitor, Python. And in fact, when the question is phrased as “should I learn R or Python”, the correct answer is probably “both”! However, you’ll make exponentially faster progress if you focus on learning just one tool at a time, and I’ve come to believe R to be the right tool to get started with for data analysis, for a few reasons: R has a lot more built-in functionality, and an abundance of specializied packages, meaning you’ll spend less time reinventing the wheel than when you’re starting with Python R is much, much, much easier to learn than Python. This hasn’t always been the case - most sources pre-2016 will identify R as being the harder language to learn - but a number of recent packages (particularly the tidyverse) have smoothed over a lot of R’s worst quirks R makes it much easier to generate reports and similar communication tools than Python. Considering that this is the bulk of many entry-level data jobs (and most scientific uses), R is typically better suited to the needs of the new analyst. It is (in my opinion) easier to switch to Python from R than the reverse. That being said, Python is undoubtedly the better tool for deploying your statistical models, or scraping data, or building software, and many other tasks besides. Data science - separate from data analysis - depends upon Python to do many things faster and more cleanly than R. But those are tasks to learn once you’ve mastered the tasks in this book. Almost all data professionals wind up using both languages eventually - but you have to learn one first, and I think R is the best decision. 16.2 Why is my code broken? Are all your parentheses in the right places? Do you have commas where you should? How’s your capitalization? Did you load the package you’re trying to use? If none of these fix your problem, try googling the error message R gives you. There’s usually a good StackOverflow question on whatever you’re trying to accomplish. 16.3 Why is (X package) named that? ggplot refers to the Grammar of Graphics, used to plot things in a consistent manner tibble refers to an old command for making tables, written tbl() and pronounced, well… rpart stands for R Partitioning, as decision trees are made by partitioning the data caret stands for Classification And REgression Training (oof) IDEAr (the acronym for this book) is named because it’s how my father pronounces idea, and I saw the opportunity for an homage while naming "],
["changelog.html", "17 Changelog 17.1 Version 1.1.0 17.2 Version 1.0.1 17.3 Version 1.0.0", " 17 Changelog 17.1 Version 1.1.0 Released 2019-06-20 Split chapter 3 into two (Workflow/Wrangling), because those are different topics and these aren’t course notes anymore, so relatedness matters more than each unit being similar length. That said, I want the Workflow chapter to be longer or rolled into another unit, so we’ll see how that lasts. Restructured chapters on Wrangling, EDA, and “More Complex Analyses” to now more tighly match the data analysis pipeline model - now form the Wrangling-EDA-Model chapters. Reordered chapters - stats to the end as a glossary, and others shifted to make room for the pipeline model in the middle. Edits to chapter 3 (Workflow). This chapter is such a nub, I want to roll it into something else - but it shouldn’t be earlier and doesn’t make sense to be later, so. Edits to chapter 4 (Wrangling), basically stealing all of chapter 3 Edits to chapter 5 (EDA), calming it down somewhat Edits to chapter 6 (Modeling), which basically rewrote the thing Slowly changing some just bizarre markdown syntax I was using (three graves for in-line code, for instance) Added parts worth saving of cut chapters into chapter 6. I’m planning on retooling this entire chapter - making it an introduction to modeling concepts. Changed file names for pretty much every chapter; whoops. Introduced massive redundancy to 9; working on reducing for next build Moved myself to Boston and relocated most of my hours to my new company; book building is going slower Goals for next incremental release: - Fix typos and general bad flow through chapters 3, 4, 5, 6 - and establish how much stats I expect people to come to the table with; it needs to go to either “basic stats” or “none” - Cleaning edits Goals for next minor release: - The above, AND - Rewrite 9 17.2 Version 1.0.1 Released 2019-04-01 (no joke!) Added “State of the Book” disclaimer to the introduction, as changes are being made Changed references to “course” and “reader” to “book”, reflecting new purpose of document Deleted 12: Machine Learning and 11: Specialized Applications, with intents to roll the relevant pieces into EDA and MCA Split chapter 1 into two (Introduction/Data Viz) due to length; this made every chapter number change Edits to chapter 1, chapter 2 17.3 Version 1.0.0 This is the build I started tracking changes from, last updated 2019-01-24. At this point, the book had 12 chapters, as well as an FAQ and list of resources. "]
]
