# Data Wrangling

> In God we trust. All others must bring data.
> <div align = "right"> --- W. Edwards Deming</div>

## Thinking with Data
At the start of this book, I established two essential aims for what this book would accomplish:

>The goal is to leave you with the basic essentials of working in R, as well as a strong foundation in thinking like a data analyst that will help you understand how to tackle more complicated problems.

So far, we've mostly focused on that first goal, which has obvious measurable outcomes - hopefully, you've found that working in R is somewhat easier now than when you first started. However, we haven't yet touched the second, for one specific reason: analytic thinking is much, much harder to do than learning a language. There are fewer strict rules, leaving much more room for judgement calls, and the world won't throw an error code if you make a mistake. That being said, it's a critical skill set to develop if you're looking to use R effectively.

It's important to note that there's a certain amount of art in data science (heck, [that's a book](https://bookdown.org/rdpeng/artofdatascience/)), and this chapter only reflects a certain model of how that art is best applied. You might come across other frameworks that work better for you. However, if you're relatively new to working with data, this should be a good starting point for you to work from. 

## The Data Analytics Model
My personal preferred model is very simple, with four steps:

1. Wrangle your data
2. Explore the data
3. Build models from the data
4. Communicate your results

This is a slightly modified version of [Peng and Matsui's model](https://bookdown.org/rdpeng/artofdatascience/), and it bears resemblance to [Grolemund and Wickham's version](https://r4ds.had.co.nz/introduction.html). This makes sense - these models are all different ways of describing the same processes, after all. We're going to dedicate a chapter to each of these steps, starting from the top with data wrangling.

I'd also like to note that these steps assume you've already decided what problem you're looking to solve and how you might go about it. Despite the best promises of pundits and pushers worldwide, Big Data^tm^ cannot solve every challenge in the world, and just poking around any dataset you've been given might stir up some incorrect or misguided conclusions. 

## Wrangle
However, if you know what problem you're looking to solve, your first step is gathering the right data to start identifying possible solutions. I'm not going to pretend that I can speak to what that gathering might look like for you - the steps you'll go through to get your data are much different if you're working in sciences versus the corporate world, and differ greatly even between various disciplines. What I can say, though, is that most data found in the wild contains a number of features that make it harder to analyze with computer software. Your data might be stored in a weird format, have missing or incorrect values, or be so unstructured that computer analyses don't understand how to parse it - for instance, this is how a lot of text data is originally gathered. We'll generally be working with structured data throughout this course - the sort of data you could put into a spreadsheet - but [there are plenty of resources on how to deal with unstructured data](https://www.tidytextmining.com/) if that's an area you find yourself interested in.

However, even our structured data will often not be in the ideal format for our desired analysis. As such, we have to spend our time `wrangling` the data (sometimes referred to as `munging`, though that term is falling out of favor) to get it into the shape we want it. This is often the most time consuming part of your work - estimates vary, but wrangling usually makes up 70-80% of the time spent on an analysis! Luckily enough, there are a number of versatile tools designed for wrangling our data which will take a lot of the trouble out of the process.

### Tidy Data
I mentioned earlier that we'd be primarily working with structured data, like you could put into a spreadsheet. In fact, we'll be working with one specific type of structured data, known as *rectangular data*. This is the term used for that spreadsheet-esque data format, where data is neatly kept in columns and rows. There are other, [more complex](http://www.cs.cmu.edu/~clo/www/CMU/DataStructures/Lessons/lesson4_1.htm) data structures that we could use, but we won't need them in this course.

Instead, in this course, we'll almost always be working with a very specific type of rectangular data known as _tidy data_. Tidy dataframes always take the same shape:

```{r echo = FALSE}
library(tidyverse)
data.frame("." = c("Observation 1", "Observation 2","...", "Observation n"),
           "Variable_1" = c("Value", "Value", "...", "Value"),
           "Variable_2" = c("Value", "Value", "...", "Value"),
           "Variable .." = c("Value", "Value", "...", "Value"),
           "Variable_n" = c("Value", "Value", "...", "Value")) %>%
  knitr::kable() %>%
  kableExtra::kable_styling()
```

Tidy data is organized as follows:

* Each column is a single **variable**
* Each row is a single **observation**
* Each cell is a single **value**

You can't expect most data you encounter in the wild to already be tidy - but that's why data workers make the big bucks. Luckily enough, there are tools designed to get you from untidy to tidy data easily, so you can then follow up with the fun parts of analyses. We'll be working

### Tidying Data
As you might guess from the name, the `tidyverse` is specifically designed to work with tidy datasets. Let's load it now:
```{r}
library(tidyverse)
```

By storing all data in a tidy format, we're able to quickly apply the same sets of tools to multiple different types of data. For instance, imagine a dataframe of seasonal temperatures, built as such:

```{r}
SeasonalTemps <- data.frame(Year = c(2015, 2016, 2017, 2018),
           Winter = c(40, 38, 42, 44),
           Spring = c(46, 40, 50, 48),
           Summer = c(70, 62, 81, 76),
           Fall = c(52, 46, 54, 56))
SeasonalTemps
```

(By the way, if you're working in notebooks, I'd suggest typing each of these blocks of code into their own chunk - press Ctrl/Cmd + Alt + I in order to quickly add a new code chunk to your document.)

This dataframe makes some sense - it's pretty easy to understand as a human reader, and would probably be a good layout for a printed table. But the problems with this format become obvious when we, for instance, try to graph the data:

```{r}
ggplot(SeasonalTemps, aes(x = Year)) + 
  geom_line(aes(y = Winter), color = "purple") + 
  geom_line(aes(y = Spring), color = "green") + 
  geom_line(aes(y = Summer), color = "blue") + 
  geom_line(aes(y = Fall), color = "red")
```
What a mess! That took far too long to type - a good general rule of thumb is that if you have to repeat yourself more than twice to do something, there's a better way to do it. And, even after all our effort, our graph doesn't have a legend, and the Y axis is labeled wrong.

This is a good time for us to use those tools I mentioned earlier, to turn our data tidy! Luckily enough, the `tidyverse` contains a package designed for making our data tidier - called, helpfully enough, ```tidyr```. We already loaded this package when we called the tidyverse earlier.

`tidyr` provides two essential functions for "reshaping" the data - changing back and forth between the _wide_ format we used above and a _long_ format, easier used by our functions. To change our ```SeasonalTemps``` data to a long format, we can use the ```gather()``` function. This function _gathers_ values stores in multiple columns into a single variable, and makes another variable - the _key_ variable - representing what column the data was originally in.

```gather()``` takes three important arguments:

* ```data```, the dataframe to gather  
* ```key```, what to name the key column   
* ```value```, what to name the column data was merged into

Additionally, we can specify columns that we want to preserve in the new, long dataframe by putting ```-ColumnName``` at the end of the function.

What this looks like for our seasonal data is something like this:

```{r}
LongTemps <- gather(data = SeasonalTemps, key = Season, value = AvgTemp, -Year)
LongTemps
```

Note that you don't have to type ```data = ```, ```key = ```, and ```value = ``` - if you don't, R assumes that you've listed the arguments in this order. 

This format makes graphing significantly easier:

```{r}
ggplot(LongTemps, aes(x = Year, y = AvgTemp, color = Season)) + 
  geom_line()
```

If, after all our hard work, we want to get back to our original wide format, we can undo our ```gather()``` using ```spread()```. Again, I'm giving spread a data, key, and value argument - but this time, the function is making a new column for each value of our key:

```{r}
WideTemps <- spread(LongTemps, Season, AvgTemp)
WideTemps
```

This new dataframe isn't quite the same as our original - the columns are now in alphabetical order! If we wanted to rearrage them, I find the easiest way is using the ```select()``` function from ```dplyr()```, another package in the tidyverse. By giving ```select()``` an argument for data and a vector of column names, we can rearrange the order the columns appear:

```{r}
OrderWideTemps <- select(WideTemps, c(Year, Winter, Spring, Summer, Fall))
OrderWideTemps
```

When doing this, though, we have to be careful we don't accidentally forget a column:

```{r}
select(WideTemps, c(Year, Winter, Spring, Fall))
```

Although, if we wanted to drop a column, we can do so by using a ```-``` sign:

```{r}
select(WideTemps, -Summer)
```
### Separating Values
Another way many datasets break the tidy format is by storing more than one value in a cell. For instance, say we had a dataframe of horse race results from three races, where the results were all written in the same column:
```{r}
df <- tibble(Horses = c("A", "B", "C"),
             Results = c("1-2-3", "3-1-2", "2-3-1"),
             TotalMinutes = c(3, 3, 3),
             TotalSeconds = c(12, 44, 15))
df
```

This makes some amount of sense as a human reading it, but makes it really hard to do any sort of analysis. So we can use the `separate()` command from `tidyr` to split that Results column into three columns, one per race. 

`separate()` needs at minimum four arguments to work properly:

* `data`, the dataframe we're operating on,
* `col`, the name of the column we're splitting up,
* `into`, a character vector (that you'd create using `c()`) of the names you want to use for the new columns, and
* `sep`,  the character that separates each of your values (in this case, `"-"`).

Altogether, that gives us a function that works something like this:

```{r}
df2 <- separate(df, Results, c("FirstRace", "SecondRace", "ThirdRace"), sep = "-")
df2
```

We can also see that there are some times, split into two columns (minutes and seconds) off to the right side of the table. If we'd rather those be in a single column, we can use the `unite()` function. This function works pretty similarly to `separate()`, with one important difference: while in `separate()`, the second argument was the column to be _split_, in `unite()` the second argument is the name of the column you want to combine values into:

```{r}
unite(df2, TotalTime, TotalMinutes, TotalSeconds, sep = ":")
```


## The Pipe
Looking back to our weather data, we can pick out a common problem facing young analysts. At this point, we've created four dataframes - ```SeasonalTemps```, ```LongTemps```, ```WideTemps```, and ```OrderedWideTemps``` - which all contain the same data. When repeatedly making similar but different dataframes, it can be hard to keep track of which object has which data - and it can be hard to keep coming up with simple, descriptive names, too. One solution could be to keep overwriting the same object with the new data:

```{r}
a <- 10
a <- a*2
a <- sqrt(a)
```

But this breaks our rule - that if you have to repeat yourself more than twice, there's a better way to do it. Plus, if you make a mistake while writing over a value that had your original data in it, you have to start all over again - assuming that your data was saved anywhere else!

You could also try and do it all in a single statement:
```{r}
a <- sqrt(10*2)
```

But with bigger demands, this gets more complicated and harder to read:
```{r}
unite(
  (separate
   (df, 
     Results, 
     c("FirstRace", "SecondRace", "ThirdRace"), 
     sep = "-")), 
  TotalTime, 
  TotalMinutes, 
  TotalSeconds, 
  sep = ":")
```
That sort of code is what nightmares are born from!

Luckily, the tidyverse also introduces a new operator ```%>%```, called the pipe, in the package `magrittr`. What the pipe does is pretty intuitive - it takes the output of whatever's on the _left_ side of the pipe, and uses it as the first input to whatever's on the _right_ side. For instance:

```{r}
Numbers <- c(5,10,15,20,25)

Numbers %>%
  mean()
```

Since all of the tidyverse functions take ```data``` as their first argument, this lets us _chain_ together multiple functions and skip those assignment steps without having to nest functions inside of each other:

```{r}
LongTemps %>%
  spread(Season, AvgTemp) %>%
  select(-Summer)
```

This makes our code much more easy to understand than constantly using the ```<-``` operator, plus it's an improved way to perform multiple steps in a way that's easy to read and harder to make serious mistakes doing.

Even when a function doesn't have data as its first input, you can still use a pipe by typing ```data = .``` into the function:

```{r}
LongTemps %>%
  spread(data = ., Season, AvgTemp) %>%
  select(-Summer)
```

And pipes work well with ggplot2, too:

```{r}
LongTemps %>%
  ggplot(aes(x = Year, y = AvgTemp, color = Season)) + 
  geom_line()
```
(Note the change from `%>%` to `+` once you're inside the graph-building process - `ggplot` was designed before its creator learned to love the pipe, and as such uses `+` to combine steps.)

## Data Transformations 

### Mutate
The pipe becomes useful when we want to transform our data itself for a graph, rather than transform the axes. For example, remember how we made our log-log graph last chapter?

```{r}
LongTemps %>%
  ggplot(aes(x = Year, y = AvgTemp, color = Season)) + 
  geom_line() + 
  scale_y_log10()
```

This is useful, but ggplot only has a certain number of transformations built in (type ```?scale_y_continuous()``` for more info). Additionally, sometimes we'll want to transform our data for analyses - not just graphing. For this purpose, we can use ```dplyr```'s ```mutate()``` function. Mutate takes three arguments: the dataframe (which it can get from ```%>%```), the name of your new column, and what value the new column should have. Say, for example, we wanted to multiply our average temperatures by two:

```{r}
LongTemps %>%
  mutate(TwiceTemp = AvgTemp * 2)
```

You can make multiple columns in the same ```mutate()``` call, even referring to columns made earlier in the same block of code:

```{r}
LongTemps %>%
  mutate(TwiceTemp = AvgTemp * 2,
         TwiceSquaredTemp = TwiceTemp^2,
         YearSeason = paste(Year, Season))
```

Notice I used a new function, ```paste()```, for that last column. Similarly to `unite()`, this function pastes together values into a single cell. However, unlike `unite()`, it can use other values in a dataframe, vectors, or strings. For instance:

```{r}
LongTemps %>%
  mutate(YearSeason = paste("The", Season, "of", Year))
```

Anyway. 

If you're transforming your data and only want to preserve your new columns, use ```transmute()```:

```{r}
LongTemps %>%
  mutate(TwiceTemp = AvgTemp * 2) %>%
  transmute(AvgTemp = AvgTemp,
            TwiceSquaredTemp = TwiceTemp^2,
            YearSeason = paste(Year, Season))
```
Note how AvgTemp is present in the new table while TwiceTemp is not, due to the former being included in the `transmute()` call.

### Tibbles
As I mentioned earlier, data in R is stored in _dataframes_. However, you may have noticed that the dataframe outputs from tidyverse functions look pretty different in your R session (I'd even say nicer) than our raw datasets! That's because of another useful tidyverse package, ```tibble```. 

Of course, the outputs in this book are pretty much the same - the technology I'm using to publish this isn't quite that advanced, yet.

We don't need to get too far into the mechanics of this package - if you load the tidyverse, any new dataframes you make will be converted into tibbles by default. If you want to force a dataframe into this format, use ```as.tibble()```; if you need the basic dataframe, use ```as.data.frame()```.

### Subsetting Data

Let's go back to our ```iris``` dataset. I'm going to turn it into a tibble and then view it:

```{r}
iris <- as.tibble(iris)
iris
```

If we only wanted to work with part of this dataset, R gives us a lot of options to _subset_ the data. For instance, if we only wanted the first column containing sepal length, we could type this:

```{r}
iris[, 1]
```

If we wanted the first row, meanwhile, we'd type this:

```{r}
iris[1, ]
```

If we wanted several rows, we can specify them with ```c()``` or, if they're consecutive, ```:```. For instance:

```{r}
iris[c(1,2,3,4), ]
iris[1:4, ]
```


And if we wanted the value in the first row of the first column, we'd type this:

```{r}
iris[1,1]
```

The pattern should be clear now - inside of the braces, you type the row number, a comma, and then the column number. Notice that ```[]``` always gives us a tibble (or dataframe) back. If we wanted a vector, we could use ```[[]]```:

```{r}
iris[[1, 1]]
```

If we want to use column names instead of numbers, we could use ```$``` in the place of ```[[]]``` - note that this always returns a vector, not a dataframe:

```{r}
iris$Sepal.Length
```

The ```$``` is really helpful in using other base R functions:

```{r}
mean(iris$Sepal.Length)
sd(iris$Sepal.Length)
cor.test(iris$Sepal.Length, iris$Sepal.Width)
```

(Note that "cor.test()" runs Pearson's correlation test for whatever vectors you feed it - more on that test later, or [here](https://bookdown.org/ndphillips/YaRrr/correlation-cor-test.html)).

And ```$``` also lets us filter our data with conditionals - getting values that are equal to something, larger or smaller than it, and so on. For instance, if we want a dataframe (so ```[]```) where the rows (```[, ]```) all have a Species value of (```==```) "setosa":

```{r}
iris[iris$Species == "setosa", ]
```

Note that the species name is in quotes, because it's a character string. We don't have to do that for numeric values:

```{r}
iris[iris$Sepal.Length > 7.5, ]
```

You can use ```==```, ```>```, ```>=```, ```<```, ```<=```, and ```!=``` (not equal) to subset your data.

### Filtering with the Tidyverse
This code is hard to read as a human, and doesn't work well with other functions. Instead, for more involved subsets, dplyr has a useful ```filter()``` function. It takes two arguments - your dataframe and the condition it should filter based on:

```{r}
iris %>%
  filter(Species == "setosa")
```

```filter()``` can use all the same operators as the ```[]``` methods of subsetting. Additionally, you can use ```&``` ("and") and ```|``` ("or") to chain filters together:

```{r}
iris %>%
  filter(Species == "setosa" & Sepal.Length == 5.1 & Sepal.Width == 3.3)
```

It's important to remember that ```&``` means things which satisfy EACH condition. A common mistake is to type:

```{r}
iris %>%
  filter(Species == "setosa" & Species == "versicolor")
```

Which, because no flower is both species, returns nothing.

In this case, you can either use an ```|``` ("or") operator, or - particularly if you have several cases you want to accept - ```%in%```:

```{r}
iris %>%
  filter(Species %in% c("setosa",
                        "versicolor"))
```

So long as your species is ```%in%``` the vector ```c()``` you provide, it will show up in the output.

(By the way, if you've coded with other languages, you might be used to seeing `&&` and `||` as the relevant operators. In R, these only check against the first value they evaluate against, and as such are almost never what you want to use. This is an incredibly common mistake made by people new to R!)

### Working with Groups

Say we wanted to find the mean sepal length in our dataset. That's pretty easy:

```{r}
mean(iris$Sepal.Length)
```

But we already know from our graphs that sepal length differs dramatically between species. If we wanted to find the mean for each species, we could calculate it individually for each group:

```{r collapse = TRUE}
setosa <- iris %>%
  filter(Species == "setosa")
virginica <- iris %>%
  filter(Species == "virginica")
versicolor <- iris %>%
  filter(Species == "versicolor")


mean(setosa$Sepal.Length)
mean(virginica$Sepal.Length)
mean(versicolor$Sepal.Length)
```

But that code is messy, the output is without any context, and it goes against our rule - that if you have to repeat yourself more than twice, there's a better way to do it.

The better way in the tidyverse is to use _grouping_ and _summary_ functions. In the following example, we'll use ```group_by()``` to group our dataframes by the species types, and ```summarise()``` to calculate the mean for each of them (in a column called "MeanSepalLength"):

```{r}
iris %>%
  group_by(Species) %>%
  summarise(MeanSepalLength = mean(Sepal.Length))
```

This is a faster and easier to understand way to perform functions on groups of data. Note that ```summarise()``` uses the British spelling - almost all functions in R have British and American spellings built in (you can use ```color``` or ```colour``` aesthetics in ggplot, for instance), but this is an important exception. While there is a function called ```summarize()```, it's highly glitchy and its use is highly discouraged.

You can use ```group_by()``` to calculate all sorts of things - for instance, we can calculate the distance of each plant's sepal length from the group mean, as follows:

```{r}
iris %>%
  group_by(Species) %>%
  mutate(SLDistanceFromMean = Sepal.Length - mean(Sepal.Length))
```

If you want to calculate variables for the whole dataset again, you'll have to ungroup your data - dataframes will stay grouped until you actively ungroup them with ```ungroup()```. For instance, to calculate the distance of each plant's sepal length from the overall mean:

```{r}
iris %>%
  select(c(Sepal.Length, Species)) %>%
  group_by(Species) %>%
  mutate(SLDistanceFromGroupMean = Sepal.Length - mean(Sepal.Length)) %>%
  ungroup() %>%
  mutate(SLDistanceFromTotalMean = Sepal.Length - mean(Sepal.Length))
```

(Note that I got rid of some columns with ```select()``` to make all the columns in the tibble fit on one page.)

## Missing Values

### Explicit Missing Values
Working with data, there are often two types of missing values we have to worry about. The obvious one are _explicit_ missing values, represented in R as ```NA``` (or, sometimes, ```NaN```). Let's make a dataframe:

```{r}
MissingExample <- tibble(w = c(1, 2, 3),
                         x = c("A", "B", "C"),
                         y = c("do", "re", NA),
                         z = c(807, NA, 780))
MissingExample
```

(I'm using ```tibble()``` in place of ```dataframe()``` here, but the outcome is almost identical.)

```NA``` values are a little tricky to work with - look what happens when we try to find the mean of ```z```:

```{r}
mean(MissingExample$z)
```

The reason this happens is because we _don't know_ what the mean is - that NA value could be anything, so it's impossible to know what the mean is. To get around this, we can set the ```na.rm``` argument to ```TRUE```:

```{r}
mean(MissingExample$z, na.rm = TRUE)
```

We can also solve the problem with filtering out the ```NA``` values. We can use ```is.na()``` to find out where certain values are, and then ask ```filter()``` to remove those rows from our dataset as follows:

```{r}
MissingExample %>%
  filter(!is.na(z)) %>%
  summarise(Mean = mean(z))
```

```!``` means "negation" in R, or "opposite" - so we're asking ```filter()``` to return the _opposite_ of any row where ```z``` is ```NA```, or, alternatively, all the rows where it has a value.

If we wanted to drop _every_ row that has a ```NA```, we could use the following ```tidyr``` function:

```{r}
MissingExample %>%
  drop_na()
```

Or, if we knew the values we wanted those ```NA``` to represent, we could use ```replace_na()```, also from ```tidyr```. We just have to specify a list of what we want those values to be:

```{r}
MissingExample %>%
  replace_na(list(y = "mi", z = "078"))
```

Notice a difference in the ```z``` column with this example? Because I put "078" in quotes, it changed the entire column to a character vector - because quotes mean characters, and a vector can only hold one class of data.

We'll talk more about that ```list()``` function later on - that's a little too complicated for this unit.  

### Implicit Missing Values
The other, harder to identify type of missing value is the _implicit_ missing value. Say we have a dataframe ```TreeData```, which lists the species that are present at two different sites:

```{r}
TreeData <- tibble(Site = c("A","A","A","B","B"),
                   Species = c("Red Maple", "Sugar Maple", "Black Cherry", "Red Maple", "Sugar Maple"),
                   Count = c(10,5,15,8,19))
TreeData
```

This system makes a lot of sense - each row represents something you found at each site. The problem with this comes when we try to calculate summary statistics for each species:

```{r}
TreeData %>%
  group_by(Species) %>%
  summarise(Mean = mean(Count), StandardDev = sd(Count))
```

Black cherry has a missing (```NaN```) standard deviation, because as far as R knows, it only has one observation to make estimates with. In reality, the fact that black cherry was missing from site B is a data point in and of itself - it's an _implicit_ value of 0.

To fix that, we can use the ```complete()``` command from ```tidyr```. This function takes column names as arguments, and returns a dataframe with every combination of the values in those columns. We can also specify what to replace ```NA``` values with, much like we did in ```replace_na()```, with ```fill```:

```{r}
TreeData %>%
  complete(Site, Species, fill = list(Count = 0))
```

This way, when we go to calculate our summary statistics, we get better answers:

```{r}
TreeData %>%
  complete(Site, Species, fill = list(Count = 0)) %>%
  group_by(Species) %>%
  summarise(Mean = mean(Count), StandardDev = sd(Count))
```

## Count Data
One other common issue with field data is that it's in a summary form - for instance, our tree data summarizes the number of trees at each site into one column. This is often easier to record in the field and easier to read as a human - but it makes some analyses much harder!

The function ```uncount()``` makes this pretty easy for us:

```{r}
LongTreeData <- TreeData %>%
  uncount(Count)

LongTreeData
```

And if we wanted to get back to the summary table, we can use ```count()```:

```{r}
LongTreeData %>%
  count(Site, Species)
```

If we want to change that column ```n```'s name to something more descriptive, we can use ```rename()```:

```{r}
LongTreeData %>%
  count(Site, Species) %>%
  rename(Count = n)
```

### Exploratory Data Analysis
Following the data wrangling comes the first new skill we're going to work on - exploratory data analysis.

You might find something surprising!
#### A Note on Surprises
Speaking of surprises, I really enjoy [this quote](https://fivethirtyeight.com/features/election-update-why-our-model-thinks-beto-orourke-really-has-a-chance-in-texas/) from Nate Silver, founder and editor in chief of FiveThirtyEight:

> You ideally want to find yourself surprised by the data some of the time — just not too often. If you never come up with a result that surprises you, it generally means that you didn’t spend a lot of time actually looking at the data; instead, you just imparted your assumptions onto your analysis and engaged in a fancy form of confirmation bias. If you’re constantly surprised, on the other hand, more often than not that means your [code] is buggy or you don’t know the field well enough; a lot of the “surprises” are really just mistakes.  
> ---Nate Silver

Surprises are awesome, and are how discoveries are made in science. But at the same time, a lot of papers are retracted because their big surprise was actually just a glitch in the code. Whenever you find something you didn't expect, make sure you go back through your code and assumptions - it never hurts to double check!

For more on this topic, check out the awesome lecture notes for [Skepticism in Data Science](https://jhu-advdatasci.github.io/2018/lectures/12-being-skeptical.html) from John Hopkins University.
summary statistics, correlations, anova, 
### Model
regression (linear + logistic)
model visualizations
cross-validation
model evaluation
### Communicate
mostly punting