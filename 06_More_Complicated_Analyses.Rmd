# More Complicated Analyses

> Computers are cheap, and thinking hurts.
><div align = "right">---Uwe Ligges</div>

## Other Datasets

### Importing Your Own Data
So far in this course, we've been working exclusively with the data pre-available in R and a few of the packages we've taken advantage of. While this is a good way to learn R, at some point you may want to use your own data for analysis.

Luckily, R has a number of functions used to import data from external files. To demonstrate these, I'll be using datasets located [on GitHub](https://github.com/mikemahoney218/Unit6Data), in the "Datasets" folder. You don't necessarily need to download these files - but we'll be using these for demonstration throughout this unit.

These datasets are from the website [Kaggle](https://www.kaggle.com/), where a number of data professionals share methods and datasets. Specifically, we'll be working with two datasets concerning [all Olympic athletes from the first 120 years of the game](https://www.kaggle.com/heesoo37/120-years-of-olympic-history-athletes-and-results). Additionally, we'll use data on [NBA athletes from the 2014-2015 season](https://www.kaggle.com/drgilermo/nba-players-stats-20142015). I'm not a huge sports guy myself, but there's a huge amount of sports-related data available publicly online, so it's a good tool for our analyses.

We're going to be assuming that your data are located in the same folder as your script - this is usually the easiest way to manage datasets. Otherwise, you'll have to wrestle a little with using relative pathways and directory names - Hadley Wickham explains these in more detail here.

In order to do this, we'll be using the ```readr``` package, which is included in the base tidyverse:

```{r}
library(tidyverse)
```

All the datasets included in our example use different _delimiters_ - the character that tells R (or Excel, or whatever program you're using) where one column stops and the next one begins. No matter what your delimiter is, we can parse the file using ```read_delim()``` - the first argument is the filename to read, while the second is the delimiter used. For instance, our text file is tab delimited - and since tabs are represented in R as ```\t```, we'll tell ```read_delim()``` to use that as a delimiter:

```{r}
NOCRegions <- read_delim("noc_regions.txt", "\t")
NOCRegions
```

We can do the same thing with documents that have comma separated values (known as CSVs):

```{r}
AthleteEvents <- read_delim("athlete_events.csv", ",")
AthleteEvents
```

(You may have noticed that took a second to load - at least, if your computer is middle-of-the-road. This dataset has 271,116 rows - even more than our ```diamonds``` dataset - so analyses might take slightly longer with it. Imagine how long "big data" problems would take on your computer, when datasets have several million rows of observations!)

However, ```readr``` also includes a pretty good tool specifically for CSV files:

```{r}
AthleteEvents <- read_csv("athlete_events.csv")
AthleteEvents
```

Now, ```readr``` doesn't have native support for more complicated files, like Excel files. Philosphically, you shouldn't store data in Excel format for long periods of time - we have no idea how long Microsoft will be around for, and the encoding used by Excel may someday disappear and take your data with it. CSVs are generally preferred for long-term data storage, as they're easy to understand visually and are easily parsed by computers.

However, data entry is much easier in Excel, and plenty of data professionals still use the format. Luckily, there's a package - ```readxl``` - designed to parse these types of files:
```
install.packages("readxl")
```

```{r}
library(readxl)
```

```{r}
NBAStats <- read_excel("players_stats.xlsx")
NBAStats
```

You'll also notice that RStudio has an "import dataset" button in the top right corner, which makes use of both the ```readr``` and ```readxl``` packages. This button lets you point and click your way through data import, and then copy the code into your script. It's a great resource for beginners!

### Exporting Data
Writing data to a file is also pretty painless using ```readr```. There are as many options for delimiters as before - you can use ```write_delim()``` to specify which you want to use with your data - but more commonly data is imported and exported as CSV files using ```write_csv()```

```{r}
write_csv(AthleteEvents, "athlete_events.csv")
```

Note that, like all other ```tidyverse``` functions, the dataset is the first thing you specify.

### Data Exploration
Let's put ```NBAStats``` off to the side for a moment, and look at our other two datasets. ```AthleteEvents``` is a list of all olympic competitors from 1892 to 2016, including basic statistics about each and any medals they may have won. ```NOCRegions```, meanwhile, maps codes used by the National Olympic Committee (NOC) to the countries they represent.

We can get a sense of the variables this dataset measures using ```psych::describe()```

```{r}
psych::describe(AthleteEvents)
```

Wow, R didn't like that!

R didn't know how to calculate most of the fields in ```describe()``` for character vectors. We can try that process again with only the numeric columns by using ```select_if()``` from ```dplyr```:

```{r}
psych::describe(select_if(AthleteEvents, is.numeric))
```

When I see this output, TK things catch my eye:

* We knew we were missing observations, but wow, we're missing observations. Over 60,000 athletes have no weight listed, while over 50,000 don't have a height.
* Mean height is 175.34 while mean weight is 70.7, suggesting that we're using metric units here
* Looking at mean, skew and kurtosis, it seems like we have a lot more athletes in recent years - the distribution is centered around 1978, almost 100 years into our 120 year dataset - and that while an incredible number of athletes are young (25, with a kurtosis of 6.27!), we have plenty of older competitors.

If we want to get a sense of correlations in the data, we can try using ```pairs()``` again - but be warned, this one might take a while due to the size of the data.

```{r cache = TRUE}
pairs(select_if(AthleteEvents, is.numeric))
```

(Pop quiz for the history nerds: what are those vertical bars in the ```Year``` column?)

That's a little chaotic, but we can still see trends nicely enough. It looks like height and weight have increased in variance over the years, probably due to the increased number of athletes overall. Height and weight look tightly correlated, as do age and the other statistics, interestingly enough. If we look at the actual correlation coefficients, we can see these trends numerically. We're going to have to remove the NAs manually, first, using dplyr's ```drop_na```

```{r}
AthleteEvents %>%
  select_if(is.numeric) %>%
  drop_na() %>%
  cor()
```

So we're right in thinking that height and weight are correlated, but maybe not age. It is interesting to see that year is more correlated with age than any other variable!

Moving back to the full dataset, I have a few other questions I want to ask. First off, when did women start competing? It would be useful to know if our gender distributions have significantly different lengths. Let's ```count()``` the number of male and female athletes per year:

```{r}
AthleteEvents %>%
  count(Year, Sex)
```

The first women appear in 1900! That's very cool.

I do wonder if the male or female athletes have it harder - that is, are there more male or female medal recipients, as a proportion of the whole? For instance, if fewer countries send female participants, each participant might be more statistically more likely to win. 

We can eyeball this by calculating the percentage of each gender who hold each type of medal, out of the total number of athletes who have participated. To do so, we'll count the number of recipients of each type of medal, rename the ```NA``` medal value "none", group our data by sex, and then divide the number of people who have received each medal by the total number of contestants:

```{r}
AthleteEvents %>%
  count(Sex, Medal) %>%
  replace_na(list(Medal = NA)) %>%
  group_by(Sex) %>%
  mutate(PercentReceiving = n / sum(n))
```

So it looks like women have a tiny edge, but generally speaking, both men and women have a 15% chance of medalling at the Olympics. 

### Modeling Winners

Personally, I'm interested in predicting if someone will win a medal at the Olympics. We could do this a number of inferential ways - for instance, dropping all the values where an athlete _didn't_ win, and looking at the summary statistics of the athletes who did:

```{r}
AthleteEvents %>%
  filter(!is.na(Medal)) %>%
  summary(object = .)
```

This doesn't help us that much - it appears that these athletes are slightly older, taller, and heavier than the whole dataset, but that's not a lot to go off of when making predictions - no one wants to bet on the oldest and heaviest high jumper. 

One way we could try to do this is using our old friend, the ```lm()``` tool, with all the data we have. We can make a new binary column ```Winner```, which will be 1 if the athlete won a medal, and then try to model it:

```{r}
AthleteEvents$Winner <- AthleteEvents$Medal
AthleteEvents$Winner[which(!is.na((AthleteEvents$Winner)))] <- 1
AthleteEvents$Winner[which(is.na(AthleteEvents$Winner))] <- 0
AthleteEvents$Winner <- as.numeric(AthleteEvents$Winner)
```


```{r eval = FALSE}
AthleteEvents %>%
  replace_na(list(Winner = 0)) %>%
  lm(Winner ~ ID * Name * Sex * Age * Height * Weight * NOC * Games * Year * Season * Sport * Event, data = .)
```
```
## Error: cannot allocate vector of size 72.4 Gb
```

But R doesn't like that much.

This process is pretty excessive - we're asking R to calculate a _lot_ of stuff - and also has the downside of probably being _too_ specific. After all, if we tell R what ID each athlete has, plus the year and the event, it can tell 100% of the time who won the medal!

Instead, we want to be able to guess what _traits_ make someone more likely to win. As such, we should only test the variables that we think are scientifically relevant - most likely, things like their age, height, and weight. If we make a model more like that:

```{r}
AthleteLinearModel <- lm(Winner ~ Age * Height * Weight, data = AthleteEvents) 

summary(AthleteLinearModel)
```

## Logistic Models
Wow! Every single term in our model is significant, but the R^2^ is horrible!

As for why that might be, let's visualize our response variable against one of its predictors - again, this might take a second to run:

```{r}
ggplot(AthleteEvents, aes(Height, Winner)) + 
  geom_point()
```

This here is the root of our problem - we're trying to model a binary outcome (a yes or no, a medal or not) with a straight line, which is useless. This sort of problem comes up all the time in real world situations - in science, whether or not a trap will catch something, or in business, whether or not a customer will click.

These problems are known as _classification_ problems, where the outcome is a categorical variable, and linear models are really bad at them. Instead, we can choose to use a _logistic model_, one of the types of _generalized linear models_ from chapter 3. These models serve to measure the probability of a binary event - while a linear model will tell you the value of your response variable, logistic models will tell you the probability your response variable is 1. 

R convieniently has a function ```glm()``` for doing exactly this:

```{r}
AthleteLogisticModel <- glm(Winner ~ Age * Height * Weight, data = AthleteEvents, family = "binomial") 

summary(AthleteLogisticModel)
```

Note that ```family = "binomial"``` is necessary to make R compute a logistic model - you can find other possible generalized linear model formats by typing ```?family```. 

## Modelling Metrics

Well, our terms are all still significant, which is a good thing - but you'll notice that we now have no R^2^! That's because R^2^ terms don't really exist for logistic models, for reasons we won't go too far into - you can read more on the topic [here](https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-what-are-pseudo-r-squareds/). But what we will get into are other methods of evaluating logistic models.

One of the more common methods of comparing logistic models is to use what's known as a pseudo R^2^ value. Unfortunately, there are plenty of different methods to compute these, and you can't compare R^2^ calculated with different formulas to one another. 

### Pseudo-R^2^
One package that gives pretty decent results is the ```pscl``` package by Simon Jackman. After we install it, we can use the ```pR2()``` function to give us some pseudo-R^2^ values:

```
install.packages("pscl")
```
```{r}
pscl::pR2(AthleteLogisticModel)
```

The McFadden R^2^ is given as the fourth output from this function, while the Cragg-Uhler R^2^ is the last value in the list. Again, these numbers can only be compared against other pseudo-R^2^ following the same formula, which limits their versatility.

### Area Under the ROC Curve (AUC)
More common in data analytics is to find the area under the receiver operating curve - abbreviated as AUC. To understand how we get there, let's first take a look at our model object:

```{r}
AthleteLogisticModel
```

Wow! R stores models as _list objects_, containing a number of elements of different data types. We can get a sense of what's going on under the hood using ```str()``` - that is, asking R to show us the ```str```ucture of the data:

```{r}
str(AthleteLogisticModel)
```

There's a ton going on in there!

Luckily enough, we don't have to worry about most of it. The two elements I do want to point out, though, are the ```y``` and ```fitted.values``` columns. ```y``` contains our response variable - whether or not an athlete medaled - as a binary value of 1 or 0. ```fitted.values```, meanwhile, stores the probability our model gives of ```y``` being 1. We can take advantage of this using the package ```pROC```, which will let us calculate how close our model got. Let's first install and load ```pROC```:

```
install.packages("pROC")
```

```{r}
library(pROC)
```

What we're going to do now is to build a receiver operating characteristic curve - a ROC - in order to understand graphically exactly what we're computing. ```pROC``` makes that easy, using the function ```roc()```:

```{r}
LogModelROC <- roc(AthleteLogisticModel$y, AthleteLogisticModel$fitted.values)
```

We can then plot our ROC object using ```plot.roc()```:

```{r}
plot.roc(LogModelROC)
```

This is cool! What the hell is it?

This is what we mean when we say receiver operating characteristic curve. That light grey line in the middle represents what would happen if we just randomly guessed whether or not each athlete got a medal, using a 50/50 chance. The black line represents how well our model did - everywhere that it's higher than the grey line, we were more accurate than random chance. We don't need to worry about exactly what the axes mean (basically, the x axis represents how confident our model is in guessing someone medaled, while the y is how surprised it is about the result), but you can read more for yourself [here](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5).

The curve is cool and all, but is hard to interpret. Luckily for us, there's a single value - similar to R^2^ for linear models - which we can use to identify how well our model did. That number is the area under the curve (AUC, sometimes called the c-statistic) that I mentioned earlier, which we can calculate with pROC by printing out our ROC object:

```{r}
LogModelROC
```

(Note that, if I didn't assign the ROC object to a name in the first place, this value would have just printed out.)

That number - 0.57 - is our single statistic for how accurate our model is. Generally speaking, models with an AUC of 0.7 are thought of as good models, with 0.8 as great and 0.9 incredible. Ecology can have slightly fuzzier models - predicting the natural world is hard, yo - but even the "random chance" line has an AUC of 0.5 - if your model is close to or below that, it's pretty much useless.


### Model Comparisons

#### AUC

So while our model is doing better than chance, it's still not doing great - 0.57 leaves a lot of room to improve. For instance, since we know the sex of each athlete in our dataset, what would happen if we added that variable to our formula?

We can calculate the new formula following all the steps above. I'm also going to use ```pscl``` and ```pROC``` to find the pseduo-R^2^ and AUC of our new model, as well:

```{r}
AthleteLogisticModel2 <- glm(Winner ~ Age * Height * Weight * Sex, data = AthleteEvents, family = "binomial") 

pscl::pR2(AthleteLogisticModel2)

LogModel2ROC <- roc(AthleteLogisticModel2$y, AthleteLogisticModel2$fitted.values)
LogModel2ROC
```

So, under each metric, our new model seems to be a slightly better fit to the data. The benefits of working with AUC - and, specifically, with ```pROC``` - is that testing to see if one model is better than the other is a piece of cake with ```roc.test```:

```{r}
roc.test(LogModelROC, LogModel2ROC)
```

And so we're able to conclude that yes, our second model is significantly better than the first - including sex in our model made it significantly more predictive.

#### AIC 

While comparing model AUCs is effective, it isn't the most popular method to analyze model performance. That honor likely goes to the Akaike Information Criterion, more commonly known as the AIC.

AIC can be used effectively in three ways:

* **Model selection**, where you compare _every combination of scientifically justifiable variables possible_ OR
* **Variable analysis** where you either compare your response variable against each predictor variable _independently_ OR
* **Variable analysis** where you standardize your predictor variables, put them into a model, and compare their coefficients

AIC can't be used to rank random models against one another without actual experimental design. More rants about this may be found [here](https://dynamicecology.wordpress.com/2015/05/21/why-aic-appeals-to-ecologists-lowest-instincts/). 

As such, our example here - only comparing two models, with no strong hypothesis as to why these are the two that deserve to be compared - isn't the best way to demonstrate the use of AIC. We'll follow up with a more clear example in a moment.

AIC can be calculated using the ```broom``` package's ```glance()``` on any model object you have:

```{r}
library(broom)
```

```{r}
glance(AthleteLogisticModel)
glance(AthleteLogisticModel2)
```

The number we're interested in right now is AIC, located halfway across the dataframe. There's a very simple rule for comparing models using AIC: $\Delta$AIC between two models > 4? The model with the smaller AIC is better. $\Delta$AIC between 2 and 4? Odds are the model with the smaller AIC is better, but it's shakier. $\Delta$AIC < 2? The two models are effectively identical.

In this case, our second model has an AIC of 168576.3, while the first model's AIC value is 169872.5. As such, the models have a $\Delta$AIC of `r 169872.5 - 168576.3`, which is a little bit bigger than 4.

Note that the actual AIC number is unitless and arbitrary - the AIC isn't so much a metric for how well the model fits the data, in the way that R^2^ and AUC are, but rather a way to tell _which_ of your models better fits the data. More commonly reported than AIC values are the $\Delta$AIC values, the number of parameters in a model (represented as _k_), and the Aikake Weights of each model - information on calculating those [here](http://www.brianomeara.info/tutorials/aic/).


## More Complicated Analyses

So far, we haven't really earned our chapter title - while what we've been doing so far has been more in depth, it hasn't necessarily been more complicated.

This is the section that'll step things up a little. If you got a strong understanding of functional programming in unit 4, and then understood making your own functions in unit 5, this part shouldn't be particularly hard. However, it is one of the more involved blocks of code we've worked through so far - so don't feel bad if it takes a little time before things work right, or before you fully understand what we're doing.

So, as we mentioned above, using AIC to compare two random models isn't exactly what the tool is designed for. While it isn't technically _wrong_, so long as the models are both of the same type (i.e., logistic) and modeling the same dataset, it isn't _useful_ to science or to you. We don't particularly care which models are slightly better than others - we want to know which ones are right!

You can either do this by comparing a small set of models that you have strong reasons to believe are accurate - this is a form of hypothesis testing, but using model AICs rather than p values - or by comparing all possible models which use the same set of _scientifically sound predictor variables_. Note the "scientifically sound" - you can't throw millions of predictors at the wall and hope that they stick; you have to be able to justify their inclusion.


### Model Selection

> Model improvement doesn't make it into papers for the same reason people donâ€™t go around introducing you to their ex-wives.
> <div align = "right"> --- Andrew Gelman </div>

Lets say with our Olympic athlete dataset I can justify expecting sex, age, height, and weight as predictors for the model, as well as the interactions between each of these terms. After all, we can expect age, height, and weight to vary together somewhat - and each of these likely differs between male and female athletes.

One way I could compare the combinations of these variables would be to ```glance()``` at each of them individually, like so:

```{r}
## Glancing at the full model
glance(glm(Winner ~ Sex * Age * Height * Weight, data = AthleteEvents, family = "binomial"))
```

However, this is inefficient, requires a lot of typing, and makes it really hard to compare AICs directly - or to export them to somewhere like Excel, for inclusion in a document. A somewhat better way is to define each model as its own function, taking the argument ```df```:

```{r}
FullMod <- function(df){
  glm(Winner ~ Sex * Age * Height * Weight, data = df, family = "binomial")
}

One <- function(df){
  glm(Winner ~ Sex * Age * Height, data = df, family = "binomial")
}

Two <- function(df){
  glm(Winner ~ Sex * Age * Weight, data = df, family = "binomial")
}

Three <- function(df){
  glm(Winner ~ Sex * Height * Weight, data = df, family = "binomial")
}

Four <- function(df){
  glm(Winner ~ Sex * Height, data = df, family = "binomial")
}

Five <- function(df){
  glm(Winner ~ Sex * Age, data = df, family = "binomial")
}

Six <- function(df){
  glm(Winner ~ Sex * Weight, data = df, family = "binomial")
}

Seven <- function(df){
  glm(Winner ~ Age * Height * Weight, data = df, family = "binomial")
}

Eight <- function(df){
  glm(Winner ~ Age * Height, data = df, family = "binomial")
}

Nine <- function(df){
  glm(Winner ~ Age * Weight, data = df, family = "binomial")
}

Ten <- function(df){
  glm(Winner ~ Height * Weight, data = df, family = "binomial")
}

Eleven <- function(df){
  glm(Winner ~ Sex, data = df, family = "binomial")
}

Twelve <- function(df){
  glm(Winner ~ Age, data = df, family = "binomial")
}

Thirteen <- function(df){
  glm(Winner ~ Height, data = df, family = "binomial")
}

Fourteen <- function(df){
  glm(Winner ~ Weight, data = df, family = "binomial")
}

NullMod <- function(df){
  glm(Winner ~ 1, data = df, family = "binomial")
}
```

That last item in the list is what's known as the _null model_ - it's a similar concept to the null hypothesis in hypothesis testing. Its purpose is to serve as a baseline for judging the rest of our models - we're hoping it has a $\Delta$AIC > 4!

Now that we have our models defined, we're able to run each of them against the dataset. Fair warning, this step will take a long time to complete - we're asking R to compute estimates for something like 4.8 million observations by doing this. We'll then overwrite the columns we just created, replacing them with the values from ```glance()``` so we can make model comparisons:

```{r}
LogModelGlances <- AthleteEvents %>%
  nest() %>%
  mutate(FullMod = map(data, FullMod),
         FullMod = map(FullMod, glance),
         One = map(data, One),
         One = map(One, glance),
         Two = map(data, Two),
         Two = map(Two, glance),
         Three = map(data, Three),
         Three = map(Three, glance),
         Four = map(data, Four),
         Four = map(Four, glance),
         Five = map(data, Five),
         Five = map(Five, glance),
         Six = map(data, Six),
         Six = map(Six, glance),
         Seven = map(data, Seven),
         Seven = map(Seven, glance),
         Eight = map(data, Eight),
         Eight = map(Eight, glance),
         Nine = map(data, Nine),
         Nine = map(Nine, glance),
         Ten = map(data, Ten),
         Ten = map(Ten, glance),
         Eleven = map(data, Eleven),
         Eleven = map(Eleven, glance),
         Twelve = map(data, Twelve),
         Twelve = map(Twelve, glance),
         Thirteen = map(data, Thirteen),
         Thirteen = map(Thirteen, glance),
         Fourteen = map(data, Fourteen),
         Fourteen = map(Fourteen, glance),
         NullMod = map(data, NullMod),
         NullMod = map(NullMod, glance))

LogModelGlances
```

(This is why you need to have smart hypotheses about which variables matter - imagine having to wait for thirty-two models to process, or even more!)

We then want to get from this wide dataframe to a tidy dataframe, by ```gather()```ing the model types into a column, paired with the dataframe their column contains:

```{r}
LogModelGlances <- LogModelGlances %>%
  gather(Model, Value) %>%
  slice(-1)
```

I also used ```slice(-1)``` to drop the first row containing our data, since we don't need it anymore. 

Now all that's left is to unlist our value columns. I'm also going to arrange the table by AIC, to show us which models performed best (and worst!)

```{r}
LogModelGlances %>%
  unnest(Value, .drop = T) %>%
  arrange(AIC)
```

Tada! Looks like all our models outperformed the null model by a good margin - and our full model is the best model of the set, with no other model getting particularly close to it. If we wanted, we could now measure just how accurate our best model is, using either AUC or a pseudo-R^2^:

```{r}
ROCMod <- glm(Winner ~ Sex * Age * Height * Weight, data = AthleteEvents, family = "binomial")

pROC::roc(ROCMod$y, ROCMod$fitted.values)
pscl::pR2(glm(Winner ~ Sex * Age * Height * Weight, data = AthleteEvents, family = "binomial"))
```

0.6 AUC and 0.255 McFadden's pseudo-R^2^. Even after all that, it seems like our model could be improved quite a bit!


#### Detour: Another Way
Now, obviously, this entire section breaks our rule that if you have to type it more than twice, there's a better way. That's true here, too - but the better way is significantly more complicated, so I introduced the longer form first. The better way is in the next section of this unit.

Typing models out this way does become prohibitive as you add more variables - for every _k_ variables you want to include, you wind up having to type out 2^k^ formulas. Even greater than the time tax that puts on you is the amount of time it will take to compute those models and fit them to your data. Generally speaking, you shouldn't have massive numbers of variables in your regression equations. Remember the old saying:

> All models are wrong, but some models are useful.

Or, alternately:

> The perfect is the enemy of the pretty good.

Lots of practicioners want to use as much data as they can to make the most accurate and precise model formula possible, no matter how efficient or practical that formula may be. That's because a lot of people lose track of the whole purpose of the model - someone should be able to take this crazy thing you've created and go _do_ something with it. Models with fifty parameters make that impossible, unless practicioners are going to be able to get as much data as you happened to collect. Plus, longer formulas may be better fits to your data - but they're also more likely to be _overfits_, worse at describing the outside world. Finally - and this is the challenge most relevant to business applications, as well - models with more terms are extremely costly to use, requiring either huge amounts of computing power or time to work properly. 

As such, a better practice is to figure out which variables are most likely to matter before you start building your models - as we did, with our correlation plots and tables. This method is a lot more sound scientifically, as well - rather than throwing things at the wall until you got one to stick, you're better able to explain your analysis methods, and the reason those terms matter.

However, if you want to throw things at the wall anyway, the statistically preferred way to do so is stepwise selection (one informative link [here](https://people.duke.edu/~rnau/regstep.htm), another - this one including information on the true "stepwise" bidirectional selection - [here](https://stats.stackexchange.com/questions/317625/what-is-stepwise-linear-regression), and a third - this one explaining things better, but including coding tips for non-R programs, [here](http://www.markirwin.net/stat135/Lecture/Lecture32.pdf)). 

The basic process to do this is to fit two models - a null model and a full model - and then pass them to the ```step()``` function. First, though, we have to drop all the rows where our predictor values have a value of ```NA```:

```{r}
StepWiseData <- AthleteEvents %>% 
                    drop_na(3:6, 14) ## Numbers specify columns to drop NA in
```


We can choose which type of stepwise selection we want by specifying certain parameters within this function - for instance, given our models:

```{r}
StepWiseFullMod <- glm(Winner ~ Sex * Age * Height * Weight, data = StepWiseData, family = "binomial")
StepWiseNullMod <- glm(Winner ~ 1, data = StepWiseData, family = "binomial")
```

We can perform a backwards selection by typing:

```{r eval = FALSE}
step(StepWiseFullMod)
```

A forward selection via:

```{r eval = FALSE}
step(StepWiseNullMod, 
     scope = list(lower = formula(StepWiseNullMod), 
                  upper = formula(StepWiseFullMod)),
     direction = "forward")
```

And a bi-directional bothways function via the following:

```{r eval = FALSE}
step(StepWiseNullMod, 
     scope = list(lower = formula(StepWiseNullMod), 
                  upper = formula(StepWiseFullMod)),
     direction = "both")
```

I'm not running them here, because they take a ton of space. Note that these methods give us slightly different formulas, due to how each calculates which terms to include.

This method is becoming disfavored, as it doesn't require you to think about _why_ you want a given variable in the model. But I can't stop you from doing whatever you're gonna do.

#### The Better (...Faster) Way

Before we launch into this, note that this section is a little more techy and a little more obscure. It shouldn't be that hard to follow, if you fully understand how for loops and map functions work. But if this is a little above your level right now, that's totally fine - think of this section as more of a template, showing how we can combine tools to make our code faster and more efficient.

There _is_ a way to programmatically generate all the models, using the ```map()``` family of functions. Before we get to that, though, let's create a smaller dataframe - ```Predictors``` - that only contains our predictor variables. We'll also get the ```names()``` of all our predictors, and store that in our column ```Cols```:

```{r}
Predictors <- data.frame(Sex = AthleteEvents$Sex,
                         Age = AthleteEvents$Age,
                         Height = AthleteEvents$Height,
                         Weight = AthleteEvents$Weight)

Cols <- names(Predictors)
```

We're now going to find the ```length()``` of our ```Cols``` vector and store it in ```n```:

```{r}
n <- length(Cols)
```

Alright, here's where things get a little more complicated.

What we want to do is use the ```combn()``` function to find all the combinations of our variables, so that we can use those to programmatically build formulas. The way we do this is to build what's known as an _anonymous function_ inside of ```combn()``` by typing ```~```, using our ```.``` pronoun to iterate through a dataset. We'll also set ```simplify``` inside of the ```combn()``` function to ```FALSE```. If we pass this to ```map()```, we get the following output:

```{r}
map(1:n, ~ combn(1:n, ., simplify = FALSE))
```

Which is very close to what we want. The issue is, each of these combinations is buried inside a list - while we want a list of combinations, what we have is a list of lists of combinations. To get rid of that higher-level list, we can use the ```unlist()``` function - specifying ```recursive = FALSE``` to make sure we only get rid of the top-level list. I'm going to assign that to ```id```:

```{r}
id <- unlist(map(1:n, ~ combn(1:n, ., simplify = FALSE)), recursive = FALSE)
```

We now have everything we need to find our formulas - the names of our predictor variables and all the combinations we want of them. We're now going to use ```map_chr()``` to ```paste()``` together each of our formulas - by indexing ```Cols``` with ```[.]``` in our anonymous function, we can call each combination of column names we have specified in ```id```. ```collapse = "*"``` will separate each of our predictors with the ```*``` operator:

```{r}
Formulas <- map_chr(id, ~ paste("Winner ~", paste(Cols[.], collapse="*")))
Formulas
```

Now, my desired outputs here are two dataframes: one of the outputs from ```tidy()```, and one of the outputs from ```glance()```. As such, I'm going to initalize two dataframes, ```Glances``` and ```Tidies```, with the outputs from those functions. I'm also going to create a column in ```Tidies``` to store which formula the output is from:

```{r}
Glances <- glance(glm(Winner ~ 1, data = AthleteEvents, family = "binomial"))
Tidies <- tidy(glm(Winner ~ 1, data = AthleteEvents, family = "binomial"))
Tidies$FormNum <- 0
```

And now we want to actually calculate these models for our data. For this, we have to use a for loop, running as many times as we have formulas. Rather than explain everything in the loop up here, I've commented the code below, with explanations following the ```##```:

```{r}
for(i in seq_along(Formulas)){
## Convert the current (index i) formula into a logistic model
  CurrentModel <- glm(as.formula(Formulas[[i]]), data = AthleteEvents, family = "binomial")
  
## Store the glance of the current model in CurrentGlance
  CurrentGlance <- glance(CurrentModel)
  
## Make row i of Glances the current model's glance
  Glances[i, ] <-  as.vector(CurrentGlance)
  
## Store the tidy of the current model, label it with the model number, and convert it to a vector
  CurrentTidy <- tidy(CurrentModel)
  CurrentTidy$FormNum <- i
  CurrentTidy <- as.vector(CurrentTidy)
## Append the current tidy to your table with all tidy outputs
  Tidies <-  rbind(Tidies, CurrentTidy)
}
```

And if everything worked properly, we can print out our dataframes to make sure we got everything right:

```{r}
Glances
```

## Relational Data

Okay, this next section should be easier and more relevant to your daily life. Before we get into it, I'm going to make two edits to the ```Glances``` table:

1. I'm going to convert the row names (those unlabeled numbers on the left) into a column called ```FormNum```
2. I'm going to only select our columns of ```FormNum``` and ```AIC```:

```{r}
Glances <- Glances %>%
  mutate(FormNum = as.numeric(row.names(.))) %>%
  select(FormNum, AIC)
Glances
```

I also want to point out one important difference between our dataframes - the ```Glances``` dataframe doesn't include our null model, while ```Tidies``` has it as ```FormNum == 0```. We can check to make sure by ```factor()```ing our dataframes' ```FormNum``` fields, which identifies each unique level in the vector. We can then find all the ```levels()``` contained in that factor, confirming that ```0``` is only present in the ```Tidies``` dataframe:

```{r}
levels(factor(Glances$FormNum))
levels(factor(Tidies$FormNum))
```


What we'll be doing now is working on _joining_ datasets, combining two dataframes into a single output. All of these functions take the form ```f(x, y)```, where ```f()``` is the function name and ```x, y``` are the arguments. What this function does is find columns with the same name - known as _keys_ - and combine your dataframes based on the values in those columns. For our examples, our key column will be the FormNum column.

### Inner Join

An inner join will preserve only the rows in your data which have keys present in both datasets - so, in this case, the null model will be dropped:
```{r eval=FALSE}
inner_join(Glances, Tidies)
```

### Left Join

A left join will preserve all the values in your ```x``` dataset - here, in Glances. 
```{r eval=FALSE}
left_join(Glances, Tidies)
```

### Right Join

A right join will preserve all the values in your ```y``` dataset - here in Tidies. As you can see, the AIC for the null model is left as NA, but the null model stays in the dataframe.
```{r eval=FALSE}
right_join(Glances, Tidies)
```

### Full Join
A full join will save all values in both your ```x``` and ```y``` dataframes.
```{r eval=FALSE}
full_join(Glances, Tidies)
```

### Semi Join
A semi join returns a row from ```x``` if that key value has a match in ```y```. 
```{r eval=FALSE}
semi_join(Glances, Tidies)
```

### Anti Join
An anti join returns a row from ```x``` if that key value _doesn't_ have a match in ```y```:
```{r eval=FALSE}
anti_join(Glances, Tidies)
```

### Specifying Key Columns
Sometimes it's helpful to specify _which_ columns you want to join your data by - particularly when you have columns with the same name but different values. In those situations, use ```by = ``` to specify which columns you want to join by:
```{r eval=FALSE}
full_join(Glances, Tidies, by = "FormNum")
```

### Merging Multiple Dataframes
In order to merge more than two dataframes at the same time, use ```reduce()``` with any of the above joins - all your dataframes have to be in a ```list()``` inside the ```reduce()``` call:
```{r eval=FALSE}
## Creating example dataframes:
x <- data.frame(key = c("A","A","B","B"),
                value = c(3,4,3,4))
y <- data.frame(key = c("A","A","B","B"),
                value = c(124,12,524,43))
z <- data.frame(key = c("A","A","B","B"),
                value = c(31, 4, 14, 124))

## Using "reduce" to merge all dataframes:
reduce(
  list(
    x,
    y,
    z
  ), 
  full_join, 
  by = "key"
)
```

### Binding Dataframes
Sometimes, however, we don't have a key column to work with. In these cases, we usually just want to combine all of our data. Consider the dataframes:
```{r}
df1 <- tibble(Name = c("Alice", "Bob"),
              EyeColor = c("Blue", "Brown"))

df2 <- tibble(BestFriend = c("Bob", "Alice"),
              HairColor = c("Brown", "Blue"))
```

If the rows of each dataframe represent the same observation - that is, row 1 in dataframe 1 is the same as row 1 in dataframe 2 - we can combine the tables using ```cbind()```:

```{r}
combineddf <- cbind(df1, df2)
```

But say we forgot someone, whose records are stored in dataframe 3:
```{r}
df3 <- tibble(Name = "Alice",
              EyeColor = "Grey",
              BestFriend = "Eve",
              HairColor = "Grey")
```

If we want to add her to the table, we can use ```rbind()``` - so long as she has all the same columns as our original table!

```{r}
rbind(combineddf, df3)
```

## Exercises
1. Try out the types of join on the Olympic datasets ```AthleteEvents``` and ```NOCRegions```. What differences do you notice between the join types?
2. Explore the ```NBAStats``` dataset. Can you predict the number of points (```PTS```) players will score based on information available to you?
3. Make multiple linear models (```lm()```) for each position (```Pos```) in the dataset. 